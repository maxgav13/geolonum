[
["index.html", "Geología Numérica: Ciencia de Datos para Geociencias Prefacio", " Geología Numérica: Ciencia de Datos para Geociencias Maximiliano Garnier Villarreal 2020-07-03 Prefacio Este libro es producido en Markdown (Xie et al., 2018), usando bookdown (Xie, 2016). El paquete bookdown puede ser instalado desde CRAN o GitHub: install.packages(&quot;bookdown&quot;) # version en desarrollo # devtools::install_github(&quot;rstudio/bookdown&quot;) Este libro se basa en el material creado para el curso G-4101 Geología Numérica, de la Escuela Centroamericana de Geologia, Universidad de Costa Rica, pero la idea es ir expandiendo los temas más allá del curso, y que sirva como base para ir promoviendo la cuantificación de la geología. El libro hace uso del software estadístico y de programación R (R Core Team, 2019) para desarrollar la parte práctica de los temas cubiertos en la teoría. Existe un repositorio en GitHub para este proyecto (geolonum), donde se pueden acceder a los documentos y datos usados en este libro. Adicionalmente, se ha creado un paquete que ofrece funciones adicionales que serán usadas en algunos de los temas que se van a tratar. Para instalar y ver la documentación del paquete se puede ir a GMisc y seguir las instrucciones de la instalación (el paquete no esta en CRAN, esta en GitHub, por lo que no se puede instalar de la forma convencional). El libro se divide en 2 partes principales: Uso de R (Capítulos 1 al 6) Análisis de datos (Capítulos 7-19) En la parte de análisis de datos se cubre lo básico de álgebra lineal, para luego caer en la parte gruesa del curso que es estadística. En esta parte se ve lo que es estadística descriptiva (univariable y bivariable), principios de probabilidad, distribuciones de probabilidad, estadística inferencial (pruebas de hipótesis, estimación), y estadística no paramétrica. La parte final del curso cubre temas relacionados con geociencias (datos direccionales, secuencias, y geoestadística), donde se aplican conceptos anteriormente adquiridos en la sección de estadística. Este obra está bajo una licencia de Creative Commons Reconocimiento-NoComercial-CompartirIgual 4.0 Internacional. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Referencias "],
["intro.html", "Capítulo 1 Introducción 1.1 Instalación de R y RStudio 1.2 Paquetes 1.3 Ayuda en R 1.4 RMarkdown 1.5 Recursos", " Capítulo 1 Introducción R es un programa estadístico y de programación, el cual le permite al usuario hacer uso de funciones (en paquetes) ya definidas o la creación de funciones propias para resolver problemas específicos. Muchas de las funciones presentan un gran detalle en los resultados, haciendo fácil la interpretación de los mismos. 1.1 Instalación de R y RStudio R es el motor, donde se realizan las operaciones, RStudio es el chasis, lo que le permite al usuario interactuar con el motor, es una interfaz de desarrollo que facilita las tareas en un proyecto. Primero hay que instalar R: Ir a R Descargar la versión dependiendo del sistema operativo (Figura 1.1) Figura 1.1: Repositorio CRAN para descargar R Después de instalar R hay que instalar RStudio: Ir a RStudio Descargar RStudio Desktop, la versión libre (Free) 1.2 Paquetes La instalación de R básica trae una serie de paquetes (librerías) instaladas, que funcionan para cosas básicas y sencillas. Para instalar paquetes de forma interactiva se va a la sección de paquetes (packages), se hace click en instalar (install), como se muestra en la Figura 1.2, y se escribe el o los nombres de los paquetes a instalar, separados por espacios. Figura 1.2: Como instalar paquetes de forma manual La forma más práctica de instalar (actualizar) paquetes es por medio de código, usando install.packages(&quot;paquete&quot;). Para cargar paquetes y sus funciones durante una sesión de R se usa la función library(paquete). Cabe mencionar que en el orden que se carguen los paquetes así se habilitaran las funciones, y si hay funciones con el mismo nombre en diferentes paquetes, la que se va a habilitar por defecto es la del ultimo paquete cargado. Una forma de solucionar un conflicto de este tipo o llamar una función de un paquete sin cargarlo es por medio de paquete::funcion, donde se usa del paquete deseado la función correspondiente (Ejemplo dplyr::select). install.packages(&quot;devtools&quot;) library(lubridate) library(tidyverse) 1.3 Ayuda en R Para buscar ayuda sobre funciones y cuales argumentos requiere se puede usar ?funcion, o se para sobre la función y se apreta F1. ?mean 1.4 RMarkdown Un documento R Markdown es un tipo de documento que permite mezclar texto con código, manteniendo el análisis y los resultados en un mismo lugar. La interfaz y estructura típica se muestran en la Figura 1.3. Cuando ejecuta el código dentro del documento el resultado aparece por debajo del código. Figura 1.3: Estructura de un documento RMarkdown Todo documento empieza con el encabezado YAML, que se define entre guiones consecutivos (---). Aquí se definen las características generales del documento: titulo (‘title’) autor (‘author’) fecha (‘date’) tipo de documento (‘output’) etc Idealmente todo bloque de código debiera llevar un nombre para poder identificarlo, así como su resultado. Para ejecutar una sección de código se puede hacer click en el botón verde de Run dentro de la sección o colocando el cursor dentro de la sección y ejecutando Ctrl+Shift+Enter (Windows) o Cmd+Shift+Enter (Mac). Así de fácil! Se puede agregar una nueva sección haciendo click en Insert Chunk o ejecutando Ctrl+Alt+I (Windows) o Cmd+Option+I (Mac). 1.4.1 Tipos de resultados Se pueden tener resultados de diferentes tipos, los cuales se muestran a continuación. 1.4.1.1 Consola Podemos crear diferentes objetos dentro de la sección como si fuera la consola de R. Creemos un objeto que contenga los números del 1 al 15. numeros &lt;- seq_len(15) numeros ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Podemos desplegar tablas (Tabla 1.1). La apariencia va a cambiar dependiendo el formato de salida y va a estar sujeta a la opción df_print en el encabezado YAML, a menos de que se sobre-escriba en el código. Para darle numero y etiqueta a una tabla se debe usar la función kable del paquete knitr (Xie, 2014, 2015). mtcars %&gt;% knitr::kable(caption = &#39;Datos de diferentes carros&#39;) Tabla 1.1: Datos de diferentes carros mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 1.4.1.2 Gráficos Gráficos de ggplot2 o cualquier otro gráfico estático son resultados que se pueden desplegar. Opciones para el control sobre las figura van a empezar con fig. en el encabezado del código (como se hace en el siguiente ejemplo, Figura ?? donde se ajusta el ancho a 6 pulgadas - fig.width=6). q = ggplot(mtcars, aes(wt,mpg,col=factor(cyl))) + geom_point(size=2,shape=1) + theme_bw() + labs(x=&#39;Peso&#39;,y=&#39;Millaje&#39;,col=&#39;Cilindros&#39;) q Figura 1.4: Grafico estatico 1.4.1.3 Widgets HTML Si el análisis en R involucra componentes interactivos, estos también son compatibles con los resultados en el cuaderno o archivo HTML. La opción eval=knitr::is_html_output() se incluye para que el código sea evaluado únicamente cuando el formato de salida es HTML, ya que estos no pueden desplegarse en PDF o Word. El siguiente código genera un gráfico interactivo (Figura 1.5). dygraph(nhtemp, main = &quot;Temperaturas de New Haven&quot;) %&gt;% dyRangeSelector(dateWindow = c(&quot;1920-01-01&quot;, &quot;1960-01-01&quot;)) ## Registered S3 method overwritten by &#39;xts&#39;: ## method from ## as.zoo.xts zoo Figura 1.5: Ejemplo de grafico interactivo En la Figura 1.6 se muestra la versión interactiva de la Figura ??, anteriormente generada. ggplotly(q) Figura 1.6: Version interactiva del grafico estatico 1.4.2 Formulas Expresiones matemáticas y formulas se pueden desplegar en linea, dentro del cuerpo del texto (\\(A = \\pi*r^{2}\\)) o por separado \\[E = mc^{2}\\] Para escribir estas expresiones se usa la metodología o lenguaje LaTeX. 1.4.3 Importando datos Los documentos R Markdown usan una dirección relativa a la ubicación del archivo. dat &lt;- import(&quot;data/LungCapData2.csv&quot;) 1.4.4 Cálculos en linea Siempre que un valor exista dentro de un objeto guardado, este se puede acceder para ser desplegado en el cuerpo del documento. Esto se realiza escribiendo código entre comillas invertidas, como por ejemplo round(mean(airquality$Temp), 2), lo que resulta en 77.88, que es la temperatura media en grados Fahrenheit. 1.4.5 Importando figuras La mejor manera es usando el paquete knitr. Aquí se ajusta no el ancho de la figura directamente sino que se le dice que se ajuste a un 50% del ancho disponible. Existen otras opciones que empiezan con out.. En la Figura 1.7 se ajusta el ancho de salida con out.width='50%'. knitr::include_graphics(&#39;images/r_rocks.jpg&#39;) Figura 1.7: R Rocks 1.4.6 Salvando y compartiendo Los documentos R Markdown tienen como extensión .Rmd. Cuando se crea y se salva un cuaderno se crea un archivo adjunto con extensión .nb.html. Este archivo contiene una copia renderizada del cuaderno, que puede ser visualizada en cualquier navegador. Cuando se abre el archivo .nb.html en un navegador se va a tener la opción de descargar el código original (.Rmd). Para previsualizar el cuaderno renderizado (.nb.html) haga click en Preview. La previsualización le muestra una copia renderizada del HTML. A diferencia de Knit (para otros documentos R Markdown), Preview no ejecuta ninguna sección de código, por lo que si una sección no se ha ejecutado en el editor el Preview no va a mostrar ningún resultado. La previsualización se actualiza cada vez que se salva el documento .Rmd. 1.4.6.1 Otros formatos El cuaderno es un documento R Markdown. Se puede cambiar el formato de salida cambiando el orden en el encabezado YAML. En ese caso el Preview es reemplazado por Knit y genera un documento HTML corriente. También se puede generar desplegando el menú contextual haciendo click en la flecha junto a Preview. También se pueden crear documentos PDF o Word. 1.5 Recursos Se presentan recursos a consultar para ahondar más en los temas presentados. La guía definitiva de RMarkdown RMarkdown Definitive Guide, para consultas sobre documentos rmakrdown y sus opciones y configuraciones. La guía definitiva de Bookdown Bookdown Guide. Este recurso cubre más sobre rmarkdown y expande sus funciones para la creación de libros. Referencias "],
["basico.html", "Capítulo 2 Funcionamiento básico de R 2.1 Introducción 2.2 Operaciones básicas 2.3 Crear objetos 2.4 Vectores 2.5 Matrices 2.6 DataFrames, listas y tibbles 2.7 Verificando objetos 2.8 Guardando el espacio de trabajo 2.9 Importando/cargando datos 2.10 Exportando datos 2.11 Inspeccionando los datos 2.12 Descripciones generales (globales) 2.13 Recursos", " Capítulo 2 Funcionamiento básico de R 2.1 Introducción En este capitulo se va a dar una introducción al funcionamiento básico de R (R Core Team, 2019), incluyendo el uso de R como una calculadora (consola), creación de objetos, y los tipos principales objetos que se utilizan en R, así como importar y exportar datos, y unas inspecciones generales de los datos. Se van a usar, aunque poco en este capitulo, funciones de los paquetes: library(rio) library(skimr) library(psych) library(DescTools) library(tidyverse) library(summarytools) 2.2 Operaciones básicas R puede funcionar como una calculadora básica, donde es posible realizar operaciones aritméticas sencillas. Los nombres de las funciones están en ingles (ej: sqrt para raíz cuadrada, round para redondeo, etc.). Como cualquier otro programa, si se va a utilizar operaciones con ángulos (ej: cos, tan, etc.), los ángulos tienen que darse en radianes, y el resultado va a estar en radianes. 1+2 ## [1] 3 1-2 ## [1] -1 1*2 ## [1] 2 1/2 ## [1] 0.5 sqrt(125) ## [1] 11.18034 2.3 Crear objetos Objetos se pueden crear usando los operadores &lt;- o =. Al crear un objeto este no se despliega en la consola a menos que uno lo llame directamente o que a la hora de crearlo sea encerrado con paréntesis redondos (). La idea de generar objetos es básica para los lenguajes de programación. Los objetos que se crean pueden ser reutilizados después de ser creados. Si se desean actualizar las operaciones o funciones que dependen de un objeto, solo se cambia el objeto una vez, y el resto se actualiza cuando se vuelve a correr. Aquí se están generando los objetos x, y, y z, donde x se imprime hasta llamarlo, y se imprime al guardarlo por estar encerrado en paréntesis, y z es función de x y y. x &lt;- 1 (y = 2) ## [1] 2 x ## [1] 1 x + y ## [1] 3 z = x + y 2.4 Vectores Los vectores son unidimensionales y deben ser (contener elementos) del mismo tipo. Aquí se muestran los diferentes tipos, y como crearlos. Los más importantes tipos son Numéricos, Texto, y Factores (tipo especial en R). 2.4.1 Numéricos Se pueden crear vectores numéricos a partir de datos puntuales, sin ningún orden, usando la función c(), y separando las entradas (elementos) por medio de comas ,. x &lt;- c(1,2,3,4,5) y &lt;- c(6,7,8,9,10) O se pueden crear vectores en secuencia (seq(from = , to = , by =, o length.out = )) o de valores repetidos (rep()). Crear vectores de valores repetidos no aplica únicamente para datos numéricos. En seq se definen los argumentos from: valor inicial, to: valor final, y by: el intervalo, o length.out: la cantidad de elementos que quiero. En rep se define lo que se quiere repetir, y el numero de veces a repetir. (f = 1:30) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 (t1 = seq(from = 0, to = 20, by = .2)) ## [1] 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 ## [16] 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 ## [31] 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6 8.8 ## [46] 9.0 9.2 9.4 9.6 9.8 10.0 10.2 10.4 10.6 10.8 11.0 11.2 11.4 11.6 11.8 ## [61] 12.0 12.2 12.4 12.6 12.8 13.0 13.2 13.4 13.6 13.8 14.0 14.2 14.4 14.6 14.8 ## [76] 15.0 15.2 15.4 15.6 15.8 16.0 16.2 16.4 16.6 16.8 17.0 17.2 17.4 17.6 17.8 ## [91] 18.0 18.2 18.4 18.6 18.8 19.0 19.2 19.4 19.6 19.8 20.0 (t2 = seq(from = 0, to = 20, length.out = 11)) ## [1] 0 2 4 6 8 10 12 14 16 18 20 (u = rep(5,20)) ## [1] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 rep(5:7,3) ## [1] 5 6 7 5 6 7 5 6 7 rep(5:7,each=3) ## [1] 5 5 5 6 6 6 7 7 7 2.4.2 Texto (string, character) Los vectores de texto son usualmente el precursor de vectores categóricos o factores. Se construyen de manera similar usando c(), pero cada entrada (elemento) va en comillas (doble &quot;&quot;, o sencilla ’’). z &lt;- &quot;pura vida&quot; z ## [1] &quot;pura vida&quot; dias &lt;- c(&quot;lunes&quot;,&quot;martes&quot;,&quot;miercoles&quot;,&quot;jueves&quot;,&quot;viernes&quot;) dias ## [1] &quot;lunes&quot; &quot;martes&quot; &quot;miercoles&quot; &quot;jueves&quot; &quot;viernes&quot; 2.4.3 Categóricos (factores) Los vectores para datos categóricos en R son llamados factores, y estos factores van a contener niveles o clases (levels). Este tipo de vector es muy utilizado en diversos análisis. El primer tipo es un factor nominal (sin orden en los niveles/clases). Aquí se esta usando el vector de texto creado anteriormente, y simplemente se convierte a factor. La función as_factor es del paquete forcats, que se carga al cargar el tidyverse. Existe una función básica as.factor; los comportamientos son un poco diferentes, siendo la principal diferencia que as_factor ordena los niveles de acuerdo al orden de aparición, mientras que as.factor ordena los niveles de manera alfabética. dias.f1 = as_factor(dias) dias.f1 ## [1] lunes martes miercoles jueves viernes ## Levels: lunes martes miercoles jueves viernes dias.f2 = as.factor(dias) dias.f2 ## [1] lunes martes miercoles jueves viernes ## Levels: jueves lunes martes miercoles viernes Factores ordinales pueden crearse usando la función básica factor, primero con el vector de datos (usualmente texto), agregando el argumento ordered = TRUE. Ademas, hay que especificar los niveles en el orden deseado con el argumento levels, donde van a ir de menor a mayor. ordenado = factor(c(&#39;Bajo&#39;,&#39;Alto&#39;,&#39;Alto&#39;,&#39;Medio&#39;,&#39;Medio&#39;,&#39;Bajo&#39;,&#39;Alto&#39;), ordered = T, levels = c(&#39;Bajo&#39;,&#39;Medio&#39;,&#39;Alto&#39;)) ordenado ## [1] Bajo Alto Alto Medio Medio Bajo Alto ## Levels: Bajo &lt; Medio &lt; Alto 2.5 Matrices Las matrices son representaciones multidimensionales de datos numéricos. La función para construirlas es matrix, donde se especifican los datos y el numero de filas o columnas. Por defecto inserta los datos por columna, si se quieren meter por fila se debe usar byrow = TRUE. Adicionalmente se le pueden agregar nombres a las filas y columnas con el argumento dimnames. matrix(data = 1:15, nrow = 3) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 4 7 10 13 ## [2,] 2 5 8 11 14 ## [3,] 3 6 9 12 15 matrix(data = 1:15, nrow = 3, byrow = T) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 matrix(data = 1:15, nrow = 3, byrow = T, dimnames = list(Filas = letters[1:3], Columnas = LETTERS[1:5])) ## Columnas ## Filas A B C D E ## a 1 2 3 4 5 ## b 6 7 8 9 10 ## c 11 12 13 14 15 2.6 DataFrames, listas y tibbles Los vectores son unidimensionales y pueden almacenar datos (elementos) de un solo tipo. DataFrames, listas y tibbles son objetos que pueden almacenar más de 1 vector y los diferentes vectores pueden ser diferentes entre ellos. Esto es similar a una hoja de calculo donde cada columna es un vector. Para DataFrames los contenidos pueden ser únicamente vectores de la misma longitud. Se crea usando la función data.frame, con los argumentos siendo los vectores, que van a pasar a ser las columnas. La función names brinda los nombres de las columnas, y a su vez se puede usar para renombrar a las columnas. DF = data.frame(Visitas = x, Revision = y, Dias = dias) DF ## # A tibble: 5 x 3 ## Visitas Revision Dias ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 6 lunes ## 2 2 7 martes ## 3 3 8 miercoles ## 4 4 9 jueves ## 5 5 10 viernes names(DF) ## [1] &quot;Visitas&quot; &quot;Revision&quot; &quot;Dias&quot; names(DF) = c(&quot;Experimento&quot;, &quot;Valores&quot;, &quot;Tiempo&quot;) DF ## # A tibble: 5 x 3 ## Experimento Valores Tiempo ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 6 lunes ## 2 2 7 martes ## 3 3 8 miercoles ## 4 4 9 jueves ## 5 5 10 viernes Para listas los contenidos pueden ser cualquier objeto y de cualquier dimensión. Muchos de los resultados de funciones en R son listas. Estas se crean con la función list. lst = list(Exp = x, Val = y, Dias = dias, Data = DF) lst ## $Exp ## [1] 1 2 3 4 5 ## ## $Val ## [1] 6 7 8 9 10 ## ## $Dias ## [1] &quot;lunes&quot; &quot;martes&quot; &quot;miercoles&quot; &quot;jueves&quot; &quot;viernes&quot; ## ## $Data ## Experimento Valores Tiempo ## 1 1 6 lunes ## 2 2 7 martes ## 3 3 8 miercoles ## 4 4 9 jueves ## 5 5 10 viernes Tibbles son un tipo especial de DataFrame, donde la principal diferencia es que pueden tener un contenido (columna) que puede ser una lista y esto puede brindar muchas facilidades a la hora de manipular y analizar los datos. Ademas no fuerza a datos de texto a factores y en la consola se despliega de manera más amigable. tb = tibble(Visitas = x, Revision = y, Dias = dias, Extra = map(6:10, ~rnorm(.x))) tb ## # A tibble: 5 x 4 ## Visitas Revision Dias Extra ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;list&gt; ## 1 1 6 lunes &lt;dbl [6]&gt; ## 2 2 7 martes &lt;dbl [7]&gt; ## 3 3 8 miercoles &lt;dbl [8]&gt; ## 4 4 9 jueves &lt;dbl [9]&gt; ## 5 5 10 viernes &lt;dbl [10]&gt; 2.7 Verificando objetos Hay funciones para verificar el tipo de objeto (mode, is, class), comprobar si es de un tipo en especifico (is.*) y cambiar de un tipo a otro (as.*). Con la funcione methods(class = *) se pueden obtener los diferentes métodos o funciones disponibles para ese tipo de objeto (va a depender de los paquetes cargados). mode(x) ## [1] &quot;numeric&quot; is(x) ## [1] &quot;numeric&quot; &quot;vector&quot; &quot;index&quot; &quot;replValue&quot; ## [5] &quot;numLike&quot; &quot;number&quot; &quot;atomicVector&quot; &quot;numericVector&quot; ## [9] &quot;EnumerationValue&quot; &quot;replValueSp&quot; &quot;Mnumeric&quot; class(x) ## [1] &quot;numeric&quot; x &lt;- c(1, 2, 3, 4, 5, 6) methods(class = class(x)) ## [1] [ [&lt;- all.equal Arith as_factor ## [6] as_mapper as.data.frame as.Date as.POSIXct as.POSIXlt ## [11] as.raster as.yearmon as.yearqtr cbind2 coerce ## [16] coerce&lt;- Compare Desc full_seq get_skimmers ## [21] Logic months Ops rbind2 recode ## [26] rescale scale_type ## see &#39;?methods&#39; for accessing help and source code Aquí se agrega un elemento de texto a un vector numérico, R por defecto lo va a cambiar a texto, que se revisa con mode(x), y específicamente si es numérico con is.numeric(x). Para cambiarlo de nuevo a numérico se usa as.numeric(x). x[6]&lt;-&quot;NA&quot; x ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;NA&quot; mode(x) ## [1] &quot;character&quot; is.numeric(x) ## [1] FALSE x2 &lt;- as.numeric(x) x2 ## [1] 1 2 3 4 5 NA 2.8 Guardando el espacio de trabajo Una vez se han generado objetos estos pueden ser guardados para compartir con otra gente o cargar en otra sesión para no tener que volver a generarlos. save.image(&quot;introR.rdata&quot;) load(&quot;introR.rdata&quot;) 2.9 Importando/cargando datos La mejor opción para importar datos es usar import del paquete rio (Chan &amp; Leeper, 2018). Uno simplemente ocupa darle la dirección del archivo que se quiere importar y la función inteligentemente escoge la forma para importarlo. Si se quiere importar un documento de Excel que contiene varias hojas, se usa import_list, donde el resultado es una lista con las diferentes hojas. El argumento setclass = 'tibble' se usa para definir que el objeto creado sea un tibble y no un DataFrame. data(&quot;airquality&quot;) head(airquality) ## # A tibble: 6 x 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 41 190 7.4 67 5 1 ## 2 36 118 8 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 # dat1 &lt;- import(file.choose(), setclass = &#39;tibble&#39;) dat1 &lt;- import(&quot;data/LungCapData2.csv&quot;, setclass = &#39;tibble&#39;) titanic &lt;- import(&quot;data/titanic.csv&quot;, setclass = &#39;tibble&#39;) lista_datos = import_list(&#39;data/datasets.xlsx&#39;, setclass = &#39;tibble&#39;) 2.10 Exportando datos Usar export del paquete rio. Se pueden exportar diferentes formatos. Si se exporta una lista con nombres a un Excel, cada entrada de la lista aparece en una hoja diferente. export(airquality, &quot;data/airquality.csv&quot;) export(list(airquality = airquality, mpg = mpg, gss = gss_cat), &quot;data/datasets.xlsx&quot;) 2.11 Inspeccionando los datos Aquí se muestran funciones básicas para explorar los datos y como acceder a ciertos datos en especifico. Dentro de las funciones más usadas están: head: Muestra las primeras 6 filas o elementos de un DataFrame o vector (no es necesario usar esto con un Tibble ya que el Tibble muestra las primeras 10 filas por defecto) tail: Muestra las ultimas 6 filas o elementos dim: Muestra la dimensión del objeto summary: Dependiendo del objeto esta función muestra diferentes cosas, en general siendo un resumen de los contenidos del objeto str: Muestra la estructura de los datos, indicando numero de variables y observaciones, así como el tipo de variables (de nuevo esto lo muestra un Tibble por defecto) names: Muestra los nombres de los objetos; en el caso de DataFrames y tibbles los nombres de las columnas, en el caso de una lista los nombres de los objetos dentro de la lista head(dat1) ## # A tibble: 6 x 5 ## Age LungCap Height Gender Smoke ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 9 3.12 57 female no ## 2 8 3.17 67.5 female no ## 3 7 3.16 54.5 female no ## 4 9 2.67 53 male no ## 5 9 3.68 57 male no ## 6 8 5.01 61 female no tail(dat1) ## # A tibble: 6 x 5 ## Age LungCap Height Gender Smoke ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 16 12.6 72 male yes ## 2 16 10.8 67 male yes ## 3 15 9.18 68 male yes ## 4 18 6.56 60 female no ## 5 16 6.38 63 female yes ## 6 15 7.63 66.5 female no dim(dat1) ## [1] 654 5 summary(dat1) ## Age LungCap Height Gender ## Min. : 3.000 Min. : 0.373 Min. :46.00 Length:654 ## 1st Qu.: 8.000 1st Qu.: 3.943 1st Qu.:57.00 Class :character ## Median :10.000 Median : 5.643 Median :61.50 Mode :character ## Mean : 9.931 Mean : 5.910 Mean :61.14 ## 3rd Qu.:12.000 3rd Qu.: 7.356 3rd Qu.:65.50 ## Max. :19.000 Max. :15.379 Max. :74.00 ## Smoke ## Length:654 ## Class :character ## Mode :character ## ## ## str(dat1) ## tibble [654 × 5] (S3: tbl_df/tbl/data.frame) ## $ Age : int [1:654] 9 8 7 9 9 8 6 6 8 9 ... ## $ LungCap: num [1:654] 3.12 3.17 3.16 2.67 3.69 ... ## $ Height : num [1:654] 57 67.5 54.5 53 57 61 58 56 58.5 60 ... ## $ Gender : chr [1:654] &quot;female&quot; &quot;female&quot; &quot;female&quot; &quot;male&quot; ... ## $ Smoke : chr [1:654] &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... names(dat1) ## [1] &quot;Age&quot; &quot;LungCap&quot; &quot;Height&quot; &quot;Gender&quot; &quot;Smoke&quot; Para acceder a elementos de un vector (de cualquier tipo) se usan los corchetes cuadrados [], con el numero de la posición del elemento dentro de estos. La posición se puede especificar por medio de un valor único, un rango (inicio:fin), o por medio de un vector de posiciones usando c(). t1[3:5] ## [1] 0.4 0.6 0.8 dias[2] ## [1] &quot;martes&quot; ordenado[c(2,5,7)] ## [1] Alto Medio Alto ## Levels: Bajo &lt; Medio &lt; Alto Para acceder a elementos de una lista se usa el corchete sencillo para extraer el elemento como tal, o doble corchete para extraer los contenido del elemento. lst[1] ## $Exp ## [1] 1 2 3 4 5 lst[[1]] ## [1] 1 2 3 4 5 lst[[1]][3] ## [1] 3 Para acceder a los datos de una matriz o tabla (DataFrame o tibble) se usan los corchetes cuadrados [,], donde el espacio antes de la coma se usa para seleccionar filas y el espacio después para seleccionar columnas (de acuerdo al numero, o al nombre para tablas). En tablas, para acceder a una columna como vector se pueden usar los dobles corchetes [[]] o el operados $. dat1[,1] # primer columna ## # A tibble: 654 x 1 ## Age ## &lt;int&gt; ## 1 9 ## 2 8 ## 3 7 ## 4 9 ## 5 9 ## 6 8 ## 7 6 ## 8 6 ## 9 8 ## 10 9 ## # … with 644 more rows dat1[1,] # primer fila ## # A tibble: 1 x 5 ## Age LungCap Height Gender Smoke ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 9 3.12 57 female no dat1[1,1] # elemento en primer fila y columna ## # A tibble: 1 x 1 ## Age ## &lt;int&gt; ## 1 9 dat1[,1:3] # columnas de la 1 a la 3 ## # A tibble: 654 x 3 ## Age LungCap Height ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 3.12 57 ## 2 8 3.17 67.5 ## 3 7 3.16 54.5 ## 4 9 2.67 53 ## 5 9 3.68 57 ## 6 8 5.01 61 ## 7 6 3.76 58 ## 8 6 2.24 56 ## 9 8 3.96 58.5 ## 10 9 3.83 60 ## # … with 644 more rows dat1[,c(1,3,5)] # columnas 1, 3, y 5 ## # A tibble: 654 x 3 ## Age Height Smoke ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 9 57 no ## 2 8 67.5 no ## 3 7 54.5 no ## 4 9 53 no ## 5 9 57 no ## 6 8 61 no ## 7 6 58 no ## 8 6 56 no ## 9 8 58.5 no ## 10 9 60 no ## # … with 644 more rows dat1[,c(&quot;Age&quot;,&quot;Height&quot;)] # columnas por nombre ## # A tibble: 654 x 2 ## Age Height ## &lt;int&gt; &lt;dbl&gt; ## 1 9 57 ## 2 8 67.5 ## 3 7 54.5 ## 4 9 53 ## 5 9 57 ## 6 8 61 ## 7 6 58 ## 8 6 56 ## 9 8 58.5 ## 10 9 60 ## # … with 644 more rows head(dat1[[&quot;Age&quot;]]) # columna como vector ## [1] 9 8 7 9 9 8 head(dat1$Age) # columna como vector ## [1] 9 8 7 9 9 8 dat1[&quot;Age&quot;] # columna por nombre ## # A tibble: 654 x 1 ## Age ## &lt;int&gt; ## 1 9 ## 2 8 ## 3 7 ## 4 9 ## 5 9 ## 6 8 ## 7 6 ## 8 6 ## 9 8 ## 10 9 ## # … with 644 more rows Para columnas de tipo factor se pueden revisar los niveles con la función levels. Si una columna es de tipo texto y se quiere cambiar a factor se reescribe la columna (tabla$columna) por medio de la función factor. Ademas, si se requieren ordenar los niveles, estos se puede hacer especificando el orden deseado con el argumento levels, usando el nombre de los elementos en el vector. levels(dat1$Smoke) ## NULL dat1$Smoke = factor(dat1$Smoke,levels = c(&quot;yes&quot;,&quot;no&quot;)) # reordenar niveles levels(dat1$Smoke) ## [1] &quot;yes&quot; &quot;no&quot; summary(dat1) ## Age LungCap Height Gender Smoke ## Min. : 3.000 Min. : 0.373 Min. :46.00 Length:654 yes: 65 ## 1st Qu.: 8.000 1st Qu.: 3.943 1st Qu.:57.00 Class :character no :589 ## Median :10.000 Median : 5.643 Median :61.50 Mode :character ## Mean : 9.931 Mean : 5.910 Mean :61.14 ## 3rd Qu.:12.000 3rd Qu.: 7.356 3rd Qu.:65.50 ## Max. :19.000 Max. :15.379 Max. :74.00 2.12 Descripciones generales (globales) Se muestran varias funciones que generan un resumen general de un vector, o tabla dependiendo del tipo de variable presente. Estas funciones se encuentran en los paquetes skimr (Waring et al., 2019), psych (Revelle, 2020), DescTools (Signorell, 2020), y summarytools (Comtois, 2019). set.seed(101) myvector = rnorm(n = 60,mean = 30,sd = 8) skim(myvector) Tabla 2.1: Data summary Name myvector Number of rows 60 Number of columns 1 _______________________ Column type frequency: numeric 1 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist data 0 1 29.11 7.42 11.45 24.06 29.78 34.57 41.42 ▂▅▇▇▆ describe(myvector) ## # A tibble: 1 x 13 ## vars n mean sd median trimmed mad min max range skew kurtosis ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 60 29.1 7.42 29.8 29.5 7.87 11.4 41.4 30.0 -0.499 -0.671 ## # … with 1 more variable: se &lt;dbl&gt; Desc(myvector) ## ------------------------------------------------------------------------------ ## myvector (numeric) ## ## length n NAs unique 0s mean meanCI ## 60 60 0 = n 0 29.10622 27.18840 ## 100.0% 0.0% 0.0% 31.02404 ## ## .05 .10 .25 median .75 .90 .95 ## 16.47568 18.28631 24.06407 29.78292 34.56941 37.43271 39.38988 ## ## range sd vcoef mad IQR skew kurt ## 29.97666 7.42400 0.25507 7.86821 10.50535 -0.49887 -0.67149 ## ## lowest : 11.44538, 13.41515, 13.59754, 16.62716, 18.12898 ## highest: 39.03847, 39.38978, 39.39173, 39.51883, 41.42204 dfSummary(myvector) %&gt;% view(method = &#39;render&#39;) Data Frame Summary myvector Dimensions: 60 x 1 Duplicates: 0 No Variable Stats / Values Freqs (% of Valid) Valid Missing 1 myvector [numeric] Mean (sd) : 29.1 (7.4) min 60 distinct values 60 (100%) 0 (0%) Generated by summarytools 0.9.4 (R version 3.6.0)2020-07-03 skim(airquality) Tabla 2.2: Data summary Name airquality Number of rows 153 Number of columns 6 _______________________ Column type frequency: numeric 6 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Ozone 37 0.76 42.13 32.99 1.0 18.00 31.5 63.25 168.0 ▇▃▂▁▁ Solar.R 7 0.95 185.93 90.06 7.0 115.75 205.0 258.75 334.0 ▅▃▅▇▅ Wind 0 1.00 9.96 3.52 1.7 7.40 9.7 11.50 20.7 ▂▇▇▃▁ Temp 0 1.00 77.88 9.47 56.0 72.00 79.0 85.00 97.0 ▂▃▇▇▃ Month 0 1.00 6.99 1.42 5.0 6.00 7.0 8.00 9.0 ▇▇▇▇▇ Day 0 1.00 15.80 8.86 1.0 8.00 16.0 23.00 31.0 ▇▇▇▇▆ describe(airquality) ## # A tibble: 6 x 13 ## vars n mean sd median trimmed mad min max range skew ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 116 42.1 33.0 31.5 37.8 25.9 1 168 167 1.21 ## 2 2 146 186. 90.1 205 190. 98.6 7 334 327 -0.419 ## 3 3 153 9.96 3.52 9.7 9.87 3.41 1.7 20.7 19 0.341 ## 4 4 153 77.9 9.47 79 78.3 8.90 56 97 41 -0.371 ## 5 5 153 6.99 1.42 7 6.99 1.48 5 9 4 -0.00234 ## 6 6 153 15.8 8.86 16 15.8 11.9 1 31 30 0.00260 ## # … with 2 more variables: kurtosis &lt;dbl&gt;, se &lt;dbl&gt; Desc(airquality) ## ------------------------------------------------------------------------------ ## Describe airquality (data.frame): ## ## data frame: 153 obs. of 6 variables ## 111 complete cases (72.5%) ## ## Nr ColName Class NAs Levels ## 1 Ozone integer 37 (24.2%) ## 2 Solar.R integer 7 (4.6%) ## 3 Wind numeric . ## 4 Temp integer . ## 5 Month integer . ## 6 Day integer . ## ## ## ------------------------------------------------------------------------------ ## 1 - Ozone (integer) ## ## length n NAs unique 0s mean meanCI ## 153 116 37 67 0 42.13 36.06 ## 75.8% 24.2% 0.0% 48.20 ## ## .05 .10 .25 median .75 .90 .95 ## 7.75 11.00 18.00 31.50 63.25 87.00 108.50 ## ## range sd vcoef mad IQR skew kurt ## 167.00 32.99 0.78 25.95 45.25 1.21 1.11 ## ## lowest : 1, 4, 6, 7 (3), 8 ## highest: 115, 118, 122, 135, 168 ## ------------------------------------------------------------------------------ ## 2 - Solar.R (integer) ## ## length n NAs unique 0s mean meanCI ## 153 146 7 117 0 185.93 171.20 ## 95.4% 4.6% 0.0% 200.66 ## ## .05 .10 .25 median .75 .90 .95 ## 24.25 47.50 115.75 205.00 258.75 288.50 311.50 ## ## range sd vcoef mad IQR skew kurt ## 327.00 90.06 0.48 98.59 143.00 -0.42 -1.00 ## ## lowest : 7, 8, 13, 14, 19 ## highest: 320, 322 (2), 323, 332, 334 ## ------------------------------------------------------------------------------ ## 3 - Wind (numeric) ## ## length n NAs unique 0s mean meanCI ## 153 153 0 31 0 9.96 9.39 ## 100.0% 0.0% 0.0% 10.52 ## ## .05 .10 .25 median .75 .90 .95 ## 4.60 5.82 7.40 9.70 11.50 14.90 15.50 ## ## range sd vcoef mad IQR skew kurt ## 19.00 3.52 0.35 3.41 4.10 0.34 0.03 ## ## lowest : 1.7, 2.3, 2.8, 3.4, 4.0 ## highest: 16.1, 16.6 (3), 18.4, 20.1, 20.7 ## ## heap(?): remarkable frequency (9.8%) for the mode(s) (= 11.5) ## ------------------------------------------------------------------------------ ## 4 - Temp (integer) ## ## length n NAs unique 0s mean meanCI ## 153 153 0 40 0 77.88 76.37 ## 100.0% 0.0% 0.0% 79.39 ## ## .05 .10 .25 median .75 .90 .95 ## 60.20 64.20 72.00 79.00 85.00 90.00 92.00 ## ## range sd vcoef mad IQR skew kurt ## 41.00 9.47 0.12 8.90 13.00 -0.37 -0.46 ## ## lowest : 56, 57 (3), 58 (2), 59 (2), 61 (3) ## highest: 92 (5), 93 (3), 94 (2), 96, 97 ## ------------------------------------------------------------------------------ ## 5 - Month (integer) ## ## length n NAs unique 0s mean meanCI ## 153 153 0 5 0 6.99 6.77 ## 100.0% 0.0% 0.0% 7.22 ## ## .05 .10 .25 median .75 .90 .95 ## 5.00 5.00 6.00 7.00 8.00 9.00 9.00 ## ## range sd vcoef mad IQR skew kurt ## 4.00 1.42 0.20 1.48 2.00 -0.00 -1.32 ## ## ## level freq perc cumfreq cumperc ## 1 5 31 20.3% 31 20.3% ## 2 6 30 19.6% 61 39.9% ## 3 7 31 20.3% 92 60.1% ## 4 8 31 20.3% 123 80.4% ## 5 9 30 19.6% 153 100.0% ## ## heap(?): remarkable frequency (20.3%) for the mode(s) (= 5, 7, 8) ## ------------------------------------------------------------------------------ ## 6 - Day (integer) ## ## length n NAs unique 0s mean meanCI ## 153 153 0 31 0 15.80 14.39 ## 100.0% 0.0% 0.0% 17.22 ## ## .05 .10 .25 median .75 .90 .95 ## 2.00 4.00 8.00 16.00 23.00 28.00 29.40 ## ## range sd vcoef mad IQR skew kurt ## 30.00 8.86 0.56 11.86 15.00 0.00 -1.22 ## ## lowest : 1 (5), 2 (5), 3 (5), 4 (5), 5 (5) ## highest: 27 (5), 28 (5), 29 (5), 30 (5), 31 (3) dfSummary(airquality) %&gt;% view(method = &#39;render&#39;) Data Frame Summary airquality Dimensions: 153 x 6 Duplicates: 0 No Variable Stats / Values Freqs (% of Valid) Valid Missing 1 Ozone [integer] Mean (sd) : 42.1 (33) min 67 distinct values 116 (75.82%) 37 (24.18%) 2 Solar.R [integer] Mean (sd) : 185.9 (90.1) min 117 distinct values 146 (95.42%) 7 (4.58%) 3 Wind [numeric] Mean (sd) : 10 (3.5) min 31 distinct values 153 (100%) 0 (0%) 4 Temp [integer] Mean (sd) : 77.9 (9.5) min 40 distinct values 153 (100%) 0 (0%) 5 Month [integer] Mean (sd) : 7 (1.4) min 5:31(20.3%)6:30(19.6%)7:31(20.3%)8:31(20.3%)9:30(19.6%) 153 (100%) 0 (0%) 6 Day [integer] Mean (sd) : 15.8 (8.9) min 31 distinct values 153 (100%) 0 (0%) Generated by summarytools 0.9.4 (R version 3.6.0)2020-07-03 2.13 Recursos Se presentan recursos a consultar para ahondar más en los temas presentados. Introducción a estadística con R Foundations of Statistics with R Referencias "],
["avanzado.html", "Capítulo 3 Funcionamiento avanzado de R 3.1 Introducción 3.2 Operadores lógicos 3.3 Operador de secuencia (Pipe operator) 3.4 Resumen de variables 3.5 Selección y renombre de variables 3.6 Filtrado de observaciones 3.7 Orden de acuerdo a variables 3.8 Creación de variables 3.9 Conteo de variables cualitativas 3.10 Tabla interactiva 3.11 Datos relacionales 3.12 Datos ordenados (Tidy data) 3.13 Datos anidados (Nesting) 3.14 Recursos", " Capítulo 3 Funcionamiento avanzado de R 3.1 Introducción En el capitulo anterior se mostraron de manera muy rápida las funciones básicas de R, esto porque de ahora en adelante se va a enfocar en el uso de funciones del tidyverse (Wickham et al., 2019). Este meta-paquete es uno que engloba a un montón de paquetes que se rigen bajo el paradigma de datos ordenados (tidy data). Datos ordenados quiere decir una observación por fila y cada variable en su columna. Lo que se cubre en este capitulo y más se puede encontrar en “R for Data Science” (Grolemund &amp; Wickham, 2016). En este capitulo se van a utilizar los siguientes paquetes: library(babynames) library(nycflights13) library(gapminder) library(DT) library(rio) library(tidyverse) Los tres primeros corresponden con conjuntos de datos. Así mismo se vuelven a importar los datos con que se venia trabajando: data(&quot;airquality&quot;) dat1 &lt;- import(&quot;data/LungCapData2.csv&quot;, setclass = &#39;tibble&#39;) titanic &lt;- import(&quot;data/titanic.csv&quot;, setclass = &#39;tibble&#39;) Los paquetes más usados e importantes del tidyverse son: dplyr: Manipulación de datos mediante selección, filtrado, creación, agrupamiento, arreglo y resumen de tablas tidyr: Convierte datos a ordenados y viceversa ggplot2: Paquete para crear gráficos de alta calidad y personalizables purrr: Brinda funciones para la racionero sobre vectores, listas y tablas forcats: Manipulación de datos categóricos (factores) stringr: Manipulación de datos de texto lubridate: Manipulación de fechas Un punto importante a destacar, es que en todas (sino la mayoría) de las funciones del tidyverse la tabla de datos es el primer argumento. 3.2 Operadores lógicos Operadores lógicos permiten hacer comparaciones o pruebas, donde usualmente el resultado es TRUE o FALSE. En términos numéricos TRUE equivale a 1 y FALSE a 0. Los operadores lógicos más usados son: &lt;, menor que &lt;=, menor o igual que ==, igual que !=, no igual que &gt;, mayor que &gt;=, mayor o igual que %in%, pertenencia Aquí se asigna 4 a x y se aplican varios de los operadores lógicos para cuestionar el contenido de el objeto x. x &lt;- 4 x == 4 ## [1] TRUE x != 4 ## [1] FALSE x &lt; 4 ## [1] FALSE x &lt;= 5 ## [1] TRUE Estos operadores se pueden usar igualmente en vectores. Recordando que el resultado de una operación lógica es TRUE o FALSE, el vector resultante es del tipo lógico. Si se desea acceder a los elementos que cumplen la condición hay que aplicar el vector lógico sobre el vector, donde va a extraer los elementos que coinciden con TRUE. Aquí se crea un vector numérico, y se tratan de extraer los valores menores a 70. Se muestra la forma básica de R (vector[condicion]) y una forma más directa e intuitiva que ofrece purrr. y &lt;- c(95, 90, 58, 87, 62, 75) y &lt; 70 ## [1] FALSE FALSE TRUE FALSE TRUE FALSE y[y &lt; 70] ## [1] 58 62 keep(y, ~.x &lt; 70) # purrr ## [1] 58 62 3.3 Operador de secuencia (Pipe operator) Uno de los operadores básicos en el tidyverse es el pipe operator (%&gt;%). Este permite que el resultado antes del operador sea la (primer) entrada de lo que se va a hacer después del operador (x %&gt;% f(y) es lo mismo que f(x,y)). El shortcut para escribirlo es: Mac: Cmd + Shift + M Windows: Ctrl + Shift + M La ventaja es que permite encadenar operaciones sin necesidad de salvar objetos intermedios y es más fácil de leer que encerrar operaciones una dentro de la otra. Se ejemplifica con un caso sencillo, donde se tiene un vector de errores y se quiere calcular el error cuadrático medio (RMSE por sus siglas en ingles). set.seed(26) e = runif(50,-10,10) round(sqrt(mean(e^2)),3) # forma clasica ## [1] 5.595 e %&gt;% .^2 %&gt;% mean() %&gt;% sqrt() %&gt;% round(3) # usando el operador ## [1] 5.595 Lo anterior se lee: agarre el vector e, eleve sus valores al cuadrado, después calcule la media, después sáquele la raíz y por ultimo redondeélo a 3 cifras. 3.4 Resumen de variables Para resumir datos la función principal es summarise, que colapsa una o varias columnas a un dato resumen. Muchas veces se tiene una variable agrupadora (factor) en los datos y se requiere calcular estadísticas por grupo, para esto se usa group_by junto con summarise. En group_by se pueden incluir más de una variable agrupadora. Funciones que ayudan a resumir datos son: first(n), el primer elemento del vector x last(x), el ultimo elemento del vector x nth(x, n), el elemento n del vector x n(), el numero de filas en una tabla u observaciones en un grupo n_distinct(x), el numero de valores únicos en el vector x dat1 %&gt;% group_by(Gender) %&gt;% summarise(mean(Age)) ## # A tibble: 2 x 2 ## Gender `mean(Age)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 female 9.84 ## 2 male 10.0 dat1 %&gt;% group_by(Gender,Smoke) %&gt;% summarise(mean(Age)) ## # A tibble: 4 x 3 ## Gender Smoke `mean(Age)` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 female no 9.37 ## 2 female yes 13.3 ## 3 male no 9.69 ## 4 male yes 13.9 dat1 %&gt;% group_by(Gender) %&gt;% summarise(N=n(), mean(Age), mean(Height), mean(LungCap)) ## # A tibble: 2 x 5 ## Gender N `mean(Age)` `mean(Height)` `mean(LungCap)` ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 318 9.84 60.2 5.35 ## 2 male 336 10.0 62.0 6.44 3.5 Selección y renombre de variables Para seleccionar columnas la función es select, la cual puede usar números o nombres y los nombres no tienen que llevar comillas. Esto también permite reordenar las columnas de una tabla. Para el caso de obtener una columna como vector se usa pull con el numero o nombre de la columna a jalar. Durante la selección se puede cambiar el nombre de la variable, o usando rename. Funciones que ayudan a seleccionar variables son: starts_with('X'), todas las columnas que empiezan con ‘X’, ends_with('X'), todas las columnas que terminan con ‘X’, contains('X'), todas las columnas que contienen ‘X’, matches('X'), todas las columnas que coinciden con ‘X’ Aquí se mezcla funciones de resumen y selección para crear resumen de las variables seleccionadas. dat1 %&gt;% group_by(Gender) %&gt;% select(Age,Height) %&gt;% summarise_all(mean) ## # A tibble: 2 x 3 ## Gender Age Height ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 9.84 60.2 ## 2 male 10.0 62.0 dat1 %&gt;% group_by(Gender) %&gt;% select(Age,Height) %&gt;% summarise_all(.funs = list(~mean(.), ~sd(.))) ## # A tibble: 2 x 5 ## Gender Age_mean Height_mean Age_sd Height_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 9.84 60.2 2.93 4.79 ## 2 male 10.0 62.0 2.98 6.33 Aquí se muestra la selección de variables, que resulta en un reordenamiento de las mismas. Así mismo se puede deseleccionar lo que no se quiere, usando - para indicar las columnas que no se quieren. airquality %&gt;% select(Temp,Wind,Ozone) ## # A tibble: 153 x 3 ## Temp Wind Ozone ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 67 7.4 41 ## 2 72 8 36 ## 3 74 12.6 12 ## 4 62 11.5 18 ## 5 56 14.3 NA ## 6 66 14.9 28 ## 7 65 8.6 23 ## 8 59 13.8 19 ## 9 61 20.1 8 ## 10 69 8.6 NA ## # … with 143 more rows dat1 %&gt;% select(-Smoke) ## # A tibble: 654 x 4 ## Age LungCap Height Gender ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 9 3.12 57 female ## 2 8 3.17 67.5 female ## 3 7 3.16 54.5 female ## 4 9 2.67 53 male ## 5 9 3.68 57 male ## 6 8 5.01 61 female ## 7 6 3.76 58 female ## 8 6 2.24 56 female ## 9 8 3.96 58.5 female ## 10 9 3.83 60 female ## # … with 644 more rows dat1 %&gt;% pull(Age) ## [1] 9 8 7 9 9 8 6 6 8 9 6 8 8 8 8 7 5 6 9 9 5 5 4 7 9 ## [26] 3 9 5 8 9 5 9 8 7 5 8 9 8 8 8 9 8 5 8 5 9 7 8 6 8 ## [51] 5 9 9 8 6 9 9 7 4 8 8 8 6 4 8 6 9 7 5 9 8 8 9 9 9 ## [76] 7 5 5 9 6 7 6 8 8 7 8 7 9 5 9 9 9 7 8 8 9 9 9 7 8 ## [101] 8 7 9 4 9 6 8 6 7 7 8 7 7 7 7 8 7 5 8 7 9 7 7 6 8 ## [126] 8 8 9 7 8 9 8 8 9 8 6 6 8 9 5 7 9 6 9 9 9 6 8 9 8 ## [151] 8 9 9 9 7 8 6 9 9 9 7 8 5 8 9 6 9 6 8 5 7 7 4 9 8 ## [176] 9 9 9 5 9 7 6 9 9 9 7 5 8 9 7 9 8 9 6 6 8 9 5 6 6 ## [201] 9 7 9 8 5 7 6 9 7 9 9 8 9 7 9 4 9 5 8 9 8 3 9 8 6 ## [226] 9 8 8 7 6 8 9 4 7 8 8 9 6 8 6 8 9 8 7 9 8 7 9 8 9 ## [251] 6 8 9 8 9 9 8 7 5 7 8 9 9 6 8 7 9 7 7 5 9 9 8 8 9 ## [276] 6 7 5 9 5 7 6 8 7 8 4 8 5 8 7 7 9 9 8 9 6 8 9 4 6 ## [301] 7 9 8 6 8 7 5 8 7 11 10 14 11 11 12 10 11 10 14 13 14 12 12 10 13 ## [326] 10 11 10 11 10 13 14 11 10 11 13 10 10 12 10 10 10 11 11 11 10 11 11 13 13 ## [351] 11 11 14 11 10 10 10 14 13 10 14 10 11 13 12 13 10 13 11 14 11 13 11 11 10 ## [376] 11 11 10 11 13 12 10 10 14 11 10 11 10 11 13 13 10 11 11 12 10 10 11 10 11 ## [401] 14 13 12 11 11 11 14 12 10 12 11 10 11 13 10 10 11 13 10 11 10 13 11 10 11 ## [426] 11 14 11 13 11 11 10 13 10 13 10 12 10 14 12 10 11 14 12 10 10 10 10 12 13 ## [451] 11 12 11 12 11 11 12 12 13 11 12 10 12 13 10 12 10 12 10 11 10 12 14 10 10 ## [476] 12 10 10 13 12 12 11 13 12 10 11 11 13 12 13 13 10 12 12 14 11 10 13 11 11 ## [501] 13 12 10 10 12 13 11 10 11 11 11 11 11 14 12 13 13 10 12 10 10 12 11 12 11 ## [526] 11 12 12 14 11 10 11 12 13 12 11 11 11 14 11 13 12 10 12 13 10 10 10 10 14 ## [551] 12 11 11 12 14 14 10 11 11 10 10 12 12 11 12 10 12 13 10 12 10 13 12 10 12 ## [576] 10 11 12 11 12 10 13 12 11 11 11 11 12 14 11 11 12 14 11 13 11 10 13 12 11 ## [601] 13 14 10 11 11 15 15 18 19 19 16 17 15 15 15 15 15 19 18 16 17 16 15 15 15 ## [626] 18 17 15 17 17 16 17 15 15 16 16 15 18 15 16 17 16 16 15 18 15 16 17 16 16 ## [651] 15 18 16 15 3.5.1 select helpers Se muestran diferentes usos y resultados de usar select helpers para facilidad de selección de columnas que cumplan con ciertos criterios. A su vez, se ejemplifica el renombrar las columnas durante la selección o usando rename (nuevo_nombre = nombre_actual). Un operador especial es everything() que selecciona todo; esto es útil cuando se quiere reordenar y poner una o varias columnas de primero y después el resto sin tener que escribir todos los nombres. select(storms, name:pressure) # columnas desde name hasta pressure ## # A tibble: 10,010 x 11 ## name year month day hour lat long status category wind pressure ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; &lt;int&gt; &lt;int&gt; ## 1 Amy 1975 6 27 0 27.5 -79 tropical d… -1 25 1013 ## 2 Amy 1975 6 27 6 28.5 -79 tropical d… -1 25 1013 ## 3 Amy 1975 6 27 12 29.5 -79 tropical d… -1 25 1013 ## 4 Amy 1975 6 27 18 30.5 -79 tropical d… -1 25 1013 ## 5 Amy 1975 6 28 0 31.5 -78.8 tropical d… -1 25 1012 ## 6 Amy 1975 6 28 6 32.4 -78.7 tropical d… -1 25 1012 ## 7 Amy 1975 6 28 12 33.3 -78 tropical d… -1 25 1011 ## 8 Amy 1975 6 28 18 34 -77 tropical d… -1 30 1006 ## 9 Amy 1975 6 29 0 34.4 -75.8 tropical s… 0 35 1004 ## 10 Amy 1975 6 29 6 34 -74.8 tropical s… 0 40 1002 ## # … with 10,000 more rows storms %&gt;% select(-c(name, pressure)) # columnas menos name y pressure ## # A tibble: 10,010 x 11 ## year month day hour lat long status category wind ts_diameter ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1975 6 27 0 27.5 -79 tropi… -1 25 NA ## 2 1975 6 27 6 28.5 -79 tropi… -1 25 NA ## 3 1975 6 27 12 29.5 -79 tropi… -1 25 NA ## 4 1975 6 27 18 30.5 -79 tropi… -1 25 NA ## 5 1975 6 28 0 31.5 -78.8 tropi… -1 25 NA ## 6 1975 6 28 6 32.4 -78.7 tropi… -1 25 NA ## 7 1975 6 28 12 33.3 -78 tropi… -1 25 NA ## 8 1975 6 28 18 34 -77 tropi… -1 30 NA ## 9 1975 6 29 0 34.4 -75.8 tropi… 0 35 NA ## 10 1975 6 29 6 34 -74.8 tropi… 0 40 NA ## # … with 10,000 more rows, and 1 more variable: hu_diameter &lt;dbl&gt; iris %&gt;% select(starts_with(&quot;Sepal&quot;)) # columnas que empiezan con &#39;Sepal&#39; ## # A tibble: 150 x 2 ## Sepal.Length Sepal.Width ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 ## 2 4.9 3 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5 3.6 ## 6 5.4 3.9 ## 7 4.6 3.4 ## 8 5 3.4 ## 9 4.4 2.9 ## 10 4.9 3.1 ## # … with 140 more rows iris %&gt;% select(ends_with(&quot;Width&quot;)) # columnas que terminan con &#39;Width&#39; ## # A tibble: 150 x 2 ## Sepal.Width Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.5 0.2 ## 2 3 0.2 ## 3 3.2 0.2 ## 4 3.1 0.2 ## 5 3.6 0.2 ## 6 3.9 0.4 ## 7 3.4 0.3 ## 8 3.4 0.2 ## 9 2.9 0.2 ## 10 3.1 0.1 ## # … with 140 more rows storms %&gt;% select(contains(&quot;d&quot;)) # columnas que contienen &#39;d&#39; ## # A tibble: 10,010 x 4 ## day wind ts_diameter hu_diameter ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 27 25 NA NA ## 2 27 25 NA NA ## 3 27 25 NA NA ## 4 27 25 NA NA ## 5 28 25 NA NA ## 6 28 25 NA NA ## 7 28 25 NA NA ## 8 28 30 NA NA ## 9 29 35 NA NA ## 10 29 40 NA NA ## # … with 10,000 more rows iris %&gt;% select(Especie = Species, everything()) # renombrar seleccion y seleccionar el resto ## # A tibble: 150 x 5 ## Especie Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.1 3.5 1.4 0.2 ## 2 setosa 4.9 3 1.4 0.2 ## 3 setosa 4.7 3.2 1.3 0.2 ## 4 setosa 4.6 3.1 1.5 0.2 ## 5 setosa 5 3.6 1.4 0.2 ## 6 setosa 5.4 3.9 1.7 0.4 ## 7 setosa 4.6 3.4 1.4 0.3 ## 8 setosa 5 3.4 1.5 0.2 ## 9 setosa 4.4 2.9 1.4 0.2 ## 10 setosa 4.9 3.1 1.5 0.1 ## # … with 140 more rows iris %&gt;% rename(Especie = Species) # renombrar columna ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Especie ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows 3.6 Filtrado de observaciones Para filtrar observaciones de acuerdo a uno a varios criterios se usa la función filter, así como operadores lógicos y funciones auxiliares. Funciones que ayudan a filtrar observaciones son las mismas de los Operadores lógicos. Dos de las funciones auxiliares más útiles son: between(x,left,right) que filtra observaciones para la variable x que se encuentren entre left (limite inferior) y right (limite superior); esta es más útil para variables numéricas, x %in% c(a,b,c) que filtra observaciones para la variable x que se encuentren en el vector c(a,b,c); esta es más útil para variables de texto o factor Se muestran diferentes ejemplos de como filtrar observaciones. Cuando se requiere que una observación cumpla varios criterios, estas condiciones se pueden separar por medio de comas (,), que es lo mismo que usar el operador lógico &amp;. Si se requiere una u otra condición se puede usar el operador lógico |, pero en ese caso y dependiendo de lo deseado es mejor usar between() o %in%. filter(airquality,Temp &gt; 85) ## # A tibble: 34 x 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 NA 273 6.9 87 6 8 ## 2 71 291 13.8 90 6 9 ## 3 39 323 11.5 87 6 10 ## 4 NA 259 10.9 93 6 11 ## 5 NA 250 9.2 92 6 12 ## 6 77 276 5.1 88 7 7 ## 7 97 267 6.3 92 7 8 ## 8 97 272 5.7 92 7 9 ## 9 85 175 7.4 89 7 10 ## 10 NA 291 14.9 91 7 14 ## # … with 24 more rows airquality %&gt;% filter(Temp &gt; 85) ## # A tibble: 34 x 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 NA 273 6.9 87 6 8 ## 2 71 291 13.8 90 6 9 ## 3 39 323 11.5 87 6 10 ## 4 NA 259 10.9 93 6 11 ## 5 NA 250 9.2 92 6 12 ## 6 77 276 5.1 88 7 7 ## 7 97 267 6.3 92 7 8 ## 8 97 272 5.7 92 7 9 ## 9 85 175 7.4 89 7 10 ## 10 NA 291 14.9 91 7 14 ## # … with 24 more rows airquality %&gt;% filter(Temp &gt; 75, Wind &gt; 10) ## # A tibble: 38 x 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 45 252 14.9 81 5 29 ## 2 NA 264 14.3 79 6 6 ## 3 71 291 13.8 90 6 9 ## 4 39 323 11.5 87 6 10 ## 5 NA 259 10.9 93 6 11 ## 6 NA 332 13.8 80 6 14 ## 7 NA 322 11.5 79 6 15 ## 8 21 191 14.9 77 6 16 ## 9 13 137 10.3 76 6 20 ## 10 NA 98 11.5 80 6 28 ## # … with 28 more rows airquality %&gt;% filter(between(Temp, 70, 80)) ## # A tibble: 53 x 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 36 118 8 72 5 2 ## 2 12 149 12.6 74 5 3 ## 3 7 NA 6.9 74 5 11 ## 4 11 320 16.6 73 5 22 ## 5 115 223 5.7 79 5 30 ## 6 37 279 7.4 76 5 31 ## 7 NA 286 8.6 78 6 1 ## 8 NA 287 9.7 74 6 2 ## 9 NA 264 14.3 79 6 6 ## 10 NA 332 13.8 80 6 14 ## # … with 43 more rows airquality %&gt;% filter(Temp &gt; 75, Wind &gt; 10) %&gt;% select(Ozone,Solar.R) ## # A tibble: 38 x 2 ## Ozone Solar.R ## &lt;int&gt; &lt;int&gt; ## 1 45 252 ## 2 NA 264 ## 3 71 291 ## 4 39 323 ## 5 NA 259 ## 6 NA 332 ## 7 NA 322 ## 8 21 191 ## 9 13 137 ## 10 NA 98 ## # … with 28 more rows babynames %&gt;% filter(name %in% c(&quot;Acura&quot;, &quot;Lexus&quot;, &quot;Yugo&quot;)) ## # A tibble: 57 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1990 F Lexus 36 0.0000175 ## 2 1990 M Lexus 12 0.00000558 ## 3 1991 F Lexus 102 0.0000502 ## 4 1991 M Lexus 16 0.00000755 ## 5 1992 F Lexus 193 0.0000963 ## 6 1992 M Lexus 25 0.0000119 ## 7 1993 F Lexus 285 0.000145 ## 8 1993 M Lexus 30 0.0000145 ## 9 1994 F Lexus 381 0.000195 ## 10 1994 F Acura 6 0.00000308 ## # … with 47 more rows 3.7 Orden de acuerdo a variables arrange se usa para ordenar los datos de acuerdo a una o más variables, donde por defecto lo hace de manera ascendente, para ordenarlos de manera descendente se encierra la variable dentro de desc(var). Si se ordena por una variable numérica se hará de menor a mayor o viceversa, si se ordena por una variable factor se hará de acuerdo al orden de los niveles del factor, y si se ordena por una variable de texto se hará por orden alfabético. airquality %&gt;% arrange(Temp) ## # A tibble: 153 x 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 NA NA 14.3 56 5 5 ## 2 6 78 18.4 57 5 18 ## 3 NA 66 16.6 57 5 25 ## 4 NA NA 8 57 5 27 ## 5 18 65 13.2 58 5 15 ## 6 NA 266 14.9 58 5 26 ## 7 19 99 13.8 59 5 8 ## 8 1 8 9.7 59 5 21 ## 9 8 19 20.1 61 5 9 ## 10 4 25 9.7 61 5 23 ## # … with 143 more rows airquality %&gt;% arrange(desc(Temp)) ## # A tibble: 153 x 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 76 203 9.7 97 8 28 ## 2 84 237 6.3 96 8 30 ## 3 118 225 2.3 94 8 29 ## 4 85 188 6.3 94 8 31 ## 5 NA 259 10.9 93 6 11 ## 6 73 183 2.8 93 9 3 ## 7 91 189 4.6 93 9 4 ## 8 NA 250 9.2 92 6 12 ## 9 97 267 6.3 92 7 8 ## 10 97 272 5.7 92 7 9 ## # … with 143 more rows gss_cat %&gt;% arrange(marital) ## # A tibble: 21,483 x 9 ## year marital age race rincome partyid relig denom tvhours ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2000 No answ… 28 Other $10000 - 1… Ind,near dem Buddhi… Not appl… 2 ## 2 2006 No answ… NA White Not applic… Strong repu… Protes… Other NA ## 3 2006 No answ… NA White No answer Strong repu… None Not appl… 2 ## 4 2006 No answ… 63 White No answer Strong demo… None Not appl… NA ## 5 2006 No answ… 40 Other $20000 - 2… Not str dem… Protes… No denom… NA ## 6 2006 No answ… 45 White No answer No answer No ans… No answer NA ## 7 2006 No answ… NA White No answer No answer No ans… No answer NA ## 8 2008 No answ… 62 White Not applic… Strong demo… Protes… Episcopal NA ## 9 2008 No answ… 43 White No answer Independent Christ… No denom… 1 ## 10 2008 No answ… 50 White No answer Ind,near dem Protes… Other 4 ## # … with 21,473 more rows 3.8 Creación de variables Para crear o modificar variables se usa mutate. Algunas veces se requiere o desea categorizar una variable continua de acuerdo a ciertos criterios o puntos de quiebre; lo anterior puede realizarse por medio de lo que se conoce como if statements, donde una función que realiza la misma tarea pero de forma más eficiente es case_when. En el primer ejemplo se trabaja con la tabla del titanic, donde se tienen varias variables como texto (‘Pclass’, ‘Survived’, ‘Sex’) y se quieren convertir a factor, por lo que simplemente se re-definen estas variables. Este cambio se puede ver con glimpse para el antes y después, donde el tipo de variable cambia. glimpse(titanic) ## Rows: 891 ## Columns: 12 ## $ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ Survived &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, … ## $ Pclass &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, … ## $ Name &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bradley (F… ## $ Sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;ma… ## $ Age &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14,… ## $ SibSp &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, … ## $ Parch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, … ## $ Ticket &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;113803&quot;, &quot;3… ## $ Fare &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625… ## $ Cabin &lt;chr&gt; &quot;&quot;, &quot;C85&quot;, &quot;&quot;, &quot;C123&quot;, &quot;&quot;, &quot;&quot;, &quot;E46&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;G6&quot;, &quot;… ## $ Embarked &lt;chr&gt; &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S… titanic = titanic %&gt;% mutate(Pclass = as_factor(Pclass), Survived = as_factor(Survived), Sex = as_factor(Sex)) glimpse(titanic) ## Rows: 891 ## Columns: 12 ## $ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ Survived &lt;fct&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, … ## $ Pclass &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, … ## $ Name &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bradley (F… ## $ Sex &lt;fct&gt; male, female, female, female, male, male, male, male, fem… ## $ Age &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14,… ## $ SibSp &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, … ## $ Parch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, … ## $ Ticket &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;113803&quot;, &quot;3… ## $ Fare &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625… ## $ Cabin &lt;chr&gt; &quot;&quot;, &quot;C85&quot;, &quot;&quot;, &quot;C123&quot;, &quot;&quot;, &quot;&quot;, &quot;E46&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;G6&quot;, &quot;… ## $ Embarked &lt;chr&gt; &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S… Se pueden crear variables nuevas que dependen de otra en la tabla. En el ejemplo se calcula la altura en centímetros a partir de la altura en pulgadas (1 pulgada = 2.54 cm) dat1 %&gt;% mutate(Altura = Height*2.54) ## # A tibble: 654 x 6 ## Age LungCap Height Gender Smoke Altura ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 9 3.12 57 female no 145. ## 2 8 3.17 67.5 female no 171. ## 3 7 3.16 54.5 female no 138. ## 4 9 2.67 53 male no 135. ## 5 9 3.68 57 male no 145. ## 6 8 5.01 61 female no 155. ## 7 6 3.76 58 female no 147. ## 8 6 2.24 56 female no 142. ## 9 8 3.96 58.5 female no 149. ## 10 9 3.83 60 female no 152. ## # … with 644 more rows En el tercer ejemplo se re define la variable ‘Month’ pasándola a factor donde se le cambian las etiquetas a algo más explicito. A su vez, se define una nueva variable condicionada en los valores de otra (sensación dependiendo del valor de la temperatura). Aquí se ejemplifica case_when, donde la estructura es: case_when(condicion1 ~ resultado1, condicion2 ~ resultado2, T ~ resultado3) airq = airquality %&gt;% mutate(Month = factor(Month, levels = 5:9, labels = c(&quot;Mayo&quot;, &quot;Junio&quot;, &quot;Julio&quot;, &quot;Agosto&quot;, &quot;Setiembre&quot;)), Sensation = case_when(Temp &lt; 60 ~ &#39;Cold&#39;, Temp &lt; 70 ~ &#39;Cool&#39;, Temp &lt; 85 ~ &#39;Warm&#39;, T ~ &#39;Hot&#39;) %&gt;% as.factor()) airq ## # A tibble: 153 x 7 ## Ozone Solar.R Wind Temp Month Day Sensation ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; ## 1 41 190 7.4 67 Mayo 1 Cool ## 2 36 118 8 72 Mayo 2 Warm ## 3 12 149 12.6 74 Mayo 3 Warm ## 4 18 313 11.5 62 Mayo 4 Cool ## 5 NA NA 14.3 56 Mayo 5 Cold ## 6 28 NA 14.9 66 Mayo 6 Cool ## 7 23 299 8.6 65 Mayo 7 Cool ## 8 19 99 13.8 59 Mayo 8 Cold ## 9 8 19 20.1 61 Mayo 9 Cool ## 10 NA 194 8.6 69 Mayo 10 Cool ## # … with 143 more rows airquality %&gt;% as_tibble() ## # A tibble: 153 x 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 41 190 7.4 67 5 1 ## 2 36 118 8 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 ## 9 8 19 20.1 61 5 9 ## 10 NA 194 8.6 69 5 10 ## # … with 143 more rows 3.9 Conteo de variables cualitativas Para contar casos de variables discretas de una manera más expedita se puede usar count. Esta función realiza un agrupamiento (group_by) y resumen (summarise) a la vez. mpg %&gt;% count(manufacturer, year) ## # A tibble: 30 x 3 ## manufacturer year n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 audi 1999 9 ## 2 audi 2008 9 ## 3 chevrolet 1999 7 ## 4 chevrolet 2008 12 ## 5 dodge 1999 16 ## 6 dodge 2008 21 ## 7 ford 1999 15 ## 8 ford 2008 10 ## 9 honda 1999 5 ## 10 honda 2008 4 ## # … with 20 more rows 3.10 Tabla interactiva Este es un ejemplo de como convertir una tabla estática a interactiva. Se usa el paquete DT (Xie et al., 2019) y la función datatable, donde se pueden definir otra serie de argumentos. Tiene la ventaja de que para columnas numéricas puedo filtrar por medio de sliders, y para columnas de facto puedo seleccionar los niveles. airq %&gt;% DT::datatable(filter = &#39;top&#39;, options = list(dom = &#39;t&#39;)) 3.11 Datos relacionales En caso de tener datos de observaciones en diferentes tablas, estas se pueden unir para juntar los datos en una única tabla (uniones de transformación), o relacionar para filtrar los datos de una tabla con respecto a otra (uniones de filtro). De manera general las uniones se van a realizar de acuerdo a las columnas que tengan el mismo nombre en ambas tablas. Si se desea especificar una columna en especifico se usa el argumento by = 'col'. Si el nombre difiere entre las tablas se define la unión de acuerdo a by = c('a' = 'b'), donde 'a' corresponde con el nombre de la columna en la primer tabla, y 'b' corresponde con el nombre de la columna en la segunda tabla. Esto aplica para todas las funciones de unión (*_join). 3.11.1 Uniones de transformación Estas uniones agregan columnas de una tabla a otra. Un tipo de unión es left_join(x, y), donde se unen los datos de la tabla de la derecha (y) a la de la izquierda (x) de acuerdo a una columna en común, y manteniendo todas las observaciones de x. flights %&gt;% left_join(airlines) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with 336,766 more rows, and 12 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, ## # name &lt;chr&gt; flights %&gt;% left_join(airports, c(&quot;dest&quot; = &quot;faa&quot;)) ## # A tibble: 336,776 x 26 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with 336,766 more rows, and 18 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, ## # name &lt;chr&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, alt &lt;int&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, ## # tzone &lt;chr&gt; Otro tipo de unión es inner_join(x, y), donde se mantienen observaciones que se encuentran en ambas tablas. df1 &lt;- tibble(x = c(1, 2), y = 2:1) df2 &lt;- tibble(x = c(1, 3), a = 10, b = &quot;a&quot;) df1 %&gt;% inner_join(df2) ## # A tibble: 1 x 4 ## x y a b ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 2 10 a Otro tipo de unión es full_join(x, y), donde se mantienen todas las observaciones de ambas tablas. df1 %&gt;% full_join(df2) ## # A tibble: 3 x 4 ## x y a b ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 2 10 a ## 2 2 1 NA &lt;NA&gt; ## 3 3 NA 10 a 3.11.2 Uniones de filtro Se filtran las observaciones de una tabla de acuerdo a si coinciden o no con las de otra tabla. Un tipo es semi_join(x, y), donde se mantienen todas las observaciones de x que coinciden con observaciones en y, pero sin agregar columnas de y. El opuesto seria anti_join(x, y), donde se eliminan todas las observaciones de x que coinciden con observaciones en y, pero sin agregar columnas de y. df1 &lt;- tibble(x = c(1, 1, 3, 4), y = 1:4) df2 &lt;- tibble(x = c(1, 1, 2), z = c(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;)) df1 %&gt;% semi_join(df2) ## # A tibble: 2 x 2 ## x y ## &lt;dbl&gt; &lt;int&gt; ## 1 1 1 ## 2 1 2 df1 %&gt;% anti_join(df2) ## # A tibble: 2 x 2 ## x y ## &lt;dbl&gt; &lt;int&gt; ## 1 3 3 ## 2 4 4 3.12 Datos ordenados (Tidy data) 3.12.1 Formatos largo y ancho Los datos ordenados corresponden con cada variable en su columna, cada fila corresponde con una observación, y en las celdas van los valores correspondientes. Esto corresponde con un formato largo (Figura 3.1). Figura 3.1: Estructura e ideología de datos ordenados (Grolemund &amp; Wickham, 2016). El ejemplo que se muestra a continuación no esta ordenado. La tabla tiene 3 variables pero no definidas correctamente. Una variable seria el país, otra seria el año (las columnas), y la tercera seria el numero de casos (las celdas). Esto se conoce como datos en formato ancho (En algunos casos puede ser necesario este formato, pero en la mayoría de ocasiones se prefiere el formato largo). casos &lt;- tribble( ~pais, ~&quot;2011&quot;, ~&quot;2012&quot;, ~&quot;2013&quot;, &quot;FR&quot;, 7000, 6900, 7000, &quot;DE&quot;, 5800, 6000, 6200, &quot;US&quot;, 15000, 14000, 13000 ) Para pasar de un formato ancho a largo, se usa la función pivot_longer(cols, names_to, values_to), donde cols son las columnas a agrupar en una sola, names_to es el nombre que se le va a dar a la columna que va a contener las columnas a agrupar, y values_to es el nombre que se le va a dar a la columna que va a contener los valores de las celdas y que corresponden con una variable. En este caso se van a agrupar todas las columnas menos el país, se le va a llamar ‘anho’ y lo que estaba en las celdas pasa a ser la columna ‘casos’. casos_tidy = casos %&gt;% pivot_longer(cols = -pais, names_to = &#39;anho&#39;, values_to = &#39;casos&#39;) casos_tidy ## # A tibble: 9 x 3 ## pais anho casos ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 FR 2011 7000 ## 2 FR 2012 6900 ## 3 FR 2013 7000 ## 4 DE 2011 5800 ## 5 DE 2012 6000 ## 6 DE 2013 6200 ## 7 US 2011 15000 ## 8 US 2012 14000 ## 9 US 2013 13000 De igual manera se puede volver al formato ancho con pivot_wider(id_cols, names_from, values_from), donde id_cols es una columna que identifica a cada observación, names_from es la columna a usar para nuevas columnas, y values_from es la columna donde están los valores a poner en las celdas. casos_tidy %&gt;% pivot_wider(id_cols = pais, names_from = anho, values_from = casos) ## # A tibble: 3 x 4 ## pais `2011` `2012` `2013` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FR 7000 6900 7000 ## 2 DE 5800 6000 6200 ## 3 US 15000 14000 13000 3.12.2 Separar y unir Otro caso de datos no ordenados es cuando una columna contiene 2 o más datos, por lo que es necesario separar cada dato en un su propia columna. En el ejemplo la columna ‘tasa’ corresponde con ‘casos’ y ‘poblacion’, por lo que hay que separarla. La función separate tiene el argumento into que corresponde con un vector de texto donde se deben definir los nombres de las columnas resultantes. casos2 &lt;- tribble( ~pais, ~anho, ~tasa, &quot;Afghanistan&quot;, 2001, &#39;745/19987071&#39;, &quot;Brasil&quot;, 2001, &#39;37737/172006362&#39;, &quot;China&quot;, 2001, &#39;212258/1272915272&#39; ) casos2 %&gt;% separate(tasa, into = c(&#39;casos&#39;, &#39;poblacion&#39;)) ## # A tibble: 3 x 4 ## pais anho casos poblacion ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 2001 745 19987071 ## 2 Brasil 2001 37737 172006362 ## 3 China 2001 212258 1272915272 Por defecto separate va a separar la columna en cualquier carácter especial que encuentre. Si se quiere especificar se puede usar el argumento sep. casos2 %&gt;% separate(tasa, into = c(&#39;casos&#39;, &#39;poblacion&#39;), sep = &#39;/&#39;) ## # A tibble: 3 x 4 ## pais anho casos poblacion ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 2001 745 19987071 ## 2 Brasil 2001 37737 172006362 ## 3 China 2001 212258 1272915272 El tipo de columna resultante de separate es de texto, pero en algunos casos ese no es el tipo deseado, por lo que se le puede pedir a la función que trate de adivinar y convertir las columnas al tipo correcto por medio del argumento convert = TRUE. casos2_sep = casos2 %&gt;% separate(tasa, into = c(&#39;casos&#39;, &#39;poblacion&#39;), convert = T) casos2_sep ## # A tibble: 3 x 4 ## pais anho casos poblacion ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 2001 745 19987071 ## 2 Brasil 2001 37737 172006362 ## 3 China 2001 212258 1272915272 El unir columnas se hace por medio de unite, donde se le pasan, primero, el nombre de la nueva columna, y segundo los nombres de las columnas a unir, así como el carácter a usar para separar los datos. casos2_sep %&gt;% unite(tasa, casos, poblacion, sep = &#39;-&#39;) ## # A tibble: 3 x 3 ## pais anho tasa ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan 2001 745-19987071 ## 2 Brasil 2001 37737-172006362 ## 3 China 2001 212258-1272915272 3.13 Datos anidados (Nesting) Esta es una de las ventajas de los tibbles, donde una columna puede ser una lista, y como una lista puede contener lo que sea, esto permite flexibilidad en el análisis y manipulación de datos, como se va a ver en el próximo capitulo. Esto es muy usado junto con group_by, donde primero se agrupa la tabla y luego se crea una columna donde para cada grupo se va a tener su tabla única (las observaciones que corresponden con ese grupo) y diferente al resto. iris %&gt;% group_by(Species) %&gt;% nest() ## # A tibble: 3 x 2 ## Species data ## &lt;fct&gt; &lt;list&gt; ## 1 setosa &lt;tibble [50 × 4]&gt; ## 2 versicolor &lt;tibble [50 × 4]&gt; ## 3 virginica &lt;tibble [50 × 4]&gt; airq %&gt;% group_by(Month) %&gt;% nest() ## # A tibble: 5 x 2 ## Month data ## &lt;fct&gt; &lt;list&gt; ## 1 Mayo &lt;tibble [31 × 6]&gt; ## 2 Junio &lt;tibble [30 × 6]&gt; ## 3 Julio &lt;tibble [31 × 6]&gt; ## 4 Agosto &lt;tibble [31 × 6]&gt; ## 5 Setiembre &lt;tibble [30 × 6]&gt; 3.14 Recursos Se presentan recursos a consultar para ahondar más en los temas presentados. tidyverse DT ModernDive Libro que cubre diversos temas desde una perspectiva moderna. Modern R with the tidyverse Strings in R Para manipular caracteres. Referencias "],
["gráficos-1.html", "Capítulo 4 Gráficos 4.1 Introducción 4.2 Estáticos 4.3 Interactivos 4.4 Recursos", " Capítulo 4 Gráficos 4.1 Introducción En este capitulo se muestra como crear diferentes tipos de gráficos, tanto estáticos usando el paquete ggplot2 (Wickham, 2016; Wickham et al., 2020), como dinámicos usando los paquetes highcharter (Kunst, 2019), plotly (Sievert et al., 2020), y dygraphs (Vanderkam et al., 2018). En este capitulo se van a utilizar los siguientes paquetes: library(babynames) library(nycflights13) library(gapminder) library(dygraphs) library(highcharter) library(plotly) library(RColorBrewer) library(viridis) library(rio) library(cowplot) library(patchwork) library(tidymodels) library(tidyverse) Los tres primeros corresponden con conjuntos de datos. Así mismo se vuelven a importar y manipular los datos con que se venia trabajando: data(&quot;airquality&quot;) dat1 &lt;- import(&quot;data/LungCapData2.csv&quot;, setclass = &#39;tibble&#39;) titanic &lt;- import(&quot;data/titanic.csv&quot;, setclass = &#39;tibble&#39;) titanic = titanic %&gt;% mutate(Pclass = as_factor(Pclass), Survived = as_factor(Survived), Sex = as_factor(Sex)) airq = airquality %&gt;% mutate(Month = factor(Month, levels = 5:9, labels = c(&quot;Mayo&quot;, &quot;Junio&quot;, &quot;Julio&quot;, &quot;Agosto&quot;, &quot;Setiembre&quot;)), Sensation = case_when(Temp &lt; 60 ~ &#39;Cold&#39;, Temp &lt; 70 ~ &#39;Cool&#39;, Temp &lt; 85 ~ &#39;Warm&#39;, T ~ &#39;Hot&#39;) %&gt;% as.factor()) 4.2 Estáticos El paquete por excelencia, como se menciono al principio del capitulo, para crear gráficos en R es ggplot2. Este se basa en la gramática de gráficos (grammar of graphics), de ahí el gg en el nombre. La estructura básica de cualquier gráficos es: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;), stat = &lt;STAT&gt;, position = &lt;POSITION&gt;) + &lt;FACET_FUNCTION&gt; + &lt;SCALE_AESTHETIC_TYPE&gt; + &lt;THEME_FUNCTION&gt; donde las partes principales son ggplot y geom_*, el resto no son necesarias. Dentro de geom_* se mapean la variables de la tabla a los argumentos de la función (x, y, col, fill, size, shape, alpha). La idea básica es trabajar en capas para poder modificar el gráfico con mayor detalle y facilidad. 4.2.1 Histograma Este se utiliza para datos numéricos continuos. Dentro de geom_histogram hay 3 opciones para definir la discretizacion: bins = Numero de clases binwidth = El ancho de las clases breaks = Un vector con los puntos donde separar los datos El primer gráfico (Figura 4.1) muestra el resultado de geom_histogram por defecto. Aquí se esta graficando la temperatura (‘Temp’) de la tabla ‘airquality’ en el eje x, es el eje que hay que usar en los histogramas. p = ggplot(data = airquality, mapping = aes(x=Temp)) + geom_histogram() p Figura 4.1: Histograma básico La apariencia del gráfico anterior se puede mejorar usando los argumentos col y fill, donde el primero corresponde con el color del borde de las barras, y el segundo con el relleno de las barras (Figura 4.2). ggplot(airquality,aes(x=Temp)) + geom_histogram(bins = 20,col=&quot;black&quot;,fill=&quot;blue&quot;) Figura 4.2: Histograma modificando la cantidad de barras y apariencia El eje x, sobre el cual estamos graficando los datos, se puede modificar con más detalle usando las funciones scale_x_*, donde en este caso usamos scale_x_continuous() por estar trabajando con datos continuos (Figura 4.3). El primer argumento de estas funciones es el nombre que se le quiere dar al eje, y en el caso siguiente, se modifican las etiquetas del eje con el argumento labels, donde se puede usar una de tantas funciones que se encuentran en el paquete scales. ggplot(airquality,aes(x=Temp)) + geom_histogram(binwidth = 2,col=&quot;black&quot;,fill=&quot;blue&quot;) + scale_x_continuous(&#39;Temperatura&#39;, labels = label_number(suffix = &#39; ºC&#39;)) Figura 4.3: Histograma modificando la apariencia del eje x Otra de las capas que se puede usar son las facetas o paneles, que permiten separar un tipo de gráfico, en este caso un histograma, en diferentes gráficos de acuerdo a otra variable, por lo general categórica (Figura 4.4). La función para esto es facet_wrap(~ variable). Esta función es útil para una variable, para dos o más variables es mejor usar facet_grid(filas ~ cols) (Ver Figuras 4.12 y 4.13). ggplot(airquality,aes(x=Temp)) + geom_histogram(bins = 20,col=&quot;black&quot;,fill=&quot;blue&quot;) + facet_wrap(~ Month) Figura 4.4: Histograma en paneles Se venían rellenando las barras todas de un mismo color, por lo que el argumento fill se pone fuera del aes(). Si se quiere rellenar las barras de acuerdo al conteo o densidad es necesario insertar dentro del aes() de geom_histogram el fill=after_stat(.), donde el punto (.) puede corresponder con el conteo (count) o la densidad (density). Así como se modifica el eje x anteriormente con scale_x_*, se puede modificar el relleno con scale_fill_*. En este caso (Figura 4.5) se usa scale_fill_distiller() para usar una de las paletas disponibles en el paquete RColorBrewer (Neuwirth, 2014), donde se tiene que especificar el nombre de la paleta de colores a usar, en este caso palette = 'YlOrRd'. ggplot(airquality,aes(x=Temp)) + geom_histogram(bins = 20,aes(fill=after_stat(count)),col=&quot;black&quot;) + scale_fill_distiller(palette = &#39;YlOrRd&#39;) Figura 4.5: Histograma con relleno de acuerdo al conteo Las paletas disponibles se observan en la Figura 4.6, donde el primer bloque de colores corresponde con las paletas secuenciales (seq), el segundo bloque con las paletas cualitativas (qual), y el tercer bloque con las paletas divergentes (div). Figura 4.6: Paletas disponibles en RColorBrewer Así como se puede modificar el relleno de las barras, se puede modificar que el eje y no corresponda con el conteo sino con la densidad (Figura 4.7), para esto es necesario insertar dentro del aes() de geom_histogram el y=after_stat(density), y esto es necesario si se quiere agregar la curva de densidad de los datos (geom_density) para ver su distribución. ggplot(airquality,aes(x=Temp)) + geom_histogram(bins = 20,aes(y=after_stat(density)), col=&quot;black&quot;,fill=&quot;blue&quot;) + geom_density(col=&quot;red&quot;) Figura 4.7: Histograma mostrando la densidad en el eje y en vez del conteo, con la curva de densidad superpuesta La densidad no es lo mismo que la frecuencia relativa (Figura 4.8). La densidad es el ajuste a la función de densidad de los datos donde la integral suma a 1, la frecuencia relativa es el porcentaje de observaciones por clase. Para esto es necesario insertar dentro del aes() de geom_histogram el y=after_stat(count/sum(count)). ggplot(airquality,aes(x=Temp)) + geom_histogram(bins = 20,aes(y=after_stat(count/sum(count))), col=&quot;black&quot;,fill=&quot;blue&quot;) Figura 4.8: Histograma mostrando la frecuencia relativa en el eje y en vez del conteo o la densidad A veces se quiere representar la frecuencia acumulada (absoluta o relativa), con lo que el histograma va a presentar una tendencia ascendente hasta llegar al total de observaciones (absoluta) o hasta uno (relativa). Para lograr esto se hace uso de la función cumsum al definir aes(y), como se muestra en las Figuras 4.9 y 4.10, para frecuencias absolutas y relativas respectivamente. ggplot(airquality,aes(x=Temp)) + geom_histogram(bins = 20,aes(y=after_stat(cumsum(count))), col=&quot;black&quot;,fill=&quot;blue&quot;) Figura 4.9: Histograma mostrando la frecuencia absoluta acumulada. En este caso \\(N = 153\\). ggplot(airquality,aes(x=Temp)) + geom_histogram(bins = 20,aes(y=after_stat(cumsum(count/sum(count)))), col=&quot;black&quot;,fill=&quot;blue&quot;) Figura 4.10: Histograma mostrando la frecuencia relativa acumulada Combinando lo aprendido hasta ahora se pueden modificar el relleno junto con el eje y, así como agregar la curva de densidad, para generar un gráfico que brinda más información (Figura 4.11). ggplot(airquality,aes(x=Temp)) + geom_histogram(bins = 20, aes(y=after_stat(density),fill=after_stat(count)), col=&quot;black&quot;) + geom_density(col=&quot;red&quot;) Figura 4.11: Histograma mostrando la densidad en el eje y en vez del conteo, con la curva de densidad superpuesta, y relleno de acuerdo al conteo Estos últimos dos gráficos agregan más cosas que permiten crear gráficos más complejos e informativos sin mucho esfuerzo, incluyendo paneles de acuerdo a dos variables usando facet_grid. El primero (Figura 4.12) agrega el relleno de acuerdo a una variable categórica, así como un paneleo por dos variables definiendo filas y columnas. El segundo (Figura 4.13) construye sobre el primero pero modificando las etiquetas de los paneles usando el argumento labeller, donde se define labeller(variable = vector con nombres), ademas se cambia el relleno con scale_fill_viridis y se modifican las etiquetas con un vector (c('actual' = 'nuevo')). El vector con nombres de labeller y el vector de etiquetas llevan la estructura c('actual' = 'nuevo'), donde ‘actual’ es el valor que tiene la variable y ‘nuevo’ es el nombre que se quiere aparezca en el gráfico. ggplot(titanic, aes(x = Age, fill = Survived)) + facet_grid(Sex ~ Pclass) + geom_density(alpha = 0.5) Figura 4.12: Curva de densidad haciendo uso de varias variables categóricas ggplot(titanic, aes(x = Age, fill = Survived)) + facet_grid(Sex ~ Pclass, labeller = labeller(Sex = c(&#39;male&#39;=&#39;Masculino&#39;, &#39;female&#39;=&#39;Femenino&#39;), Pclass = c(&#39;1&#39;=&#39;1era&#39;,&#39;2&#39;=&#39;2nda&#39;,&#39;3&#39;=&#39;3era&#39;))) + geom_density(alpha = 0.5) + scale_fill_viridis_d(&#39;Sobrevivió&#39;, labels = c(&#39;1&#39;=&#39;Si&#39;,&#39;0&#39;=&#39;No&#39;)) Figura 4.13: Versión mejorada del gráfico anterior 4.2.2 Barras Este se utiliza para datos categóricos. Dentro de geom_bar el argumento position puede tener cualquiera de estos tres valores: stack: Apila barras una encima de otra dodge: Pone barras de manera adyacente fill: Las barras tienen la misma altura, normalizadas a 1 (proporciones) geom_bar hace el conteo de clases, en caso de tener ya el conteo hecho se usa geom_col. De manera general se puede pasar solo una variable para realizar el conteo, pero este tipo de gráficos es más útil cuando se pueden agregar otras variables categóricas. En el primer ejemplo (Figura 4.14) se hace el conteo por genero, y se rellena por si fuman o no, lo que brinda una visión de la cantidad (o proporción) de hombres y mujeres que fuman o no. En este ejemplo se utiliza position = &quot;fill&quot;, lo que hace que todas las barras tengan las misma altura y comprendan el rango de 0 a 1 en el eje y, lo que asemeja a una visión de proporciones. Adicionalmente, se modifica el eje y (scale_y_continuous) asignándole un nombre y cambiando las etiquetas a porcentaje (labels = label_percent()). ggplot(dat1, aes(Gender,fill=Smoke)) + geom_bar(position = &quot;fill&quot;) + scale_y_continuous(&#39;Proporción&#39;,labels = label_percent()) Figura 4.14: Gráfico de barras con el argumento de posición fill, para mostrar proporciones entre categorias El siguiente gráfico (Figura 4.15) es similar al primero, en que se grafican los mismos datos, pero de otra manera. Se usa position = &quot;dodge&quot;, lo que pone una barra a la par de la otra (esto para la categoría usada en el relleno); adicionalmente, se modifica el eje x (scale_x_discrete) asignándole un nombre y modificando los nombres de las etiquetas (labels); por ultimo, se modifica de forma manual el relleno (scale_fill_manual) asignándole un nombre, que es el que aparecer en la leyenda, las etiquetas (que deben tener el mismo orden de los niveles de la variable), y los valores (values) son los colores a usar para cada nivel. ggplot(dat1, aes(Gender,fill=Smoke)) + geom_bar(position = &quot;dodge&quot;) + scale_x_discrete(&#39;Genero&#39;, labels = c(&#39;Femenino&#39;,&#39;Masculino&#39;)) + scale_fill_manual(&#39;Fumado&#39;, labels = c(&#39;Si&#39;,&#39;No&#39;), values = c(&#39;darkred&#39;,&#39;green4&#39;)) Figura 4.15: Gráfico de barras con apariencia modificada y posición de las barras una a la par de la otra De igual manera se pueden generar paneles de acuerdo a una variable categórica (Figura 4.16). ggplot(titanic, aes(x = Sex, fill = Survived)) + facet_wrap(~ Pclass) + geom_bar() Figura 4.16: Gráfico de barras en paneles Cuando se gráfica una variable únicamente el orden de las barras va a estar en función del orden de los niveles (Figura 4.17), pero esta representación puede que no sea la más clara visualmente. Para corregir lo anterior se pueden reordenar los niveles de la variable (únicamente para el gráfico) de acuerdo a la frecuencia (de mayor a menor), usando fct_infreq del paquete forcats (Figura 4.18). gss_cat %&gt;% ggplot(aes(marital)) + geom_bar() Figura 4.17: Gráfico de barras básico con orden de barras de acuerdo al orden de los niveles gss_cat %&gt;% ggplot(aes(fct_infreq(marital))) + geom_bar() Figura 4.18: Gráfico de barras ordenado de acuerdo a la frecuencia de los niveles Los ejemplos anteriores usaban geom_bar, pero en el caso de que ya se tenga el conteo se puede usar geom_col. En este caso hay que especificar la variable a graficar en x y el conteo en y, el resto de modificaciones se pueden aplicar de igual manera a como se venia mostrando. En este ejemplo (Figura 4.19) debido a que los niveles corresponden con nombres largos y hay muchos, se usa el cambiar los ejes por medio de coord_flip, lo que va a poner en ‘y’ lo que estaba en ‘x’ y viceversa. mpg %&gt;% count(manufacturer, year) %&gt;% mutate(year = as.factor(year)) %&gt;% ggplot(aes(manufacturer,n,fill=year)) + geom_col(position = &#39;dodge&#39;) + coord_flip() + scale_fill_brewer(palette = &#39;Dark2&#39;) Figura 4.19: Gráfico de barras precontado y cambiando ejes para mayor claridad 4.2.3 Boxplot Este tipo se usa para datos numéricos continuos que normalmente se separan por una variable categórica. Los datos continuos se grafican en el eje y, por lo que hay que especificar esto explícitamente en el aes() (Figura 4.20), y si se quiere separar por una variable categórica, esta se asigna al eje x (Figura 4.21). ggplot(airq,aes(y=Temp)) + geom_boxplot() Figura 4.20: Gráfico boxplot básico ggplot(airq,aes(x = Month,y = Temp)) + geom_boxplot() Figura 4.21: Gráfico boxplot separado por variable categórica En el tercer ejemplo (Figura 4.22) se agregan un par de funciones que no se habían visto: labs y theme_bw. labs permite modificar los nombres de los ejes y estéticas (col, fill, etc.) sin tener que usar las funciones scale_*_*. theme_bw es uno de los tantos temas que vienen definidos y cambia la apariencia a un gráfico en blanco y negro, removiendo el fondo gris que en muchos casos no es lo mejor. ggplot(airq,aes(x = Month,y = Temp)) + geom_boxplot(fill=&quot;white&quot;,col=&quot;red&quot;) + labs(x=&quot;Mes&quot;,y=&quot;Temperatura&quot;) + theme_bw() Figura 4.22: Gráfico boxplot con apariencia modificada 4.2.4 Dispersión Estos aplican para datos numéricos continuos en ambos ejes. Un gráfico básico se muestra en el primer ejemplo (Figura 4.23). ggplot(airquality, aes(Ozone,Temp)) + geom_point() Figura 4.23: Gráfico de dispersión básico Para el caso de puntos se puede cambiar el tipo de icono con el argumento shape, este puede definirse de manera global para todos los puntos o de acuerdo a una variable categórica (Figura 4.24). ggplot(airq, aes(Ozone,Temp,shape=Month)) + geom_point() Figura 4.24: Gráfico de dispersión con la forma de los puntos de acuerdo a una variable categórica Una tarea común en gráficos de dispersión es agregar líneas de tendencia. Para agregar líneas de tendencia en ggplot2 se usa la función geom_smooth. Por defecto ajusta una curva loess, pero para cambiarlo se usa el argumento method = 'lm' (Figura 4.25), y para especificar una ecuación diferente a la regresión simple (y ~ x) se usa formula, donde y y x son genéricos (Figura 4.26), NO hay que poner el nombre de las variables que se esta graficando. ggplot(airquality, aes(Wind,Temp)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Figura 4.25: Gráfico de dispersión con línea de tendencia lineal ggplot(airquality, aes(Ozone,Temp)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y~poly(x,2)) Figura 4.26: Gráfico de dispersión con línea de tendencia polinomial En este caso (Figura 4.27) cuando se aplica el paneleo (facet_wrap) se obtiene una gráfico de dispersión con su respectiva línea de tendencia para cada uno de los niveles de la variable categórica. ggplot(airquality, aes(Wind,Temp)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + facet_wrap(~ Month) Figura 4.27: Gráfico de dispersión en paneles con línea de tendencia para cada panel 4.2.5 líneas Estos gráficos son una versión del gráfico de dispersión, donde el eje x corresponde con una variable continua que tiene cierta secuencia o patrón, por lo general tiempo o espacio. El primer gráfico (Figura 4.28) muestra como ha cambiado el uso del nombre ‘Max’ a lo largo del tiempo. Primero se filtran los datos para el nombre de interés, y se seleccionan las variables a usar en el gráfico. Como el eje y corresponde con proporción, se modifica para que muestre el porcentaje. babynames %&gt;% filter(name == &quot;Max&quot;) %&gt;% select(year, prop, sex) %&gt;% ggplot(mapping = aes(x = year, y = prop)) + geom_line(mapping = aes(color = sex)) + scale_y_continuous(labels = label_percent()) Figura 4.28: Gráfico de línea básico En el segundo gráfico de líneas (Figura 4.29) se grafica el numero de tormentas por año, de nuevo, realizando una manipulación de los datos para obtener la información que se desea desplegar. El ultimo gráfico (Figura 4.30) simplemente hace la separación de diferentes líneas de acuerdo a una variable categórica. storms %&gt;% group_by(year) %&gt;% summarize(n_storm = n_distinct(name)) %&gt;% ggplot() + geom_line(mapping = aes(x = year, y = n_storm)) Figura 4.29: Gráfico de línea básico con otros datos storms %&gt;% group_by(year,status) %&gt;% summarize(n_storm = n_distinct(name)) %&gt;% ggplot() + geom_line(mapping = aes(x = year, y = n_storm, col = status)) Figura 4.30: Gráfico de línea básico, con diferentes líneas de acuerdo a una variable categórica 4.2.6 Gráficos estadísticos Estos sirven para resumir los datos. Los ejemplos que aquí se muestran corresponden con el despliegue de intervalos de confianza de una variable numérica para diferentes niveles de una variable categórica. ggplot(airquality, aes(Month, Temp)) + stat_summary(fun.y = mean, geom = &quot;point&quot;, color = &quot;black&quot;) + stat_summary(fun.data = mean_cl_normal, geom = &quot;errorbar&quot;, width = 0.2) + theme_bw() Figura 4.31: Gráfico estadístico mostrando intervalo de confianza como barras de error ggplot(airquality, aes(Month, Temp)) + stat_summary(fun.data = mean_cl_normal, geom = &quot;pointrange&quot;, color = &quot;red&quot;, size=1) + theme_bw() Figura 4.32: Gráfico estadístico mostrando intervalo de confianza como punto y rango 4.2.7 Transformación de ejes En algunas ocasiones, dependiendo de la escala y rango que tenga los datos, una transformación de uno o ambos ejes sea necesaria, siendo los más típico para datos numéricos continuos. El ejemplo más típico es la transformacional logarítmica (natural o base 10), para los casos donde los datos comprenden varios ordenes de magnitud. En ggplot2 esto puede hacerse de dos maneras: scale_&lt;eje&gt;_&lt;tipo&gt;(trans = '#'), coord_trans(&lt;eje&gt; = '#') donde # corresponde con la transformacional a realizar, y puede tomar los siguientes valores: “asn”, “atanh”, “boxcox”, “date”, “exp”, “hms”, “identity”, “log”, “log10”, “log1p”, “log2”, “logit”, “modulus”, “probability”, “probit”, “pseudo_log”, “reciprocal”, “reverse”, “sqrt”, y “time”. Siendo las más comunes las resaltadas en negrita, donde “log” se refiere al logaritmo natural, “log10” al logaritmo base 10, y “reverse” para invertir el eje. La diferencia de usar scale_* o coord_trans radica en cuando se aplica la transformacional. Con scale_* la transformación es aplicada a los datos antes de ser graficados, por lo que se grafica son los datos transformados (Figura 4.33, B); con coord_trans la transformación se aplica después de ser graficados, o sea se aplica sobre el eje, modificando la apariencia y no los datos en si (Figura 4.33, C). La Figura 4.33, es una grilla de gráficos, la cual fue realizada por medio de cowplot (Wilke, 2019). Otro paquete para generar grillas de gráficos es patchwork (Pedersen, 2019), y este ofrece más flexibilidad con la manipulación y posicionamiento de los gráficos (Figura 4.34). En ambos casos, se ve claramente en el subgráfico A que el eje x tiene varios ordenes de magnitud. En el subgráfico B se aplico scale_x_continuous(trans = 'log10'), lo cual aplica una transformación logarítmica a los datos antes de plotearlos y por eso los diferentes valores. En el subgráfico C se aplico coord_trans(x = 'log10'), lo cual simplemente modifica la apariencia del eje, donde se observa esa escala logarítmica, manteniendo los valores originales; adicionalmente se cambian los valores del eje (scale_x_continuous(breaks = c(10,100,1000,10000))) para que no se vean tan apilados. cow1 = ggplot(msleep) + geom_point(aes(bodywt,sleep_total)) cow2 = ggplot(msleep) + geom_point(aes(bodywt,sleep_total)) + scale_x_continuous(trans = &#39;log10&#39;) cow3 = ggplot(msleep) + geom_point(aes(bodywt,sleep_total)) + coord_trans(x = &#39;log10&#39;) + scale_x_continuous(breaks = c(10,100,1000,10000)) plot_grid(cow1, cow2, cow3, ncol = 1, labels = &#39;AUTO&#39;) Figura 4.33: Ejemplo de transformación de ejes, en este caso solo el eje x, usando cowplot. A es el gráfico sin datos tranformados; B es el gráfico usando scale_*; C es el gráfico usando coord_trans cow1 / (cow2 | cow3) + plot_annotation(tag_levels = &#39;A&#39;) &amp; theme_bw() Figura 4.34: Ejemplo de transformación de ejes, en este caso solo el eje x, usando patchwork. A es el gráfico sin datos tranformados; B es el gráfico usando scale_*; C es el gráfico usando coord_trans 4.2.8 Limites de ejes (Zoom) Otra acción que tal vez se quiera realizar puede ser delimitar los valores mínimos y máximos de los ejes, ya sea para tener control sobre estos, o para realizar una especie de acercamiento (zoom) en una región del gráfico. La manera apropiada de realizar esto es por medio de coord_cartesian(*lim), donde *lim se refiere al eje x o y. la otra forma que tal vez aparezca por ahí, pero no da los resultados deseados es usando scale_&lt;eje&gt;_&lt;tipo&gt;(limits). En ambos casos se brinda un vector de mínimo y máximo. La Figura 4.35 muestra el gráfico inicial, y como afectan las diferentes funciones. De manera similar a la transformación, scale_* aplica los limites antes de graficar, por lo que los datos que caen fuera de esos limites son descartados y no ploteados (de ahí la advertencia). En cambio, coord_cartesian aplica los limites después de graficados los datos, por lo que simplemente es un cambio en la representación del eje y no en los datos. z1 = gss_cat %&gt;% ggplot(aes(y=marital)) + geom_bar() z2 = gss_cat %&gt;% ggplot(aes(y=marital)) + geom_bar() + scale_x_continuous(limits = c(0,5000)) z3 = gss_cat %&gt;% ggplot(aes(y=marital)) + geom_bar() + coord_cartesian(xlim = c(0,5000)) z1 / (z2 | z3) + plot_annotation(tag_levels = &#39;A&#39;) &amp; theme_bw() Figura 4.35: Ejemplo de modificar los límites del eje y en este caso. A es el gráfico original B es el gráfico usando scale_*; C es el gráfico usando coord_cartesian 4.2.9 Salvando gráficos Se muestran funciones para salvar gráficos, donde las extensiones más usadas son .png, .tiff, y .pdf. Por defecto ggsave salva el ultimo gráfico creado, a menos que se haya guardado el gráfico en un objeto y se le pase dicho objeto al argumento plot. El resto de argumentos son claros en lo que representan. Para el caso de un .pdf hay que remover el argumento type. ggsave(filename = &quot;figures/Testgg.png&quot;, plot = p, dpi = 300, width = 7, height = 4, units = &quot;in&quot;, type = &quot;cairo&quot;) ggsave(filename = &quot;figures/Testgg.pdf&quot;, plot = p, dpi = 300, width = 7, height = 4, units = &quot;in&quot;) 4.3 Interactivos Una vez sabiendo utilizar ggplot2 la forma más sencilla de hacer un gráfico interactivo es mediante plotly::ggplotly(). El paquete plotly (Sievert et al., 2020) se usa para gráficos interactivos y tiene una sintaxis un poco diferente a ggplot2, por lo que hay ciertos gráficos que no van a ser convertidos apropiadamente, pero la mayoría de gráficos debieran funcionar. Otros paquetes para gráficos interactivos son: highcarter: Sintaxis similar a ggplot, con ciertas limitantes, rbokeh: Gráficos interactivos generales, dygraphs: Series temporales, mapview y leaflet: Mapas En las siguientes secciones se va a recrear alguno de los gráficos anteriores para hacerlo interactivo con ggplotly, y se va a crear uno similar con highcharter. El ejercicio de entender como funciona highcharter queda a cargo del lector, en general los ejemplos y funciones son claras. Una característica de estos gráficos interactivos, cuando tienen leyenda, es que al hacer click sobre una de la entradas de la leyenda, esta serie de datos es escondida del gráfico, dejando visible únicamente lo otro. En el caso de highcharter los ejes se ajustan automáticamente, este no es el caso para plotly. 4.3.1 Histograma La versión interactiva de la Figura 4.11 se muestra en la Figura 4.36, la versión usando highcharter se muestra en la Figura 4.37. (ggplot(airquality,aes(x=Temp)) + geom_histogram(bins = 20, aes(y=after_stat(density),fill=after_stat(count)), col=&quot;black&quot;) + geom_density(col=&quot;red&quot;)) %&gt;% plotly::ggplotly() Figura 4.36: Gráfico interactivo de un histograma con plotly with(airq, hchist(Temp,color=&#39;red&#39;,name=&#39;Temp&#39;)) %&gt;% hc_xAxis(title = list(text = &#39;Temperatura&#39;)) %&gt;% hc_exporting(enabled=T) Figura 4.37: Gráfico interactivo de un histograma con highcharter 4.3.2 Barras La versión interactiva de la Figura 4.19 se muestra en la Figura 4.38, la versión usando highcharter se muestra en la Figura 4.39. En la versión de highcharter, si se cambia ‘bar’ por ‘column’, el gráfico cambia orientación. (mpg %&gt;% count(manufacturer, year) %&gt;% mutate(year = as.factor(year)) %&gt;% ggplot(aes(manufacturer,n,fill=year)) + geom_col(position = &#39;dodge&#39;) + coord_flip() + scale_fill_brewer(palette = &#39;Dark2&#39;)) %&gt;% plotly::ggplotly() Figura 4.38: Gráfico interactivo de barras con plotly mpg %&gt;% count(manufacturer, year) %&gt;% hchart(&#39;bar&#39;, hcaes(x = manufacturer, y = n, group = year)) %&gt;% hc_xAxis(title = list(text = &#39;Constructor&#39;)) %&gt;% hc_yAxis(title = list(text = &#39;Cantidad&#39;)) %&gt;% hc_exporting(enabled=T) Figura 4.39: Gráfico interactivo de barras con highcharter 4.3.3 Boxplot La versión interactiva de la Figura 4.21 se muestra en la Figura 4.40, la versión usando highcharter se muestra en la Figura 4.41. (ggplot(airq,aes(x = Month,y = Temp)) + geom_boxplot()) %&gt;% plotly::ggplotly() Figura 4.40: Gráfico interactivo boxplot con plotly with(airq, hcboxplot(x = Temp, var = Month)) %&gt;% hc_yAxis(title = list(text = &#39;Temperatura&#39;)) %&gt;% hc_xAxis(title = list(text = &#39;Mes&#39;)) %&gt;% hc_exporting(enabled=T) Figura 4.41: Gráfico interactivo boxplot con highcharter 4.3.4 Dispersión La versión interactiva de la Figura 4.25 se muestra en la Figura 4.42, la versión usando highcharter se muestra en la Figura 4.43, con el agregado de que muestra la ecuación de cada línea de tendencia. (ggplot(airquality, aes(Wind,Temp)) + geom_point() + geom_smooth(method = &quot;lm&quot;)) %&gt;% plotly::ggplotly() Figura 4.42: Gráfico interactivo de dispersión con plotly hchart(airq, &#39;scatter&#39;, hcaes(Wind, Temp, group=Month), regression = T) %&gt;% hc_xAxis(title = list(text = &#39;Viento&#39;)) %&gt;% hc_yAxis(title = list(text = &#39;Temperatura&#39;)) %&gt;% hc_colors(viridis(n_distinct(airq$Month))) %&gt;% hc_add_dependency(&#39;plugins/highcharts-regression.js&#39;) %&gt;% hc_exporting(enabled=T) Figura 4.43: Gráfico interactivo de dispersión con highcharter 4.3.5 líneas La versión interactiva de la Figura 4.30 se muestra en la Figura 4.44, la versión usando highcharter se muestra en la Figura 4.45, y la versión usando dygraphs se muestra en la Figura 4.46. En este ultimo caso es necesario que los datos estén en formato ancho, donde la primer columna corresponde con el eje x, y el resto de columnas corresponden con las series temporales/espaciales a graficar por separado. (storms %&gt;% group_by(year,status) %&gt;% summarize(n_storm = n_distinct(name)) %&gt;% ggplot() + geom_line(mapping = aes(x = year, y = n_storm, col = status))) %&gt;% plotly::ggplotly() Figura 4.44: Gráfico interactivo de líneas con plotly storms %&gt;% group_by(year,status) %&gt;% summarize(n_storm = n_distinct(name)) %&gt;% hchart(&#39;line&#39;, hcaes(year, n_storm, group = status)) %&gt;% hc_xAxis(title = list(text = &#39;Año&#39;)) %&gt;% hc_yAxis(title = list(text = &#39;Cantidad&#39;)) %&gt;% hc_exporting(enabled=T) %&gt;% hc_tooltip(shared=T,crosshairs=T, backgroundColor=&#39;rgba(247,247,247,0.5)&#39;,shadow=F) %&gt;% hc_chart(zoomType=&#39;x&#39;)%&gt;% hc_add_theme(hc_theme_google()) %&gt;% hc_plotOptions(line = list(marker = list( enabled = F, radius = 2 ) )) Figura 4.45: Gráfico interactivo de líneas con highcharter storms %&gt;% group_by(year,status) %&gt;% summarize(n_storm = n_distinct(name)) %&gt;% pivot_wider(names_from = status,values_from = n_storm) %&gt;% dygraph() %&gt;% dyAxis(&#39;x&#39;, label = &#39;Año&#39;) %&gt;% dyAxis(&#39;y&#39;, label = &#39;Cantidad&#39;) %&gt;% dyRangeSelector() Figura 4.46: Gráfico interactivo de líneas con dygraphs 4.3.6 Transformación de ejes La versión interactiva, usando highcharter, de la Figura 4.33 C se muestra en la Figura 4.47; la versión con ggplotly seria igual que las anteriores. hchart(msleep, &#39;scatter&#39;, hcaes(bodywt, sleep_total)) %&gt;% hc_xAxis(title = list(text = &#39;Body Weight&#39;), type = &#39;logarithmic&#39;) %&gt;% hc_yAxis(title = list(text = &#39;Sleep&#39;)) %&gt;% hc_chart(zoomType=&#39;xy&#39;) %&gt;% hc_add_theme(hc_theme_gridlight()) Figura 4.47: Gráfico interactivo con eje logarítmico 4.4 Recursos Se presentan recursos a consultar para ahondar más en los temas presentados. tidyverse ggplot2 Libro de ggplot2. highcharter rbokeh dygraphs Referencias "],
["iteración.html", "Capítulo 5 Iteración 5.1 Introducción 5.2 Iterando sobre un objeto 5.3 Iterando sobre dos objetos 5.4 Leyendo archivos y combinándolos 5.5 Datos anidados, caso 1 5.6 Datos anidados, caso 2 5.7 Recursos", " Capítulo 5 Iteración 5.1 Introducción En este capitulo se muestra como iterar funciones sobre diferentes objetos (vectores, tablas, listas). La idea de las iteraciones es ser más eficiente a la hora de realizar cálculos repetitivos. Se va a introducir al paquete purrr (Henry &amp; Wickham, 2019) que brinda funciones para realizar diferentes tareas que requieren iterar sobre 1 o más objetos. En este capitulo se van a utilizar los siguientes paquetes: library(gapminder) library(fs) library(rio) library(tidymodels) library(tidyverse) Así mismo se vuelven a importar y manipular los datos con que se venia trabajando: data(&quot;airquality&quot;) airq = airquality %&gt;% mutate(Month = factor(Month, levels = 5:9, labels = c(&quot;Mayo&quot;, &quot;Junio&quot;, &quot;Julio&quot;, &quot;Agosto&quot;, &quot;Setiembre&quot;)), Sensation = case_when(Temp &lt; 60 ~ &#39;Cold&#39;, Temp &lt; 70 ~ &#39;Cool&#39;, Temp &lt; 85 ~ &#39;Warm&#39;, T ~ &#39;Hot&#39;) %&gt;% as.factor()) 5.2 Iterando sobre un objeto La función básica de purrr es map(.x, .f, ...), donde .x es el objeto sobre el cual iterar (vector, tabla o lista), .f es la función o tarea a realizar durante la iteración, y ... son argumentos extra dependiendo de la función. Esta función (map) siempre va a resultar en una lista; existen variantes de esta que son especificas para cuando se conoce cual va a ser el tipo de dato de salida. Por ejemplo, map_dbl se usa cuando el resultado de la función es un numero con decimales. En el siguiente bloque de código se generan dos listas ficticias, ambas de 7 elementos, donde la primera corresponde con notas de estudiantes en pruebas durante un semestre, y la segunda son puntos extra para cada estudiante. set.seed(4101) n = 8 minima = 60 maxima = 100 exams &lt;- list( student1 = round(runif(n, minima, maxima)), student2 = round(runif(n, minima, maxima)), student3 = round(runif(n, minima, maxima)), student4 = round(runif(n, minima, maxima)), student5 = round(runif(n, minima, maxima)), student6 = round(runif(n, minima, maxima)), student7 = round(runif(n, minima, maxima)) ) extra_credit &lt;- list(10, 5, 0, 15, 5, 0, 5) Usando los datos generados anteriormente, se muestra la funcionalidad de varias de las funciones map_*. Estas funciones se pueden usar con el pipe operator (%&gt;%). El primer ejemplo muestra como con map se obtiene una lista de la nota media de los exámenes por estudiante. Se itera sobre la lista ‘exams’, y a cada elemento de la lista (en este caso vectores) se le calcula la media. map(exams, mean) # media ## $student1 ## [1] 79.25 ## ## $student2 ## [1] 75.875 ## ## $student3 ## [1] 80.25 ## ## $student4 ## [1] 85.125 ## ## $student5 ## [1] 84.875 ## ## $student6 ## [1] 79.625 ## ## $student7 ## [1] 81.875 En el segundo ejemplo se utiliza el pipe (%&gt;%) y una de las variantes de map (map_dbl), ya que lo que se va a calcular (nota máxima) se sabe es un numero con decimales. exams %&gt;% map_dbl(max) # nota maxima ## student1 student2 student3 student4 student5 student6 student7 ## 100 96 91 99 95 93 100 En el tercer ejemplo se itera sobre una tabla, donde en este caso la iteración es sobre las columnas. Recordemos que una tabla es una lista donde las columnas son los elementos de la lista. Lo que se quiere hacer es obtener el valor de la media para cada columna de la tabla ‘airq’. Al hacer esto encontramos dos situaciones; la primera que dice que hay un argumento no numérico o lógico (en este caso se refiere a las columnas ‘Month’ y ‘Sensation’ que son factor), por lo que al ser un factor no se le puede aplicar una función numérica; la segunda que hay valores ‘NA’ aun en columnas que son numéricas (‘Ozone’, ‘Solar.R’), esto porque en esas columnas hay NAs y por defecto la función mean no los remueve a la hora de hacer el calculo. airq %&gt;% map_dbl(mean) ## Ozone Solar.R Wind Temp Month Day Sensation ## NA NA 9.957516 77.882353 NA 15.803922 NA Por lo anterior, hay dos soluciones dependiendo de lo que se quiera resolver. Si solo se quiere lidiar con los ‘NA’, se puede agregar el argumento na.rm = T de la función mean, pero las columnas de tipo factor van a seguir estando presentes y dar ‘NA’ como resultado. airq %&gt;% map_dbl(mean, na.rm = T) ## Ozone Solar.R Wind Temp Month Day Sensation ## 42.129310 185.931507 9.957516 77.882353 NA 15.803922 NA La solución más adecuada en este caso es primero seleccionar las columnas de tipo numérico (select_if(is.numeric)) y a estas aplicarle el calculo de la media removiendo los ‘NA’. Esta ultima forma de aplicar la función es, a mi parecer, la más practica y clara. Se tiene que escribir la función empezando con el símbolo ~, esto le dice a purrr que lo que va a estar a al derecha va a ser una función, seguido de este símbolo se escribe la función de manera normal, con la excepción de que en el lugar donde iría típicamente el vector se pone .x, para decirle a purrr donde incrustar los elementos del objeto sobre el cual se esta iterando. airq %&gt;% select_if(is.numeric) %&gt;% map_dbl(~ mean(.x, na.rm = T)) ## Ozone Solar.R Wind Temp Day ## 42.129310 185.931507 9.957516 77.882353 15.803922 5.3 Iterando sobre dos objetos Los ejemplos anteriores se estaba iterando únicamente sobre un objeto. Para iterar sobre dos objetos (que tienen que tener la misma cantidad de elementos), existen las funciones map2_*, que tienen al estructura map2(.x, .y, .f, ...), donde .x es el primer objeto, .y es el segundo objeto, y .f es al función a utilizar sobre los dos objetos. Un ejemplo de esto es calcular la nota final de los estudiantes por medio de la media de los exámenes y agregarle el crédito extra. exams %&gt;% map2_dbl(extra_credit, ~ mean(.x) + .y) ## student1 student2 student3 student4 student5 student6 student7 ## 89.250 80.875 80.250 100.125 89.875 79.625 86.875 5.4 Leyendo archivos y combinándolos Un caso típico donde se desea iterar es leer varios archivos de texto que tienen el mismo formato y como combinarlos en una sola tabla para posterior manipulación. En este caso se usa la función dir_ls del paquete fs (Hester &amp; Wickham, 2020), donde se define la carpeta donde se encuentran los archivos (path) y con glob se define un patrón en el nombre de los archivos (en este caso todos los archivos empiezan con ‘datos_’). archivos &lt;- dir_ls(path = &#39;data&#39;, glob = &quot;*datos_*&quot;) archivos ## data/datos_cuarto_grado.csv data/datos_quinto_grado.csv ## data/datos_tercer_grado.csv file_info(archivos) ## # A tibble: 3 x 18 ## path type size permissions modification_time user group device_id ## &lt;fs::path&gt; &lt;fct&gt; &lt;fs:&gt; &lt;fs::perms&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 data/dato… file 1.79K rw-r--r-- 2019-07-01 16:52:58 maxi… staff 16777220 ## 2 data/dato… file 1.84K rw-r--r-- 2019-07-01 16:52:58 maxi… staff 16777220 ## 3 data/dato… file 1.81K rw-r--r-- 2019-07-01 16:52:58 maxi… staff 16777220 ## # … with 10 more variables: hard_links &lt;dbl&gt;, special_device_id &lt;dbl&gt;, ## # inode &lt;dbl&gt;, block_size &lt;dbl&gt;, blocks &lt;dbl&gt;, flags &lt;int&gt;, generation &lt;dbl&gt;, ## # access_time &lt;dttm&gt;, change_time &lt;dttm&gt;, birth_time &lt;dttm&gt; Una vez se tiene le objeto con los nombres de los archivos (‘archivos’), se puede proceder a realizar la iteración. Como estamos importando archivos de texto (.csv) usamos la función import del paquete rio. Primeramente podemos generar una lista donde iteramos sobre el objeto ‘archivos’ e importamos cada uno, para posteriormente “pegar” uno tras otro con la función bind_rows de dplyr. map(archivos, import) %&gt;% bind_rows() ## # A tibble: 144 x 5 ## fecha nombre matematica ingles matricula ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1/1/2015 Hernandez, Rodrigo 90 60 100 ## 2 1/2/2015 Hernandez, Rodrigo 85 70 100 ## 3 1/3/2015 Hernandez, Rodrigo 70 80 100 ## 4 1/4/2015 Hernandez, Rodrigo 75 85 100 ## 5 1/5/2015 Hernandez, Rodrigo 70 90 100 ## 6 1/6/2015 Hernandez, Rodrigo 66 90 100 ## 7 1/1/2015 Sanchez, Juan 60 80 102 ## 8 1/2/2015 Sanchez, Juan 70 80 102 ## 9 1/3/2015 Sanchez, Juan 80 90 102 ## 10 1/4/2015 Sanchez, Juan 85 85 102 ## # … with 134 more rows Con lo anterior logramos generar una tabla con todos los datos pero no sabemos cuales datos corresponden con cual archivo (y consecuentemente con que nivel). Para remediar lo anterior la función bind_rows tiene un argumento .id, al cual se le pasa el nombre de la columna que se quiere agregar mostrando el nombre del archivo al cual pertenece cada observación. archivos %&gt;% map_dfr(import, .id = &quot;archivo&quot;) ## # A tibble: 144 x 6 ## archivo fecha nombre matematica ingles matricula ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 data/datos_cuarto_grado… 1/1/2015 Hernandez, Rod… 90 60 100 ## 2 data/datos_cuarto_grado… 1/2/2015 Hernandez, Rod… 85 70 100 ## 3 data/datos_cuarto_grado… 1/3/2015 Hernandez, Rod… 70 80 100 ## 4 data/datos_cuarto_grado… 1/4/2015 Hernandez, Rod… 75 85 100 ## 5 data/datos_cuarto_grado… 1/5/2015 Hernandez, Rod… 70 90 100 ## 6 data/datos_cuarto_grado… 1/6/2015 Hernandez, Rod… 66 90 100 ## 7 data/datos_cuarto_grado… 1/1/2015 Sanchez, Juan 60 80 102 ## 8 data/datos_cuarto_grado… 1/2/2015 Sanchez, Juan 70 80 102 ## 9 data/datos_cuarto_grado… 1/3/2015 Sanchez, Juan 80 90 102 ## 10 data/datos_cuarto_grado… 1/4/2015 Sanchez, Juan 85 85 102 ## # … with 134 more rows La siguiente situación que podemos encontrar es que el nombre del archivo (o cualquier otra columna de la tabla) tiene más información de la necesaria, por lo que hay que separar los contenidos de la columna. Para esto usamos separate de tidyr para separar la columna en varias. En el caso de la columna ‘archivo’ podemos esperar tres columnas si especificamos el separador (sep = '_'), pero hay columnas que no ofrecen ninguna información (de las 3, la 1 y la 3, la 2 es la que tiene el nombre del nivel); para descartar estas columnas a la hora de separarlas se puede incluir NA en la posición de las columnas que se desea descartar. archivos %&gt;% map_dfr(import, .id = &quot;archivo&quot;) %&gt;% separate(archivo, into = letters[1:3], sep = &#39;_&#39;) ## # A tibble: 144 x 8 ## a b c fecha nombre matematica ingles matricula ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 data/dat… cuarto grado.c… 1/1/20… Hernandez, Rod… 90 60 100 ## 2 data/dat… cuarto grado.c… 1/2/20… Hernandez, Rod… 85 70 100 ## 3 data/dat… cuarto grado.c… 1/3/20… Hernandez, Rod… 70 80 100 ## 4 data/dat… cuarto grado.c… 1/4/20… Hernandez, Rod… 75 85 100 ## 5 data/dat… cuarto grado.c… 1/5/20… Hernandez, Rod… 70 90 100 ## 6 data/dat… cuarto grado.c… 1/6/20… Hernandez, Rod… 66 90 100 ## 7 data/dat… cuarto grado.c… 1/1/20… Sanchez, Juan 60 80 102 ## 8 data/dat… cuarto grado.c… 1/2/20… Sanchez, Juan 70 80 102 ## 9 data/dat… cuarto grado.c… 1/3/20… Sanchez, Juan 80 90 102 ## 10 data/dat… cuarto grado.c… 1/4/20… Sanchez, Juan 85 85 102 ## # … with 134 more rows archivos %&gt;% map_dfr(import, .id = &quot;archivo&quot;) %&gt;% separate(archivo, into = c(NA, &#39;grado&#39;, NA), sep = &#39;_&#39;) ## # A tibble: 144 x 6 ## grado fecha nombre matematica ingles matricula ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 cuarto 1/1/2015 Hernandez, Rodrigo 90 60 100 ## 2 cuarto 1/2/2015 Hernandez, Rodrigo 85 70 100 ## 3 cuarto 1/3/2015 Hernandez, Rodrigo 70 80 100 ## 4 cuarto 1/4/2015 Hernandez, Rodrigo 75 85 100 ## 5 cuarto 1/5/2015 Hernandez, Rodrigo 70 90 100 ## 6 cuarto 1/6/2015 Hernandez, Rodrigo 66 90 100 ## 7 cuarto 1/1/2015 Sanchez, Juan 60 80 102 ## 8 cuarto 1/2/2015 Sanchez, Juan 70 80 102 ## 9 cuarto 1/3/2015 Sanchez, Juan 80 90 102 ## 10 cuarto 1/4/2015 Sanchez, Juan 85 85 102 ## # … with 134 more rows Por ultimo, en este caso también se puede separar la columna ‘nombre’ en ‘apellido’ y ‘nombre’, usando los mismos principios anteriores. archivos %&gt;% map_dfr(import, .id = &quot;archivo&quot;) %&gt;% separate(archivo, into = c(NA, &#39;grado&#39;, NA), sep = &#39;_&#39;) %&gt;% separate(nombre, into = c(&#39;apellido&#39;, &#39;nombre&#39;), sep = &#39;, &#39;) ## # A tibble: 144 x 7 ## grado fecha apellido nombre matematica ingles matricula ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 cuarto 1/1/2015 Hernandez Rodrigo 90 60 100 ## 2 cuarto 1/2/2015 Hernandez Rodrigo 85 70 100 ## 3 cuarto 1/3/2015 Hernandez Rodrigo 70 80 100 ## 4 cuarto 1/4/2015 Hernandez Rodrigo 75 85 100 ## 5 cuarto 1/5/2015 Hernandez Rodrigo 70 90 100 ## 6 cuarto 1/6/2015 Hernandez Rodrigo 66 90 100 ## 7 cuarto 1/1/2015 Sanchez Juan 60 80 102 ## 8 cuarto 1/2/2015 Sanchez Juan 70 80 102 ## 9 cuarto 1/3/2015 Sanchez Juan 80 90 102 ## 10 cuarto 1/4/2015 Sanchez Juan 85 85 102 ## # … with 134 more rows 5.5 Datos anidados, caso 1 Como se había mencionado en la Sección 3.13 del Capitulo Funcionamiento avanzado de R, una de las ventajas de los tibbles es que permiten tener columnas tipo lista, las cuales son muy útiles para iterar y realizar cálculos de manera expedita. En este caso 1 se trabaja con los datos de ‘airq’, que era la tabla modificada de ‘airquality’. Un caso típico de datos anidados es el agrupar la tabla de acuerdo a una variable categórica y aplicar la función nest de tidyr. Esto genera una columna ‘data’, del tipo lista, donde se almacena una tabla para cada nivel de la variable agrupadora. airq_nest = airq %&gt;% group_by(Month) %&gt;% nest() El poder de los datos anidados es la combinación de mutate (dplyr) para generar nuevas columnas, y de las funciones map (purrr) para iterar sobre una columna tipo lista. De forma general esta combinación se plasma de la siguiente forma: mutate(nueva_columna = map(columna_lista, ~ .f(.x))), donde ‘nueva_columna’ es el nombre de la columna a crear, ‘columna_lista’ es el nombre de la columna tipo lista sobre la cual se va a iterar, y ~ .f(.x) es la función o secuencia de funciones a realizar sobre cada elemento (.x) de la ‘columna_lista’. Aplicando lo mencionado anteriormente sobre la tabla anidada ‘airq_nest’ se tienen los siguientes pasos, en diferentes mutate: mod = map(data, ~lm(Wind ~ Temp, data = .x)): Crea una nueva columna ‘mod’, que va a ser el resultado de un modelo lineal para cada mes (iterando sobre ‘data’), en función del viento (‘Wind’) y la temperatura (‘Temp’). La función para modelos lineales es lm y el primer argumento es la formula que lleva la estructura y ~ x, el argumento data se pone de forma explicita y aquí es donde se le indica los elementos sobre los cuales iterar (.x). El resultado es una lista, de ahí que se usara map y no una de sus versiones. slope = map_dbl(mod, ~tidy(.) %&gt;% filter(term == 'Temp') %&gt;% pull(estimate)): Crea una nueva columna ‘slope’, que va a almacenar la pendiente del modelo lineal (‘mod’) anteriormente calculado, como se sabe que es un numero se usa map_dbl. r2 = map_dbl(mod, ~glance(.) %&gt;% pull(r.squared)): Crea una nueva columna ‘r2’, donde se va a almacenar el valor del coeficiente de determinación (\\(R^2\\)), como se sabe que es un numero se usa map_dbl. plot = map2(data,Month, ~ggplot(.x, aes(Temp, Wind)) + geom_point() + geom_smooth(method = 'lm') + labs(title = .y) + theme_bw(base_size = 12)): Crea una nueva columna, donde se va a almacenar el gráfico de dispersión para cada mes, y se le agrega un titulo para saber a que mes corresponde. En este caso se esta iterando sobre dos objetos, por lo que se usa map2: la columna tipo lista donde están los datos a graficar (‘data’), y la columna tipo factor (vector) donde esta la variable agrupadora (‘Month’) para poder poner el titulo correspondiente. airq_nest = airq_nest %&gt;% mutate(mod = map(data, ~lm(Wind ~ Temp, data = .x))) %&gt;% mutate(slope = map_dbl(mod, ~tidy(.) %&gt;% filter(term == &#39;Temp&#39;) %&gt;% pull(estimate)), r2 = map_dbl(mod, ~glance(.) %&gt;% pull(r.squared)), plot = map2(data,Month, ~ggplot(.x, aes(Temp, Wind)) + geom_point() + geom_smooth(method = &#39;lm&#39;) + labs(title = .y) + theme_bw(base_size = 12))) airq_nest ## # A tibble: 5 x 6 ## Month data mod slope r2 plot ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 Mayo &lt;tibble [31 × 6]&gt; &lt;lm&gt; -0.192 0.139 &lt;gg&gt; ## 2 Junio &lt;tibble [30 × 6]&gt; &lt;lm&gt; -0.0691 0.0146 &lt;gg&gt; ## 3 Julio &lt;tibble [31 × 6]&gt; &lt;lm&gt; -0.215 0.0932 &lt;gg&gt; ## 4 Agosto &lt;tibble [31 × 6]&gt; &lt;lm&gt; -0.249 0.258 &lt;gg&gt; ## 5 Setiembre &lt;tibble [30 × 6]&gt; &lt;lm&gt; -0.236 0.325 &lt;gg&gt; 5.5.1 Efectos secundarios En algunas ocasiones el resultado de una iteración no corresponde con un vector, tabla o lista, sino que puede ser la creación de gráficos o el exportar objetos (lo que se conoce en ingles como ‘side effect’); para estos casos existe la función walk y sus variantes. En el primer ejemplo se quiere imprimir cada gráfico en la columna ‘plot’, por lo que se itera sobre la columna deseada, y se llama a la función ~ print(.) para que despliegue cada uno de los elementos. walk(airq_nest$plot, ~print(.)) Un resultado similar se puede obtener usando pull, donde se jala como vector los elementos de la columna deseada. airq_nest %&gt;% pull(plot) ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] El ultimo ejemplo hace uso de walk2 ya que se desea iterar sobre dos objetos: la columna de gráficos (‘plot’) y la columna agrupadora (‘Month’). Lo que se desea realizar es exportar cada gráfico por separado, de ahí la necesidad de usar ambos objetos, el gráfico a exportar y la variable agrupadora para incluirla en el nombre del archivo. Para esto ultimo se usa la función str_glue de stringr que lo que hace es crear una linea de texto donde se pueden ingresar variables usando {variable}. En el ejemplo específicamente, se guarda cada gráfico en la carpeta ‘figures’, con el nombre ‘regresion_{.y}.png’, donde ‘{.y}’ corresponde con el segundo objeto a iterar, en este caso el mes (‘Month’). walk2(airq_nest$plot, airq_nest$Month, ~ggsave(filename = str_glue(&quot;figures/regresion_{.y}.png&quot;), plot = .x, dpi = 300, width = 7, height = 4, units = &quot;in&quot;, type = &quot;cairo&quot;)) 5.6 Datos anidados, caso 2 En este caso 2 se trabaja con los datos de ‘gapminder’, donde se agrupa por país (‘country’), y se crea una tabla para cada país. Este caso es bastante ilustrativo del poder de los tibbles y la iteración, ya que la tabla anidada cuenta con 142 filas (1 por país), y si se quisiera realizar una tarea por país a pie, seria muy tedioso y poco eficiente. gap_nest = gapminder %&gt;% group_by(country) %&gt;% nest() De manera similar al caso 1, se genera un modelo lineal para cada país en función de la expectativa de vida (‘lifeExp’) por año (‘year’), y adicionalmente se calcula el coeficiente de determinación (\\(R^2\\)) para cada modelo lineal. gap_nest = gap_nest %&gt;% mutate(mod = map(data, ~lm(lifeExp ~ year, data = .x))) %&gt;% mutate(r2 = map_dbl(mod, ~glance(.) %&gt;% pull(r.squared))) gap_nest ## # A tibble: 142 x 4 ## country data mod r2 ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; ## 1 Afghanistan &lt;tibble [12 × 5]&gt; &lt;lm&gt; 0.948 ## 2 Albania &lt;tibble [12 × 5]&gt; &lt;lm&gt; 0.911 ## 3 Algeria &lt;tibble [12 × 5]&gt; &lt;lm&gt; 0.985 ## 4 Angola &lt;tibble [12 × 5]&gt; &lt;lm&gt; 0.888 ## 5 Argentina &lt;tibble [12 × 5]&gt; &lt;lm&gt; 0.996 ## 6 Australia &lt;tibble [12 × 5]&gt; &lt;lm&gt; 0.980 ## 7 Austria &lt;tibble [12 × 5]&gt; &lt;lm&gt; 0.992 ## 8 Bahrain &lt;tibble [12 × 5]&gt; &lt;lm&gt; 0.967 ## 9 Bangladesh &lt;tibble [12 × 5]&gt; &lt;lm&gt; 0.989 ## 10 Belgium &lt;tibble [12 × 5]&gt; &lt;lm&gt; 0.995 ## # … with 132 more rows Con los datos anteriores se pueden filtrar los países que hayan tenido un \\(R^2\\) por debajo de 0.25, lo que seria indicio de un comportamiento no lineal, lo que podría estar asociado a problemas de desarrollo en esos países. Para poder graficar los datos es necesario desanidarlos (unnest) para volver a contar con las columnas a como estaban en la tabla original, pero ahora con las columnas calculadas en las iteraciones. gap_nest %&gt;% # ungroup() %&gt;% # arrange(r2) %&gt;% # slice(1:10) %&gt;% filter(r2 &lt; .25) %&gt;% unnest(data) %&gt;% ggplot() + geom_line(aes(year, lifeExp, col = country, group = country)) 5.7 Recursos Se presentan recursos a consultar para ahondar más en los temas presentados. tidyverse Modern R with the tidyverse (Capítulo 8) Referencias "],
["datos-espaciales.html", "Capítulo 6 Datos espaciales 6.1 Introducción 6.2 Paquetes para datos espaciales 6.3 Sistemas de Referencias de Coordenadas (CRS) 6.4 Importar datos 6.5 Exportar datos 6.6 Mapas 6.7 Recursos", " Capítulo 6 Datos espaciales 6.1 Introducción En este capitulo se presenta una breve introducción al uso de R como ambiente para trabajar datos espaciales. Los paquetes a utilizar son: library(sp) library(sf) library(ggspatial) library(raster) library(stars) library(viridis) library(rgeos) library(rgdal) library(mapview) library(RColorBrewer) library(ggrepel) library(rio) library(tmap) library(tidymodels) library(tidyverse) library(rayshader) 6.2 Paquetes para datos espaciales La comunidad de R ha desarrollado una gran variedad de paquetes para datos espaciales, algunos de los cuales han ido evolucionando y facilitando la manipulación y presentación de los mismos. Dentro de los paquetes más usados están: sf (Pebesma, 2020): Para datos vectoriales dentro de la filosofía tidyverse sp (Pebesma &amp; Bivand, 2019): El predecesor de sf para datos vectoriales y en grilla (no exactamente raster) raster (Hijmans, 2020): Para datos raster stars (Pebesma, 2019): El candidato a suceder a raster rgeos y rgdal (Bivand et al., 2019; Bivand &amp; Rundel, 2019): Interfaces para librerías básicas de manipulación de datos espaciales ggplot2 y tmap (Tennekes, 2019): Creación de mapas estáticos (tmap puede crear mapas interactivos) mapview y leaflet (Appelhans et al., 2020; Cheng et al., 2018): Creación de mapas interactivos Otro montón de paquetes brindan funciones adicionales y funciones para tareas especificas. 6.3 Sistemas de Referencias de Coordenadas (CRS) Los datos espaciales tienen por lo general un sistema de coordenadas asociado (geográficas, lambert, UTM, etc.). Estos sistemas se pueden identificar por medio de códigos EPSG que han sido estandarizados para uso general. Como ejemplo, las coordenadas geográficas en grados (WGS84) tiene el código 4326, las coordenadas geográficas en metros (WGS84/pseudo-mercator; son las utilizadas por GoogleMaps y otros) tiene el codigo 3857. Las coordenadas para Costa Rica con la proyección CRTM05 tienen el codigo 5367, mientras que para la proyección Lambert Norte tienen el codigo 5456. Todos estos se pueden obtener por medio del sitio web epsg.io o en QGIS a la hora de buscar sistemas de coordenadas ahí aparece el codigo. 6.4 Importar datos 6.4.1 Desde archivos de texto Datos en archivos de texto (.txt, .csv, etc. y principalmente para datos puntuales) se pueden importar de manera convencional con rio::import. datos &lt;- rio::import(&quot;data/BroomsBarn.txt&quot;, setclass = &#39;tibble&#39;) %&gt;% mutate(x = x*40, y = y*40, logK = log(K), logP = log(P)) Posteriormente pueden convertirse a datos espaciales (st_as_sf), indicando la posición o el nombre de las columnas con las coordenadas X,Y y asignándole un CRS. datos_sf = st_as_sf(datos, coords = 1:2, crs = NA, remove = F) La ventaja de sf es que permite manipular los dato asociados al objeto espacial haciendo uso de los verbos del tidyverse. datos_sf %&gt;% filter(pH &gt; 8) ## # A tibble: 163 x 8 ## x y K logK pH P logP geometry ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;POINT&gt; ## 1 80 1240 14 2.64 8.2 6.6 1.89 (80 1240) ## 2 120 40 28 3.33 8.2 4.2 1.44 (120 40) ## 3 120 80 26 3.26 8.2 7.5 2.01 (120 80) ## 4 120 120 23 3.14 8.2 5.1 1.63 (120 120) ## 5 120 160 21 3.04 8.2 4 1.39 (120 160) ## 6 120 240 22 3.09 8.3 4.8 1.57 (120 240) ## 7 120 280 24 3.18 8.3 4.8 1.57 (120 280) ## 8 120 320 41 3.71 8.2 4 1.39 (120 320) ## 9 120 1240 15 2.71 8.2 8.4 2.13 (120 1240) ## 10 160 80 24 3.18 8.2 6.7 1.90 (160 80) ## # … with 153 more rows Para algunas funciones todavía es necesario usar objetos sp por lo que se pueden crear estos a partir de los objetos sf. datos_sp = as(datos_sf, &#39;Spatial&#39;) coordnames(datos_sp) = c(&#39;X&#39;,&#39;Y&#39;) 6.4.2 Desde archivos espaciales 6.4.2.1 Shapefiles De igual manera se pueden importar/leer directamente datos en muchos formatos espaciales (ejemplo: shapefiles, geopackage, raster, etc.). Las funciones para leer datos son: st_read y read_sf; la primera importa los datos como DataFrame, la segunda como tibble. fallas = read_sf(&#39;data/fallas.shp&#39;) geomorfo = read_sf(&#39;data/geomorfo.shp&#39;) En este caso no reconocen los metadatos de la proyeccion por lo que se les puede asignar por medio de st_set_crs, donde el argumento necesario es el codigo EPSG. fallas_ln = fallas %&gt;% st_set_crs(5456) geomorfo_ln = geomorfo %&gt;% st_set_crs(5456) Si se desean transformar a otro sistema se usa st_transform, donde, de nuevo, el argumento necesario es el codigo EPSG del sistema destino (En el caso del ejemplo a coordenadas geográficas). fallas_geog = fallas_ln %&gt;% st_transform(4326) geomorfo_geog = geomorfo_ln %&gt;% st_transform(4326) 6.4.2.2 Geopackage Si se tiene un archivo .gpkg es necesario explorar primero las capas disponibles para luego importar los datos deseados. Esto se realiza con st_layers y la dirección del archivo geopackage. st_layers(dsn = &#39;data/espaciales.gpkg&#39;) ## Driver: GPKG ## Available layers: ## layer_name geometry_type features fields ## 1 fallas Line String 2028 7 ## 2 geomorfo Polygon 467 6 Una vez se han identificado las capas se pueden importar con cualquiera de los métodos deseados, donde el argumento a especificar es layer y el nombre de la capa a importar. fallas2 = read_sf(&#39;data/espaciales.gpkg&#39;, layer = &#39;fallas&#39;) geomorfo2 = read_sf(&#39;data/espaciales.gpkg&#39;, layer = &#39;geomorfo&#39;) 6.4.2.3 Raster Para leer rasters de una banda se usa la función raster, para rasters multi-banda se puede usar brick. Para objetos stars es indiferente y se pueden leer por medio de read_stars. Para convertir de raster a stars se usa st_as_stars. La lectura de un raster de una banda se ejemplifica con un modelo de elevación digital del Pacifico. pacifico = raster(&#39;data/pacifico.tif&#39;) pacifico ## class : RasterLayer ## dimensions : 300, 300, 90000 (nrow, ncol, ncell) ## resolution : 0.01661111, 0.01661111 (x, y) ## extent : -87.99167, -83.00833, 5.008333, 9.991667 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 ## source : /Users/maximiliano/Documents/UCR/Docencia/Extras/R/bookdown/geolonum/data/pacifico.tif ## names : pacifico ## values : -4770, 3601 (min, max) pacifico_stars = read_stars(&#39;data/pacifico.tif&#39;) pacifico_stars = st_as_stars(pacifico) pacifico_stars ## stars object with 2 dimensions and 1 attribute ## attribute(s): ## pacifico ## Min. :-4770 ## 1st Qu.:-3144 ## Median :-2417 ## Mean :-2155 ## 3rd Qu.:-1627 ## Max. : 3601 ## dimension(s): ## from to offset delta refsys point values ## x 1 300 -87.9917 0.0166111 +proj=longlat +datum=WGS8... NA NULL [x] ## y 1 300 9.99167 -0.0166111 +proj=longlat +datum=WGS8... NA NULL [y] La lectura de un raster multi-banda se ejemplifica con un geotiff que viene con el paquete stars. sat_ras = brick(system.file(&#39;tif/L7_ETMs.tif&#39;, package = &#39;stars&#39;)) sat_ras ## class : RasterBrick ## dimensions : 352, 349, 122848, 6 (nrow, ncol, ncell, nlayers) ## resolution : 28.5, 28.5 (x, y) ## extent : 288776.3, 298722.8, 9110729, 9120761 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=25 +south +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs ## source : /Library/Frameworks/R.framework/Versions/3.6/Resources/library/stars/tif/L7_ETMs.tif ## names : L7_ETMs.1, L7_ETMs.2, L7_ETMs.3, L7_ETMs.4, L7_ETMs.5, L7_ETMs.6 ## min values : 0, 0, 0, 0, 0, 0 ## max values : 255, 255, 255, 255, 255, 255 sat_df = as.data.frame(sat_ras,xy=T) sat_stars = read_stars(system.file(&#39;tif/L7_ETMs.tif&#39;, package = &#39;stars&#39;)) sat_stars ## stars object with 3 dimensions and 1 attribute ## attribute(s): ## L7_ETMs.tif ## Min. : 1.00 ## 1st Qu.: 54.00 ## Median : 69.00 ## Mean : 68.91 ## 3rd Qu.: 86.00 ## Max. :255.00 ## dimension(s): ## from to offset delta refsys point values ## x 1 349 288776 28.5 NA FALSE NULL [x] ## y 1 352 9120761 -28.5 NA FALSE NULL [y] ## band 1 6 NA NA NA NA NULL 6.5 Exportar datos 6.5.1 Vectoriales Para exportar datos vectoriales se puede usar st_write, donde se define el objeto espacial a exportar y el tipo de archivo a generar (.shp, .gpkg, etc.). Las opciones layer_options = 'OVERWRITE=YES' y delete_layer = T permiten reescribir un objeto si ya se encontraba presente en la carpeta destino. st_write(fallas_geog, dsn = &#39;data/espaciales_geog.gpkg&#39;, layer = &#39;fallas&#39;, layer_options = &#39;OVERWRITE=YES&#39;, quiet = T, delete_layer = T) st_write(geomorfo_geog, dsn = &#39;data/espaciales_geog.gpkg&#39;, layer = &#39;geomorfo&#39;, layer_options = &#39;OVERWRITE=YES&#39;, quiet = T, delete_layer = T) Se puede revisar que el objeto haya sido creado apropiadamente, de nuevo usando st_layers. st_layers(dsn = &#39;data/espaciales_geog.gpkg&#39;) ## Driver: GPKG ## Available layers: ## layer_name geometry_type features fields ## 1 fallas Line String 2028 7 ## 2 geomorfo Polygon 467 6 6.5.2 Raster Dependiendo de si el objeto es raster o stars, se deberá usar la función respectiva, writeRaster o write_stars. en el caso de writeRaster se tiene que especificar el formato (para esto se puede consultar la ayuda de la función). writeRaster(sat_ras, &#39;data/imagen_satelite.tif&#39;, format=&quot;GTiff&quot;, overwrite=TRUE) write_stars(sat_stars, &#39;data/imagen_satelite_stars.tif&#39;) 6.6 Mapas Existen diferentes formas de graficar datos espaciales, aquí se presentan las más usadas, donde se recomienda ggplot2 o tmap. El primero ya que se esta familiarizado con el funcionamiento del mismo y es nada más agregar un par de funciones especificas para datos espaciales; el segundo ya que es especifico para datos espaciales siguiendo una idea similar a ggplot2. 6.6.1 Estáticos 6.6.1.1 básico Todos los objetos espaciales tienen un método de ploteo básico (plot). Si el objeto tiene más de 1 atributo va a plotear todos o los que pueda. Para evitar esto se puede especificar cual atributo se quiere plotear. En general esto se puede usar más para una visualización rápida pero no para mapas finales. Se muestran diferentes opciones para visualizar los diferentes tipos de datos, pero no se muestra el mapa final por ser muy básico. plot(fallas_ln) plot(datos_sf[,&#39;K&#39;]) plot(geomorfo_ln[,&#39;CODIGO&#39;]) plot(pacifico) plot(pacifico_stars) 6.6.1.2 ggplot El paquete ggplot tiene geometrías especificas para objetos sf (vector) y stars (raster o grillas), por lo que facilita la creación de mapas en un ambiente ya conocido. La forma más básica de crear un mapa vectorial usando ggplot es usando geom_sf y especificando el argumento data que corresponde con el objeto espacial, pero por defecto despliega coordenadas geográficas (Figura 6.1). ggplot() + geom_sf(data = fallas_ln) Figura 6.1: Mapa básico en ggplot2 Para que el mapa se despliegue en las coordenadas del objeto y no geográficas, hay que cambiar el argumento datum en coord_sf al CRS del archivo deseado (Figura 6.2). ggplot() + geom_sf(data = fallas_ln) + coord_sf(datum = st_crs(fallas_ln)) Figura 6.2: Mapa en ggplot2 con el datum modificado El paquete ggspatial (Dunnington, 2018) ofrece algunas capas adicionales especificas (elementos cartográficos) para datos espaciales, con las cuales se pueden agregar una escala (annotation_scale) y el norte (annotation_north_arrow), ademas de poder agregar un mapa de fondo (annotation_map_tile), donde zoom define el nivel de detalle, a menor zoom menor detalle (Figura 6.3). ggplot() + annotation_map_tile(zoom = 8) + geom_sf(data = fallas_ln) + coord_sf(datum = st_crs(fallas_ln)) + annotation_scale(location = &#39;bl&#39;) + annotation_north_arrow(location = &#39;tr&#39;, height = unit(.75, &quot;cm&quot;), width = unit(.75, &quot;cm&quot;)) Figura 6.3: Mapa en ggplot con elementos cartográficos (escala y norte) Para datos puntuales (Figura 6.4) o de polígonos (Figura 6.5) se puede especificar una columna de los datos, por la cual colorear los puntos. ggplot() + geom_sf(data = datos_sf, aes(col = K), size = 3, alpha = 0.6) + scale_color_viridis_c() Figura 6.4: Mapa de puntos en ggplot coloreados por la variable “K” ggplot() + geom_sf(aes(fill = as_factor(FORMA)), data = geomorfo_ln) + coord_sf(datum = st_crs(geomorfo_ln)) + scale_fill_brewer(palette = &#39;Set3&#39;) Figura 6.5: Mapa de polígonos en ggplot coloreados por la variable “FORMA” La forma de crear un mapa a partir de un objeto stars (grilla de una banda) es por medio de geom_stars, donde, de nuevo, se especifica el objeto espacial en el argumento data. En este caso automáticamente se aplica al relleno (fill) la variable del objeto. Ademas, es necesario agregar coord_equal para que los ejes de coordenadas tengan las misma escala (Figura 6.6). ggplot() + geom_stars(data = pacifico_stars) + scale_fill_gradient(low = &#39;black&#39;, high = &#39;white&#39;, na.value = &#39;white&#39;) + coord_equal() Figura 6.6: Mapa de objeto stars en ggplot. Se especifica el relleno como un gradiente y se aplica la misma escala a ámbos ejes Para imágenes multi-banda es necesario transforma el objeto espacial en una tabla (sat_df = as.data.frame(sat_ras,xy=T)) y graficar el relleno (fill) usando la función rgb, donde se especifica el nombre de las bandas que corresponde con cada uno de los canales rgb, y se debe definir maxColorValue=255 (Figura 6.7). Como lo que se va a graficar es una grilla se usa la función geom_tile, y de nuevo es necesario definir los ejes de coordenadas iguales con coord_equal. ggplot(sat_df, aes(x, y, fill=rgb(red = L7_ETMs.3, green = L7_ETMs.2, blue = L7_ETMs.1, maxColorValue = 255))) + geom_tile() + scale_fill_identity() + coord_equal() + scale_x_continuous(expand = c(0,0)) + scale_y_continuous(expand = c(0,0)) + theme(axis.text.y = element_text(angle = 90, hjust = .5)) Figura 6.7: Mapa de color verdadero para una imagen satelital multi-banda 6.6.1.3 tmap El paquete tmap es una opción especifica para datos espaciales y sigue la misma ideología de ggplot al trabajar en capas. Por defecto no despliega la grilla de coordenadas, hay que agregárselas con tm_grid. Para revertir los colores en la paleta se le agrega un menos (-) antes del nombre, y para colocar la leyenda fuera del área del mapa se debe usar tm_layout(legend.outside = T). Existen opciones globales para los mapas generados con tmap, estas se pueden modificar con tmao_options, y uno de los argumentos es la leyenda afuera, así no hay que aplicarlo en cada mapa por separado. tmap_options(legend.outside = T) La estructura básica de cualquier gráfico es: tm_shape(shp = &lt;DATA&gt;) + &lt;tm_function&gt;(col = &lt;&#39;VARIABLE&#39;&gt;, palette = &#39;&#39;, style = &#39;&#39;, n = 5) + &lt;tm_layout&gt; + &lt;tm_xlab&gt; + &lt;tm_ylab&gt; + &lt;tm_grid&gt; + &lt;tm_scale_bar&gt; + &lt;tm_compass&gt; + Se pueden agregar diferentes objetos espaciales agregando diferentes tm_shape. A diferencia de ggplot y geom_sf que era una función para todo objeto vectorial, es necesario definir el tipo de objeto con las diferentes funciones tm_*. Lo que se va a hacer es recrear los mapas de la sección anterior pero con tmap. Un mapa básico se observa en la Figura 6.8, donde se grafican las líneass de falla. tm_shape(fallas_ln) + tm_lines() Figura 6.8: Mapa básico con tmap El mapa de puntos se crea por medio de tm_dots. Por defecto tmap discretiza los datos (Figura 6.9), pero si se quiere representar como un gradiente (Figura 6.10) es necesario definir style = 'cont', para continuo. tm_shape(datos_sf) + tm_dots(&#39;K&#39;, size = .3, palette = &#39;viridis&#39;) Figura 6.9: Mapa de puntos con tmap coloreado de acuerdo a una variable tm_shape(datos_sf) + tm_dots(&#39;K&#39;, size = .3, palette = &#39;viridis&#39;, style = &#39;cont&#39;) + tm_layout(legend.outside = T) Figura 6.10: Mapa de puntos con tmap coloreado de acuerdo a una variable, con la leyenda tipo gradiente En la Figura 6.11 se observa un mapa de polígonos, donde ademas se agrega la grilla de coordenadas pero sin las líneas. geomorfo_ln %&gt;% mutate(FORMA = as_factor(FORMA)) %&gt;% tm_shape() + tm_polygons(&#39;FORMA&#39;) + tm_grid(lines = F) Figura 6.11: Mapa de poligonos con tmap, agregando la grilla de coordenadas sin la líneas internas Para graficar objetos raster de una banda se usa tm_raster (Figura 6.12), donde hay que definir la paleta de colores a usar. Si se desea invertir simplemente se le agrega un - en frente del nombre. tm_shape(pacifico) + tm_raster(palette = &#39;-Greys&#39;, style = &#39;cont&#39;) Figura 6.12: Mapa raster con tmap Para ver las opciones de las paletas de color se puede usar: tmaptools::palette_explorer() Para graficar objetos raster multi-banda se debe usar tm_rgb (Figura 6.13) y especificar el orden de las bandas que corresponde con cada uno de los canales rgb. tm_shape(sat_ras) + tm_rgb(r = 3, g = 2, b = 1) Figura 6.13: Mapa de color verdadero con tmap Finalmente, tmap trae incorporadas funciones para agregar escala (tm_scale_bar) y norte (tm_compass) (Figura 6.14). tm_shape(fallas_ln) + tm_lines() + tm_scale_bar(width = .2,position = c(&#39;left&#39;,&#39;bottom&#39;)) + tm_compass(position = c(&#39;right&#39;,&#39;top&#39;)) Figura 6.14: Mapa incorporando elementos de escala y norte Para salvar un mapa de tmap como imagen se usa tmap_save. Idealmente hay que guardar el mapa en un objeto. mapa1 = tm_shape(fallas_ln) + tm_lines() + tm_scale_bar(width = .2,position = c(&#39;left&#39;,&#39;bottom&#39;)) + tm_compass(position = c(&#39;right&#39;,&#39;top&#39;)) tmap_save(mapa1, &#39;figures/mapa_fallas.png&#39;, dpi = 300) 6.6.2 Dinámicos Una de la opciones con mapas interactivos es que se pueden definir diferentes mapas de fondo. Dentro de los más usados están los que se muestran a continuación: mybasemaps = c(&#39;CartoDB.Positron&#39;, &#39;OpenStreetMap&#39;, &#39;OpenTopoMap&#39;, &#39;Esri.WorldImagery&#39;, &#39;Esri.WorldTopoMap&#39;, &#39;Esri.OceanBasemap&#39;) De manera general se pueden asignar mapas de fondo para todos los mapas de una sesión definiéndolos en las opciones de tmap y mapview: tmap_options(basemaps = mybasemaps) mapviewOptions(basemaps = mybasemaps) La Figura 6.15 muestra las diferentes opciones para manipular un mapa dinámico creado con mapview. Las mismas opciones y elementos se pueden agregar a un mapa de leaflet pero requieren ser especificadas explícitamente (haciendo más largo el codigo), mientras que mapview ya agrega muchos de estos por defecto. Figura 6.15: Partes de un mapa dinámico creado con mapview 6.6.2.1 tmap Este paquete ofrece la opción de visualizar un mapa estático como dinámico, cambiando el modo de visualización (tmap_mod(mode = c('plot','view'))), donde plot es para mapas estático y view para mapas dinámicos y traduce el mapa a un mapa leaflet. Lo anterior cambia el modo para todos los mapas que se creen a partir de que esto se modifica. Para visualizar de manera dinámica un mapa guardado en un objeto se puede usar tmap_leaflet, que genera un mapa leaflet a partir de uno de tmap, pero solo para el mapa que se quiere (Figura 6.16). Para demostrar esto se genera el mapa dinámico a partir del mapa que se salvo anteriormente (mapa1), que corresponde con la Figura 6.8. tmap_leaflet(mapa1) Figura 6.16: Mapa interactivo leaflet a partir de un mapa estático tmap 6.6.2.2 mapview Este paquete permite generar mapas interactivos de manera eficiente y rápida, pero no brinda la personalización de leaflet, el cual es mucho más complejo y para generar un mapa similar se requiere aproximadamente de 4 a 5 veces mas líneass de codigo. La función básica es mapview donde se le pasa el objeto espacial. Por defecto le asigna un color único. El argumento layer_name es para definir el nombre que se quiere aparezca en la leyenda. Si se desean agregar diferentes capas simplemente se agregan funciones mapview con el operador + (Figura 6.17). mapview es inteligente en el sentido de que dependiendo del tipo de objeto (polígono, líneas, punto) lo va a graficar en el orden jerarquico visual (puntos por encima de líneass, líneass por encima de polígonos). mapview(fallas_ln, layer.name = &#39;Fallas&#39;, color = &#39;red&#39;) + mapview(geomorfo_ln, layer.name = &#39;Geomorfologia&#39;) Figura 6.17: Mapa con mapview, donde se pueden combinar objetos espaciales usando el operador + Se pueden colorear objetos espaciales definiendo una columna de la tabla de atributos por medio del argumento zcol (Figura 6.18). mapview(datos_sf, zcol = &#39;logK&#39;, layer.name = &#39;logK&#39;) Figura 6.18: Mapa de puntos con mapview coloreados de acuerdo a una variable Para graficar objetos raster de una banda simplemente se pasa el objeto a la función mapview (Figura 6.19). Para raster multi-banda se debe usar viewRGB y definir el orden de las bandas de acuerdo a los canales rgb (Figura 6.20). mapview(pacifico) Figura 6.19: Mapa raster con mapview viewRGB(sat_ras, r = 3, g = 2, b = 1) Figura 6.20: Mapa de color verdradero con mapview Con mapview se pueden crear diferentes mapas y sincronizarlos por medio de sync del paquete leafsync (Simplemente se muestra el codigo). m1 = mapview(franconia, zcol = &#39;district&#39;, layer.name = &#39;Distrito&#39;) m2 = mapview(breweries, legend = F) leafsync::sync(m1, m2) 6.6.3 Modelos de sombras Para generar modelos de sombras en 2D y 3D se usa el paquete rayshader (Morgan-Wall, 2019), el cual funciona a partir de objetos raster. Funciona con el pipe operator (%&gt;%), por lo que es familiar a trabajar en el tidyverse. Primero hay que pasar el raster a matriz. pacifico_mat = raster_to_matrix(pacifico) Para mejorar la apariencia del modelo de sombras es recomendado agregar diferentes sombras (‘shades’), las cuales son calculadas de diferente manera para resaltar diferentes aspectos del modelo. zscale = 200 elmat = pacifico_mat raymat = ray_shade(elmat, zscale = zscale) ambmat = ambient_shade(elmat, zscale = zscale) lambmat = lamb_shade(elmat, zscale = zscale) Para generar un modelo de sombras en 2D, se empieza con la matriz de elevación, se le agregan las diferentes capas, y para mostrar el mapa se termina la cadena de comandos con plot_map (Figura 6.21). elmat %&gt;% sphere_shade(texture = &quot;bw&quot;) %&gt;% add_shadow(raymat, 0.5) %&gt;% add_shadow(lambmat, 0.5) %&gt;% add_shadow(ambmat, 0.5) %&gt;% plot_map() Figura 6.21: Modelo de sombras en 2D Para generar el modelo en 3D (Figura 6.22), simplemente se termina la cadena de comandos con plot_3d. Esto abre una ventana interactiva donde se puede manipular el modelo, en este caso simplemente se toma una captura de la apariencia en 3D. elmat %&gt;% sphere_shade(texture = &quot;imhof1&quot;) %&gt;% add_shadow(raymat, 0.5) %&gt;% add_shadow(lambmat, 0.5) %&gt;% add_shadow(ambmat, 0.5) %&gt;% plot_3d(elmat, zscale = zscale, water = T, theta = 0, phi = 40, zoom = .5, windowsize = c(1000,600)) render_snapshot(clear=TRUE) Figura 6.22: Modelo de sombras en 3D Adicionalmente se puede crear una pequeña película o animación con render_movie (No se presenta aquí pero se muestra el codigo). elmat %&gt;% sphere_shade(texture = &quot;imhof1&quot;) %&gt;% add_shadow(raymat, 0.5) %&gt;% add_shadow(lambmat, 0.5) %&gt;% add_shadow(ambmat, 0.5) %&gt;% plot_3d(elmat, zscale = zscale, water = T, theta = 0, phi = 40, zoom = .5, windowsize = c(1000,600)) render_movie(filename = &#39;modelo3D&#39;) rgl::rgl.close() 6.7 Recursos Se presentan recursos a consultar para ahondar más en los temas presentados. Geocomputatio with R Es un libro virtual donde se introducen conceptos de como usar R para análisis espacial y creación de mapas. Spatial Data Sience Intro to GIS and Spatial Analysis RSpatial Geoestadistica con R (El sitio web está en polaco, pero se puede traducir en Chrome) Introducción a estadística con R (Capítulo 7) Spatial workshop notes sf tmap stars mapview leaflet rayshader Referencias "],
["álgebra-lineal.html", "Capítulo 7 Álgebra lineal 7.1 Introducción 7.2 Tensores 7.3 Eigenvectors y Eigenvalues", " Capítulo 7 Álgebra lineal 7.1 Introducción El álgebra se usa en muchas de las operaciones básicas que se realizan rutinariamente, y es la base para poder aplicar y resolver problemas más complejos que involucaran gran cantidad de datos y variables. Este capitulo hace una introducción básica a conceptos y técnicas en álgebra lineal, para familiarizar al lector y sentar una base para la comprensión de casos donde soluciones analíticas no pueden emplearse y es necesario recurrir a métodos algebráicos. 7.2 Tensores En general cualquier arreglo de datos numéricos se considera un tensor de dimensión variable, dependiendo de la estructura de los datos. Un tensor es una representación matemática que cuantifica la variación de la magnitud con respecto a la dirección. Numéricamente se representa como una matriz y gráficamente como un elipsoide, en el caso de 3 dimensiones (Figura 7.1). Figura 7.1: Tensor como elipsoide. Tomado de: http://www.geosci.usyd.edu.au/users/prey/Teaching/Geol-3101/Strain/ellipse.gif Los tensores más conocidos son: Escalar: Tensor de orden 0, cantidad que tiene magnitud pero no dirección. Ejemplos: densidad y temperatura. Vector: Tensor de orden 1, cantidad que tiene magnitud y dirección. Ejemplos: velocidad, aceleración, fuerza. Matriz: Tensor de orden 2 o mas, arreglo de vectores en 2 o más dimensiones, con magnitud y 2 o más direcciones. Ejemplos: esfuerzo, deformación. 7.2.1 Vectores Los vectores son representaciones univariables de datos, ya que pueden almacenar únicamente un tipo de datos. En el sentido estricto de álgebra los datos tienen que ser del tipo numérico. Los vectores se denotan con letras minúsculas (ej: \\(x\\)). La estructura y representación matemática de un vector se presenta en Ecuación (7.1), mientras que la representación gráfica se presenta en la Figura 7.2: \\[\\begin{equation} x = \\left( \\begin{matrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{matrix} \\right) \\tag{7.1} \\end{equation}\\] donde \\(n\\) corresponde con la dimensión del vector. Figura 7.2: Representacion grafica de un vector. Tomado de: http://www.cyberphysics.co.uk/graphics/diagrams/forces/vector_components4.gif 7.2.1.1 Operaciones con vectores Suma: \\[\\begin{equation} x + y = \\left( \\begin{matrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{matrix} \\right) + \\left( \\begin{matrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_n \\end{matrix} \\right) \\tag{7.2} \\end{equation}\\] Se requiere que ambos vectores tengan la misma dimensión. Un ejemplo seria cuando un vector contiene la concentración de \\(Fe^{2+}\\) y otro la de \\(Fe^{3+}\\), al sumarlos se obtiene la concentración total de \\(Fe\\) en la roca. En R esto se realiza simplemente creando los vectores respectivos y sumandolos, porque por defecto hace la suma elemento por elemento. Fe2 = c(2,5,4,7,10) Fe3 = c(4,8,3,5,9) Fe = Fe2 + Fe3 Fe ## [1] 6 13 7 12 19 Multiplicación por escalar: \\[\\begin{equation} \\alpha x = \\left( \\begin{matrix} \\alpha x_1\\\\ \\alpha x_2\\\\ \\vdots\\\\ \\alpha x_n \\end{matrix} \\right) \\tag{7.3} \\end{equation}\\] El escalar multiplica a cada uno de los elementos. Un ejemplo seria cuando se tienen mediciones (decenas o cientos) de la longitud de fósiles en pulgadas y se desean convertir a milímetros, entonces se multiplica el vector por 25.4. Para demostralo en R primero estoy creando un vector aleatorio de 20 datos con limite inferior de 10 y limite superior de 30. Imprimo los resultados para ver los valores, y posteriormente multiplico el vector por el escalar respectivo, de nuevo donde la operacion es elemento por elemento. set.seed(4101) longitud = runif(n = 20, min = 10, max = 30) longitud ## [1] 20.27580 15.18745 17.46071 15.10536 29.76256 16.82955 19.37020 22.68005 ## [9] 20.54253 10.93677 12.86850 23.29475 17.20729 28.22859 13.06465 17.29120 ## [17] 22.63767 20.98795 25.27941 23.90460 25.4 * longitud ## [1] 515.0054 385.7612 443.5020 383.6761 755.9691 427.4706 492.0031 576.0732 ## [9] 521.7804 277.7939 326.8600 591.6867 437.0653 717.0062 331.8420 439.1964 ## [17] 574.9968 533.0940 642.0970 607.1768 Producto punto: \\[\\begin{equation} x y = \\left( \\begin{matrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{matrix} \\right) \\left( \\begin{matrix} y_1 &amp; y_2 &amp; \\dotsb &amp; y_n \\end{matrix} \\right) = x_1 y_1 + x_2 y_2 + \\dotsb + x_n y_n \\tag{7.4} \\end{equation}\\] Se requiere que ambos vectores tengan la misma dimensión. Un ejemplo seria cuando el precio de los diferentes agregados (piedra de construcción) se encuentra en un vector y la cantidad del tipo de agregado se tiene en otro vector; el precio a ganar al vender dicha cantidad de acuerdo al precio establecido es el resultado del producto punto. En R la forma de calcular el producto punto es haciendo uso del multiplicador matricial %*%, lo que arroja un resultado de una matriz de \\(1 \\times 1\\). Lo anterior es lo mismo a hacer la suma del producto entre los vectores. Estos procedimientos se muestran a continuación. precio = c(500, 700, 1200, 400) cantidad = c(30, 15, 12, 23) precio %*% cantidad ## [,1] ## [1,] 49100 sum(precio * cantidad) ## [1] 49100 7.2.2 Matrices Una matriz es una representación bivariable (2 columnas) o multivariable (&gt; 2 columnas) de datos. Similar a los vectores, en el sentido estricto de álgebra los datos tienen que ser del tipo numérico. Las matrices se denotan con letras mayúsculas (ej: \\(A\\)). La estructura y representación matemática de una matriz se presenta en Ecuación (7.5) \\[\\begin{equation} A = \\left( \\begin{matrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\ a_{21} &amp; a_{22} &amp; a_{23}\\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{matrix} \\right) \\tag{7.5} \\end{equation}\\] La dimensión de una matriz es el numero de filas (por lo general denominado \\(i\\)) por el numero de columnas (por lo general denominado \\(j\\)), por lo que en el caso de la matriz mostrada en (7.5) la dimensión es 9 (\\(i \\times j\\)). Los subíndices denotan la ubicación del elemento, siendo el primer subíndice la fila y el segundo la columna; el elemento \\(a_{23}\\) corresponde al elemento en la fila 2 y columna 3. 7.2.2.1 Operaciones con matrices Suma: \\[\\begin{equation} A + B = \\left( \\begin{matrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\ a_{21} &amp; a_{22} &amp; a_{23}\\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{matrix} \\right) + \\left( \\begin{matrix} b_{11} &amp; b_{12} &amp; b_{13}\\\\ b_{21} &amp; b_{22} &amp; b_{23}\\\\ b_{31} &amp; b_{32} &amp; b_{33} \\end{matrix} \\right) \\tag{7.6} \\end{equation}\\] Se requiere que ambas matrices tengan no solo la misma dimensión, pero la misma cantidad de filas y columnas. En este caso la operación es elemento por elemento \\(a_{11} + b_{11}\\). Un ejemplo seria donde una matriz contiene la producción de diversos tipos arcilla para un año dado y la otra tiene la producción para el año siguiente, la matriz resultante tiene la producción sobre esos dos años En R simplemente se hace la suma (o resta) de las matrices, ya que hace la operación elemento por elmento. A1 = matrix(data = c(105,218,220,63,80,76,5,2,1), nrow = 3) A2 = matrix(data = c(84,240,302,102,121,28,4,1,0), nrow = 3) A1 + A2 ## [,1] [,2] [,3] ## [1,] 189 165 9 ## [2,] 458 201 3 ## [3,] 522 104 1 Multiplicación por escalar: \\[\\begin{equation} \\alpha A = \\left( \\begin{matrix} \\alpha a_{11} &amp; \\alpha a_{12} &amp; \\alpha a_{13}\\\\ \\alpha a_{21} &amp; \\alpha a_{22} &amp; \\alpha a_{23}\\\\ \\alpha a_{31} &amp; \\alpha a_{32} &amp; \\alpha a_{33} \\end{matrix} \\right) \\tag{7.7} \\end{equation}\\] El escalar multiplica a cada uno de los elementos. Un ejemplo seria cuando se tienen mediciones de los ejes de cantos de piedra en pulgadas para diversos especímenes y se requiere tenerlos en milímetros, como se muestra en el siguiente ejemplo. cantos = matrix(data = c(3.4,4.6,5.4,2.2,4.3,4.7,1.8,4.3,4.7), nrow = 3) 25.4 * cantos ## [,1] [,2] [,3] ## [1,] 86.36 55.88 45.72 ## [2,] 116.84 109.22 109.22 ## [3,] 137.16 119.38 119.38 Multiplicación: \\[\\begin{equation} A B = \\left( \\begin{matrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\ a_{21} &amp; a_{22} &amp; a_{23}\\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{matrix} \\right) \\left( \\begin{matrix} b_{11} &amp; b_{12} &amp; b_{13}\\\\ b_{21} &amp; b_{22} &amp; b_{23}\\\\ b_{31} &amp; b_{32} &amp; b_{33} \\end{matrix} \\right) \\tag{7.8} \\end{equation}\\] Se requiere que la matriz izquierda tenga la misma cantidad de columnas que filas de la matriz derecha, resultando en una matriz con dimensiones de las filas de la izquierda por las columnas de la derecha (\\(A(i,j)B(m,n)=C(i,n)\\)). Se demuestra con un ejemplo trivial, pero retomando el operador de multipliación matricial %*% presentado anteriormente. Si se utiliza solo * el resultado es elemento por elemento y las matrices debieran ser de exactamente el mismo tamaño. A1 = matrix(data = 1:9, nrow = 3) A2 = matrix(data = 1:9, nrow = 3, byrow = T) A1 %*% A2 ## [,1] [,2] [,3] ## [1,] 66 78 90 ## [2,] 78 93 108 ## [3,] 90 108 126 Determinante: \\[\\begin{equation} A = \\left( \\begin{matrix} a_{11} &amp; a_{12}\\\\ a_{21} &amp; a_{22} \\end{matrix} \\right)\\\\ det(A) = |A| = a_{11} a_{22} - a_{21} a_{12} \\tag{7.9} \\end{equation}\\] Esto aplica para matrices cuadradas, o sea que tienen la misma cantidad de filas y columnas (\\(i \\times i\\)). Aquí se presenta la forma manual para una matriz de \\(2 \\times 2\\) ya que es la más sencilla, para matrices de mayor dimensión se puede hacer uso del software R, como se realiza en el siguiente ejemplo. El comando para calcular el determinante es det(). A = matrix(data = c(4,10,10,30), nrow = 2) det(A) ## [1] 20 7.2.2.2 Tipos de matrices Hay ciertos tipos de matrices que de acuerdo a su estructura reciben nombres especiales. Dentro de estas matrices están: Diagonal: Hay entradas diferentes de cero en la diagonal y el resto de los elementos son ceros. la diagonal es donde el numero de fila y columna es el mismo (ej: \\(a_{22}\\)). \\[\\begin{equation} \\left( \\begin{matrix} 3 &amp; 0 &amp; 0\\\\ 0 &amp; 4 &amp; 0\\\\ 0 &amp; 0 &amp; 2 \\end{matrix} \\right) \\tag{7.10} \\end{equation}\\] Identidad: Es una matriz diagonal donde la diagonal tiene únicamente unos, y tiene una denominación especial \\(I\\). \\[\\begin{equation} I = \\left( \\begin{matrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{matrix} \\right) \\tag{7.11} \\end{equation}\\] Transpuesta: Se “rota” la matriz y lo que antes eran las filas ahora son las columnas y viceversa. Se denota por medio de \\(A^T\\). Si las dimensiones originales eran \\((i,j)\\), las dimensiones de la transpuesta son \\((j,i)\\). \\[\\begin{equation} A = \\left( \\begin{matrix} 3 &amp; 4 &amp; 1\\\\ 2 &amp; 7 &amp; 5 \\end{matrix} \\right)\\\\ A^T = \\left( \\begin{matrix} 3 &amp; 2\\\\ 4 &amp; 7\\\\ 1 &amp; 5 \\end{matrix} \\right) \\tag{7.12} \\end{equation}\\] Simétrica: Se puede pensar como un espejo en los elementos no de la diagonal, donde se pueden intercambiar los subíndices y la matriz no cambia (\\(a_{21} = a_{12}\\)) \\[\\begin{equation} \\left( \\begin{matrix} 2 &amp; 5 &amp; 3\\\\ 5 &amp; 1 &amp; 4\\\\ 3 &amp; 4 &amp; 9 \\end{matrix} \\right) \\tag{7.13} \\end{equation}\\] Un caso típico de una matriz simétrica es el tensor de esfuerzos, presentado gráficamente en la Figura 7.3 y en la Ecuación (7.14). Figura 7.3: Tensor de esfuerzos, un ejemplo de una matriz simetrica. Tomado de: https://www.efunda.com/formulae/solid_mechanics/mat_mechanics/images/StressState3D.gif \\[\\begin{equation} \\sigma = \\left( \\begin{matrix} \\sigma_{xx} &amp; \\sigma_{xy} &amp; \\sigma_{xz}\\\\ \\sigma_{yx} &amp; \\sigma_{yy} &amp; \\sigma_{yz}\\\\ \\sigma_{zx} &amp; \\sigma_{zx} &amp; \\sigma_{zz} \\end{matrix} \\right) \\tag{7.14} \\end{equation}\\] Inversa: Cuando se multiplica la matriz original por la inversa se obtiene la matriz identidad \\(I\\). Se denota por medio de \\(A^{-1}\\). Aquí no se pretende demostrar o indicar como obtener la matriz inversa, esta se puede calcular con software dedicado para ello. \\[\\begin{equation} A A^{-1} = I \\tag{7.15} \\end{equation}\\] 7.2.2.3 Usos Las matrices son usadas de forma rutinaria aun cuando uno no se de cuenta. Muchas de las operaciones que se realizan se pueden presentar en notación matricial, y de hecho así es como se procesan los datos a lo interno de muchas funciones. Aquí se presentan dos usos típicos y conocidos: resolver sistemas de ecuaciones y obtener predicciones para un modelo lineal. Sistemas de ecuaciones Se puede resolver de dos maneras: haciendo uso de la matriz inversa (Ecuación (7.16)) o de la Regla de Cramer usando determinantes (Ecuación (7.18)). Haciendo uso de la matriz inversa seria de la siguiente manera: \\[\\begin{align*} A x &amp;= b\\\\ A A^{-1} x &amp;= A^{-1} b\\\\ x &amp;= A^{-1} b \\tag{7.16} \\end{align*}\\] Se muestra en el siguiente ejemplo, donde se hace uso de operaciones anteriormente demostradas, primero de forma manual y seguido en R. La idea es descomponer el sistema de ecuaciones en matrices y vectores, una matriz para los coeficientes de las incógnitas, un vector para las incógnitas, y un vector para la solución de la ecuación. A partir de esto se encuentra la matriz inversa de los coeficientes, se multiplica a ambos lados de la ecuación (recordando que al multiplicar \\(A A^{-1} = I\\)), lo que despeja a las incógnitas y se termina de resolver el problema. \\[\\begin{align*} 4 x_1 + 10 x_2 &amp;= 38\\\\ 10 x_1 + 30 x_2 &amp;= 110\\\\ A x &amp;= b\\\\ \\left( \\begin{matrix} 4 &amp; 10\\\\ 10 &amp; 30 \\end{matrix} \\right) \\left( \\begin{matrix} x_1\\\\ x_2 \\end{matrix} \\right) &amp;= \\left( \\begin{matrix} 38\\\\ 110 \\end{matrix} \\right)\\\\ A^{-1} b &amp;= x\\\\ \\left( \\begin{matrix} 1.5 &amp; -0.5\\\\ -0.5 &amp; 2 \\end{matrix} \\right) \\left( \\begin{matrix} 38\\\\ 110 \\end{matrix} \\right) &amp;= \\left( \\begin{matrix} 2\\\\ 3 \\end{matrix} \\right) \\tag{7.17} \\end{align*}\\] En R la inversa de una matriz se obtiene por medio de solve(). A = matrix(data = c(4,10,10,30), nrow = 2) b = c(38,110) Ainv = solve(A) Ainv ## [,1] [,2] ## [1,] 1.5 -0.5 ## [2,] -0.5 0.2 Ainv %*% b ## [,1] ## [1,] 2 ## [2,] 3 Haciendo uso de la Regla de Cramer se muestra en el siguiente ejemplo. La idea es primero calcular el determinante de la matriz de coeficientes. Posteriormente se reemplaza la primer columna por el vector de las soluciones en la matriz de coeficientes, se calcula el determinante de esta nueva matriz y la primer incógnita (\\(x_1\\)) seria la división entre el determinante de la matriz modificada sobre el determinante de la matriz original. Se prosigue de manera similar para el resto de incógnitas. \\[\\begin{equation} \\left| \\begin{matrix} 4 &amp; 10\\\\ 10 &amp; 30 \\end{matrix} \\right| = 4 \\times 30 - 10 \\times 10 = 20\\\\ \\left| \\begin{matrix} 30 &amp; 110\\\\ 10 &amp; 30 \\end{matrix} \\right| = 38 \\times 30 - 110 \\times 10 = 40\\\\ x_1 = \\frac{40}{20} = 2\\\\ \\left| \\begin{matrix} 4 &amp; 10\\\\ 30 &amp; 110 \\end{matrix} \\right| = 4 \\times 110 - 10 \\times 38 = 60\\\\ x_2 = \\frac{60}{20} = 3\\\\ \\tag{7.18} \\end{equation}\\] Obtener predicciones para un modelo lineal El procedimiento se demuestra en la Ecuación (7.19), para una regresión lineal simple (\\(y = b_0 + b_1 x\\)). Aquí la idea es que los valores a predecir no se toman como un vector o serie de vectores, sino que se ordenan en una matriz, donde se incluye una columna de unos para el intercepto (\\(A\\)). Se cuenta con un vector de los coeficientes de la regresión lineal (\\(b\\)). Para obtener las predicciones se realiza una multiplicación entre estas dos matrices, resultando en un vector de las predicciones deseadas (\\(y\\)). \\[\\begin{equation} A = \\left( \\begin{matrix} 1 &amp; 3.24\\\\ 1 &amp; 1.37\\\\ 1 &amp; 4.52\\\\ 1 &amp; 4.63\\\\ 1 &amp; 4.21 \\end{matrix} \\right); b = \\left( \\begin{matrix} 0.5\\\\ 8.1 \\end{matrix} \\right)\\\\ y = b_0 + b_1 x\\\\ y = A b = \\left( \\begin{matrix} 1 &amp; 3.24\\\\ 1 &amp; 1.37\\\\ 1 &amp; 4.52\\\\ 1 &amp; 4.63\\\\ 1 &amp; 4.21 \\end{matrix} \\right) \\left( \\begin{matrix} 0.5\\\\ 8.1 \\end{matrix} \\right) = \\left( \\begin{matrix} 26.74\\\\ 11.59\\\\ 37.08\\\\ 38.03\\\\ 34.56 \\end{matrix} \\right)\\\\ \\text{donde } y_1 = 0.5 \\cdot 1 + 8.1 \\cdot 3.24 = 26.74 \\tag{7.19} \\end{equation}\\] En R se hace uso de las operaciones matriciales convencionales. A = matrix(data = c(rep(1,5), 3.24, 1.37, 4.52, 4.63, 4.21), ncol = 2) A ## [,1] [,2] ## [1,] 1 3.24 ## [2,] 1 1.37 ## [3,] 1 4.52 ## [4,] 1 4.63 ## [5,] 1 4.21 a = c(0.5,8.1) a ## [1] 0.5 8.1 y = A %*% a y ## [,1] ## [1,] 26.744 ## [2,] 11.597 ## [3,] 37.112 ## [4,] 38.003 ## [5,] 34.601 7.3 Eigenvectors y Eigenvalues 7.3.1 Definición Los eigenvectors (vectores característicos) son vectores especiales para una dada matriz, que al ser multiplicados por esta matriz no cambian de dirección, lo cual si ocurre con vectores no característicos. Al multiplicar la matriz por el eigenvector se obtiene un vector, el cual va a ser simplemente el eigenvector escalado (dimensionado) de acuerdo al eigenvalue (valor característico), ver Ecuación (7.20). El eigenvalue (\\(\\lambda\\)) indica si el eigenvector es estirado (\\(&gt;1\\)), comprimido (\\(&lt;1\\)), invertido (valor negativo), o dejado sin cambios (\\(1\\)). \\[\\begin{equation} A x = \\lambda x \\tag{7.20} \\end{equation}\\] donde \\(x\\) es el eigenvector de \\(A\\) y \\(\\lambda\\) es el eigenvalue de \\(A\\) para un eigenvector dado. 7.3.2 Cálculo Primero y los más fácil es encontrar los eigenvalues, para esto se sigue el procedimiento mostrado en la Ecuación (7.21), donde la idea es despejar pasar todo a un lado de la ecuación igualando a cero, factorizar el eigenvector \\(x\\) y multiplicar la matriz identidad por los eigenvalues. Posteriormente a la parte factorizada se le calcula el determinante y se iguala a cero para resolver la ecuación (cuadrática en el caso de una matriz de \\(2 \\times 2\\)) y encontrar los eigenvalues. \\[\\begin{equation} \\left( A - \\lambda I \\right) x = 0\\\\ \\left| A - \\lambda I \\right| = 0 \\tag{7.21} \\end{equation}\\] Se puede mostrar con el siguiente ejemplo. \\[\\begin{equation} A = \\left( \\begin{matrix} 17 &amp; -6\\\\ 45 &amp; -16 \\end{matrix} \\right)\\\\ \\left( A - \\lambda I \\right) = \\left( \\begin{matrix} 17-\\lambda &amp; -6\\\\ 45 &amp; -16-\\lambda \\end{matrix} \\right)\\\\ \\left| \\begin{matrix} 17-\\lambda &amp; -6\\\\ 45 &amp; -16-\\lambda \\end{matrix} \\right| = 0\\\\ (17-\\lambda)(-16-\\lambda) - (45)(-6) = 0\\\\ \\lambda^2 - \\lambda - 2 = 0\\\\ (\\lambda - 2)(\\lambda + 1) = 0\\\\ \\lambda_1 = 2, \\lambda_2 = -1 \\tag{7.22} \\end{equation}\\] Una vez encontrados los eigenvalues se puede corroborar que cumplan con dos criterios: El producto de los eigenvalues es igual al determinante de la matriz original La suma de los eigenvalues es igual a la suma de la diagonal de la matriz original En el caso del ejemplo mostrado en (7.22): \\[\\begin{equation} \\left| \\begin{matrix} 17 &amp; -6\\\\ 45 &amp; -16 \\end{matrix} \\right| = (17)(-16) - (45)(-6) = -2\\\\ \\lambda_1 = 2, \\lambda_2 = -1\\\\ \\lambda_1 \\times \\lambda_2 = -2 \\to determinante = -2\\\\ \\lambda_1 + \\lambda_2 = 1 \\to 17 - 16 = 1 \\tag{7.23} \\end{equation}\\] Aquí se va a obviar el procedimiento manual del cálculo de los eigenvectors, pero e muestra el resultado de los eigenvectors del ejemplo mostrado en (7.22). Para \\(\\lambda_1\\): \\[\\begin{equation} A x = \\lambda x\\\\ \\left( \\begin{matrix} 17 &amp; -6\\\\ 45 &amp; -16 \\end{matrix} \\right) \\left( \\begin{matrix} 2\\\\ 5 \\end{matrix} \\right) = \\left( \\begin{matrix} 4\\\\ 10\\\\ \\end{matrix} \\right)\\\\ \\left( \\begin{matrix} 17 &amp; -6\\\\ 45 &amp; -16 \\end{matrix} \\right) \\left( \\begin{matrix} 2\\\\ 5 \\end{matrix} \\right) = 2 \\left( \\begin{matrix} 2\\\\ 5\\\\ \\end{matrix} \\right) \\tag{7.24} \\end{equation}\\] Para \\(\\lambda_2\\): \\[\\begin{equation} A x = \\lambda x\\\\ \\left( \\begin{matrix} 17 &amp; -6\\\\ 45 &amp; -16 \\end{matrix} \\right) \\left( \\begin{matrix} 1\\\\ 3 \\end{matrix} \\right) = \\left( \\begin{matrix} -1\\\\ -3\\\\ \\end{matrix} \\right)\\\\ \\left( \\begin{matrix} 17 &amp; -6\\\\ 45 &amp; -16 \\end{matrix} \\right) \\left( \\begin{matrix} 1\\\\ 3 \\end{matrix} \\right) = -1 \\left( \\begin{matrix} 1\\\\ 3\\\\ \\end{matrix} \\right) \\tag{7.25} \\end{equation}\\] En el siguiente ejemplo se muestra como obtener en R tanto los eigenvectors como eigenvalues, lo que se realiza mediante la función eigen(). A = matrix(data = c(17,45,-6,-16), nrow = 2) eigen(A) ## eigen() decomposition ## $values ## [1] 2 -1 ## ## $vectors ## [,1] [,2] ## [1,] 0.3713907 0.3162278 ## [2,] 0.9284767 0.9486833 Como se menciona en los términos generales a continuación, y se ve comparando el resultado de (7.24) y (7.25) con el resultado de R, los valores de los elementos de los eigenvectors no son de importancia, sino la relación entre esos elementos. En (7.24) se muestra que el eigenvector es \\(x_1=(2,5)\\), pero en el ejemplo anterior es \\(x_1=(0.371,0.928)\\); si se hace la relación del segundo elemento por el primer elemento se obtiene \\(2.5\\) en ambos casos, lo mismo pasa para \\(x_2\\) donde la relación es de \\(3\\). En términos generales: Los eigenvectors apuntan en la dirección que se esparcen los datos, y lo que importa es la relación entre los elementos del vector, no sus magnitudes. Los eigenvalues es cuaánto se esparcen los datos. En matrices simétricas los eigenvectors van a ser ortogonales. "],
["introducción-a-estadística.html", "Capítulo 8 Introducción a estadística 8.1 Introducción 8.2 Tipos 8.3 Modelos 8.4 Nomenclatura 8.5 Variables 8.6 Métodos de análisis 8.7 Muestreo 8.8 Incertidumbre", " Capítulo 8 Introducción a estadística 8.1 Introducción Este capitulo da una introducción a estadística, en qué consiste, para qué se usa, los tipos, modelos y nomenclatura que se usan. Estadística es la ciencia que estudia la manera en que se recolecta, se analiza, se interpreta la información proveniente de una población, así como el modo en que se extrapola esos resultados a otros casos similares. Tiene como objetivo principal analizar datos y transformarlos en información útil para tomar decisiones y sacar conclusiones, donde hay incertidumbre y variación. 8.2 Tipos En general hay dos tipos de estadística: Descriptiva: Lo que se hace es recopilar, organizar, resumir y presentar datos para facilitar su análisis y aplicación (Tablas y/o gráficos). En este tipo es donde se ubica lo que se conoce como análisis exploratorio de datos (AED o EDA en inglés), y consiste en el proceso para utilizar herramientas estadísticas (como gráficas, medidas de tendencia central y medidas de variación), con la finalidad de investigar conjuntos de datos para comprender sus características importantes. Un ejemplo se observa en la Figura 8.1. Figura 8.1: Ejemplo de estadística desciptiva y análisis exploratorio de datos. Tomado de: http://www.universoformulas.com/imagenes/estadistica/descriptiva/estadistica-descriptiva.jpg Inferencial: El objetivo es hacer inferencias en base a una muestra, con la intención de generalizar a una población de interés (Figura 8.2). En este caso los resultados se usan para corroborar o refutar creencias sobre la población de interés y sacar conclusiones con sustento estadístico. Figura 8.2: Proceso de estadística inferencial. Tomado de: http://www.universoformulas.com/imagenes/estadistica/inferencia/proceso-estadistica-inferencial.jpg 8.3 Modelos Determinístico: Donde las mismas entradas producirán las mismas salidas, sin variación, esto debido a que no hay incertidumbre en los datos, se conocen con certeza. Probabilístico (aleatorio, estocástico): Donde los resultados (salidas) dependen de las entradas y de los componentes aleatorios (incertidumbre), pudiendo producir resultados distintos a partir de una misma entrada. Los resultados se expresan en términos de probabilidad, que refleja la incertidumbre del modelo. 8.4 Nomenclatura En estadística se maneja cierta nomenclatura (definiciones) que es importante conocer y saber usar apropiadamente. Dentro de las definiciones que se van a trabajar a los largo del curso son: Población Conjunto con alguna característica de interés Normalmente muy grande para poder abarcarlo (estudiarlo) por completo Muestra Subconjunto de la población de interés sobre el cual se hacen las observaciones y análisis Debería ser representativa Variable Característica observable de los componentes de una población (muestra) y que puede tomar distintos valores Observación o dato Valor obtenido para los componentes de la muestra, como resultado de algún tipo de medición Los conceptos de población y muestra se muestran en la siguiente figura. Figura 8.3: Ejemplo de una población a la cual se le toma una muestra. Tomado de: http://aprendiendoadministracion.com/wp-content/uploads/2016/01/muestra-estadistica.jpg No hay que confundir los conceptos de muestra estadistica y muestra geologica. Una muestra estadistica es un conjunto de observaciones o datos, mientras que una muestra geologica en terminos estadisticos es una observacion o dato. Entonces una muestra estadistica es un conjunto de muestras geologicas. 8.5 Variables Hay dos tipos generales de variables: Cualitativa y Cuantitativa. Esto es presentado por Triola (2004), así como la subdivisión de dichos tipos de variables, presenta a continuación. 8.5.1 Cualitativa Lo que se conoce como datos categóricos, donde las entradas o datos toman valores de clases o niveles. Dentro de R estas son las que so codifican como factores. Dentro de este tipo podemos encontrar una subdivisión: Nominal (nom): donde las clases o niveles no tienen un orden específico, Ordinal (ord): donde las clases o niveles tienen un orden relativo, que se puede pensar como una escala. Ejemplos de datos cualitativos son: color (nom), grado de meteorización (ord), nivel de fisuramiento (ord), intensidad (ord), escala de dureza de Mohs (ord), etc. 8.5.2 Cuantitativa Corresponden con datos numéricos, es el tipo de dato que por lo general se asocia a técnicas estadísticas y de análisis de datos, pero no se deben obviar los datos cualitativos. Se puede dividir en: Intervalo: donde datos los datos se encuentran igualmente espaciados y no hay un cero absoluto, o sea pueden haber valores negativos, Razón: donde hay un cero absoluto, correspondiendo con valores positivos. También se puede dividir en: Discretos (disc): por medio de conteos, corresponde con números enteros, Continuos (cont): por medio de mediciones (magnitudes), corresponde con infinitos valores entre enteros, puede expresarse con decimales. Ejemplos de datos cuantitativos son: edad (cont), orientación (cont), espesor (cont), número de fisuras (disc), magnitud (cont), esfuerzos (cont), deformaciones (cont), longitud (cont), superficie (cont), volumen (cont), tiempo (cont), temperatura (cont), etc. En las ciencias geológicas y de la tierra se manejan varios tipos de datos especiales, que van a requerir de adaptar técnicas de análisis generales para los casos específicos. Dentro de estos datos se tiene: Cerrados: Las variables son expresadas como proporciones y suman hasta un valor fijo total (Ej: 100%), en general el interés es más en la razón entre las variables y no el valor de la variable. Composiciones (Ej: tipo de roca) representan la mayoría de este tipo de datos. Espaciales: La(s) variable(e) de interés posee(n) componentes en 2D o 3D representando su distribución espacial (ubicación en un área determinada). Ejemplos: distribución de un tipo de fósil, cambios en el espesor de una capa de arenisca, distribución de trazas en el agua subterránea. Direccionales: Los datos son expresados en ángulos/orientaciones entre 0 y 360 y pueden tener ademas una inclinación. Ejemplos: rumbo o buzamiento de una capa, orientación de fósiles elongados, dirección del flujo de una colada de lava. Una representación gráfica de los tipos de datos se observa en la Figura 8.4. Figura 8.4: Representación gráfica de los tipos de datos. Los datos cerrados (closed) y direccionales (directional) son más típicos de la ciencias geológicas (Swan &amp; Sandilands, 1995). 8.6 Métodos de análisis Existen diferentes métodos de análisis de datos. Éstos van a depender del tipo de dato y la cantidad de variables que se tengan. Dentro de las principales metodologías se tienen y una representación gráfica de las variable de interés se presenta en la Figura 8.5: Univariable: Se analiza cada variable por separado, el interés está en enfocarse en una variable por si sola a la vez y sin considerar la relación que pueda tener con otras. Bivariable: Se analizan dos variables en conjunto, enfocándose en su relación y/o dependencia. Es la versión simple de un análisis multivariable. Multivariable: Cuando el análisis involucra 2 o más variable de interés a la vez y se requiere determinar la relación e interacción entre dichas variables. Secuencias: Cuando los datos se presentan como secuencias (por lo general implicando algún patrón o ciclicidad) en el tiempo/espacio, donde la forma más simple es un análisis bivariable donde una de las variables es el tiempo/espacio. Espacial: Cuando la ubicación de las muestras es de interés y se requiere entender o determinar cómo se dispone una variable en un área determinada. Por lo general tres (o cuatro) variables analizadas a la vez, donde dos (o tres) corresponden con la ubicación espacial y la otra corresponde con alguna medida de interés geológico, existen opciones univariables y multivariables. Figura 8.5: Representación gráfica de las variables correspondientes a diferentes metodologías de análisis de datos (Swan &amp; Sandilands, 1995). 8.7 Muestreo El muestreo es un procedimiento para obtener datos/observaciones de una población, con la finalidad de usar esta información para realizar inferencias acerca de dicha población (Davis, 2002). Las muestras son subconjuntos de los datos. El conjunto de todas las muestras que se pueden obtener de la población se denomina espacio muestral. La(s) muestra(s) debe(n) ser representativa(s), donde esto va a depender de la adecuada implementación de alguna de las técnicas de muestreo presentadas más adelante. En la Figura 8.6 se presenta y retoma la el concepto de muestra geológica, donde la población de interés es un afloramiento especifico, pero dicho afloramiento es demasiado grande para muestrear por completo, por lo que se deben tomar varias observaciones (muestras geológicas) para obtener una muestra que representa a la población meta. Figura 8.6: Representación de una población de interés geológico donde es necesaria la toma de observaciones (muestras geológicas) para obtener una muestra representativa de dicha población (Trauth, 2015). 8.7.1 Tipos Se presentan los tipos o técnicas de muestreo más comunes y que permiten obtener una muestra representativa de la población, descritos en más detalle en Triola (2004). 8.7.1.1 Aleatorio Simple Una muestra se selecciona de modo que todos los elementos de la población tengan la misma probabilidad de ser elegidos. En general es poco recomendado cuando la población es muy grande o heterogénea (Figura 8.7). Figura 8.7: Representación de un muestreo aleatorio simple. Tomado de: http://www.universoformulas.com/imagenes/estadistica/inferencia/muestreo-probabilistico.jpg 8.7.1.2 Sistemático Se elige un punto de partida y luego seleccionamos cada k-ésimo (por ejemplo cada quincuagésimo) elemento en la población. Conlleva algunos riesgos cuando el marco muestral es repetitivo o de naturaleza cíclica (Figura 8.8). Figura 8.8: Representación de un muestreo sistematico. Tomado de: http://www.universoformulas.com/imagenes/estadistica/inferencia/muestreo-sistematico.jpg 8.7.1.3 Estratificado Consiste en definir previamente los estratos (grupos) que posee una población a partir de características comunes entre sus elementos y distintas con los elementos de los otros estratos. A partir de eso se deben tomar muestras aleatorias en cada estrato (Figura 8.9). Figura 8.9: Representación de un muestreo estratificado. Tomado de: http://www.universoformulas.com/imagenes/estadistica/inferencia/muestreo-estratificado.jpg 8.7.1.4 Bloques (conglomerados) Cuando la población está agrupada en conglomerados naturales, después se seleccionan aleatoriamente algunos de estos conglomerados, y luego se elige a todos los miembros de los conglomerados seleccionados o se muestrean los conglomerados con alguna otra técnica. Se usa cuando los conglomerados son muy heterogéneos y no existen muchas diferencias entre conglomerados (Figura 8.10). Figura 8.10: Representación de un muestreo en bloques o por conglomerados. Tomado de: http://www.universoformulas.com/imagenes/estadistica/inferencia/muestreo-estratificado.jpg En geología muchas veces nos vemos forzados por disposición de afloramientos (clustered) o accesibilidad (traverse – cortes de carretera, ríos, etc), especialmente en climas tropicales, ver Figura 8.11, siendo estas las localidades donde se encuentra disponible la población a estudiar. Idealmente un muestreo exhaustivo y de alta resolución seria el que se define en grilla (regular). Figura 8.11: Típicas disposiciones de afloramientos en ciencias geológicas, especialmente en climas tropicales (Swan &amp; Sandilands, 1995). 8.8 Incertidumbre Además de los retos de recolectar muestras representativas, las mediciones geológicas tienen incertidumbre. No es posible realizar una medición exacta (que siempre es la misma) y equipos de alta precisión aún van a tener incertidumbre, reducida pero existe. El objetivo durante la recolección de muestras es reducir la incertidumbre a la hora de tomar mediciones, haciéndolas más precisas y exactas. Con lo anterior se puede diferenciar entre error reproducible y sistemático. El error reproducible es donde se tienen diferencias entre mediciones repetidas y se asocia con precisión, donde se puede detectar por medio de la dispersión de los datos, a menor dispersión mayor precisión. El error sistemático es más difícil de detectar, no se puede estimar únicamente a partir de mediciones repetidas, a menos que se tenga un valor de referencia (valor verdadero); se asocia con exactitud, donde mientras más cerca del valor de referencia mayor la exactitud. Estos conceptos se pueden visualizar en la Figura 8.12. Figura 8.12: Diferencia entre precisión y exactitud. Tomado de: https://www.diferenciador.com/diferencia-entre-exactitud-y-precision/ Referencias "],
["estadística-descriptiva-univariable.html", "Capítulo 9 Estadística Descriptiva Univariable 9.1 Introducción 9.2 Tablas de frecuencias 9.3 Gráficas 9.4 Resúmenes numéricos 9.5 Resumen general", " Capítulo 9 Estadística Descriptiva Univariable 9.1 Introducción Como se mencionó en la introducción, la estadística univariable se enfoca en describir una variable a la vez, sin considerar la relación con cualquier otra variable. La descripción y métodos a usar va a estar en función del tipo de variable: cualitativa o cuantitativa. Las tres formas en que se pueden describir los datos son: tabularmente (datos cualitativos o cuantitativos discretos), gráficamente, y por medio de resúmenes numéricos. 9.2 Tablas de frecuencias La idea es presentar un resumen de los datos, por lo general por medio de conteos de observaciones por clases. Con datos cualitativos simplemente se cuenta la cantidad de observaciones en cada clase, para datos cuantitativos discretos se cuenta la cantidad de observaciones para cada valor único observado, y para datos cuantitativos continuos lo común es dividir el rango de los valores en una cierta cantidad de clases y contar las observaciones que caen en cada clase. Las tablas pueden contener diferentes tipos de frecuencias: Absoluta: Conteo de elementos por clase (\\(n\\)) Relativa: Porcentaje de elementos por clase Absoluta acumulada: Conteo de elementos que toman un valor menor o igual a una clase, suma a la cantidad de observaciones (\\(N\\)) Relativa acumulada: Porcentaje de elementos que toman un valor menor o igual a una clase, suma a 1 o \\(100\\%\\) Para datos cualitativos hay diferentes formas de generar tablas de frecuencias, y dependiendo de lo que se quiera mostrar. Lo más fácil es usar tabyl del paquete janitor y formatearla (Tabla 9.1). Se va a ejemplificar con los datos gss_cat y la variable marital que vienen con el paquete forcats. Por defecto cuando se usa solo una variable realiza el conteo y calcula el porcentaje de cada categoría. En el ejemplo se redondea a 3 dígitos y se ordena de forma descendente de acuerdo al conteo. gss_cat %&gt;% tabyl(marital) %&gt;% adorn_rounding(3) %&gt;% arrange(-n) Tabla 9.1: Tabla de frecuencias para una variable cualitativa marital n percent Married 10117 0.471 Never married 5416 0.252 Divorced 3383 0.157 Widowed 1807 0.084 Separated 743 0.035 No answer 17 0.001 En el caso de datos cuantitativos continuos, donde hay que dividir el rango de valores en cierta cantidad de clases, no hay una manera única o definida de escoger el número de clases, pero hay algunas sugerencias. Una de las sugerencias se muestra en la Ecuación (9.1). \\[\\begin{equation} k = 1 + 3.33 log_{10}(N) \\tag{9.1} \\end{equation}\\] donde \\(k\\) es el número de clases y \\(N\\) es el total de observaciones (datos). Otra sugerencia se muestra en la Tabla 9.2 Tabla 9.2: Sugerencia para el número de clases de acuerdo a la cantidad de observaciones N k &lt; 50 5 - 7 50 - 100 6 - 10 100 - 250 7 - 12 &gt; 250 10 - 20 Un ejemplo de una tabla de frecuencia se muestra a continuación en la Tabla 9.3, usando los datos: 3.1 4.5 2.9 2.7 3.8 5.1 4.9 3.5 2.1 4.2 2.2 1.8 2.5 3.6 3.6 4.3 5.1 6.1 5.7 2.8 2.8 3.7 3.5 4.4 2.5 5.6 5.1 4.7 4.9 4.2 3.6 4.1 4.1 3.7 2.9 6.2 4.8 3.9 4.6 3.1. Tabla 9.3: Tabla de frecuencias para una variable continua Clase Frecuencia absoluta Frecuencias relativa Frec. Abs. Acumulada Frec. Rel. Acumulada 1-2 1 0.025 1 0.025 2-3 9 0.225 10 0.250 3-4 11 0.275 21 0.525 4-5 12 0.300 33 0.825 5-6 5 0.125 38 0.950 6-7 2 0.050 40 1.000 El código para generar la tabla fue el siguiente, donde x corresponde con el vector de datos, breaks con las divisiones para las clases, y labels con los nombres de la clases: Freq(x, breaks = 1:7, labels = c(&#39;1-2&#39;,&#39;2-3&#39;,&#39;3-4&#39;,&#39;4-5&#39;,&#39;5-6&#39;,&#39;6-7&#39;)) 9.3 Gráficas Dependiendo del tipo de variable así será la representación gráfica. Para datos cualitativos y cuantitativos discretos lo típico es usar gráficos de barras, mientras que para datos cuantitativos continuos se usa el histograma o gráfico de caja. El gráfico de barras correspondiente a los datos de la Tabla 9.1 se muestra en la Figura 9.1. Para refrescar cómo se pueden construir estos gráficos ir a Gráficos. La finalidad de esto gráficos es brindar una idea del balance entre las categorías o niveles de una clase y ver si hay alguna categoría o nivel que sobre salga tanto por valores altos como por valores bajos. Figura 9.1: Gráfico de barras para una variable cualitativa El histograma (Figura 9.2) permite dar una primera mirada al tipo de distribución de los datos: Si las alturas de las barras son similares se dice que tiene distribución tipo “uniforme”. Si las alturas son mayores en la zona central se dice que tiene forma tipo “campana” (distribución normal) y puede ser simétrica o asimétrica, con sesgo hacia el lado positivo o al lado negativo. Si hay barras muy alejadas del grupo, se dice que son datos atípicos. Figura 9.2: Ejemplos de histogramas. Tomado de: http://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/ El gráfico de caja (Figura 9.3) también puede brindar una idea de la distribución de los datos. Recordar que este tipo de gráfico se compone de los cuartiles \\(Q_1\\) (límite inferior de la caja), \\(Q_2\\) o mediana (barra dentro de la caja), y \\(Q_3\\) (límite superior de la caja). Si los datos presentan una distribución normal tipo campana la mediana aparecería cerca del centro de la caja. Si los datos presentan una asimetría la mediana va a estar más cerca de uno de los bordes, y van a observarse puntos al extremo opuesto de la mediana, los cuales podrían representar valores extremos o atípicos. Figura 9.3: Gráficos de caja, donde las cruces representan la media. A mostrando datos con distribución normal donde la mediana está cerca del medio de la caja. B mostrando datos asimétricos donde la mediana está más cerca de uno de los extremos y además se presentan posibles valores atípisoc (puntos). Siguiendo con los datos del ejemplo de la Tabla 9.3, se pueden generar los histogramas (Figura 9.4) correspondientes a las columnas. Para refrescar cómo se pueden construir estos gráficos ir a Gráficos. Figura 9.4: Histogramas de los datos de ejemplo. A Frecuencia absoluta, B Frecuencia relativa, C Frecuencia absoluta acumulada, D Frecuencia relativa acumulada. 9.4 Resúmenes numéricos Dependiendo del dato que se quiera presentar, éstos se pueden dividir en diferentes tipos. En esta sección se muestran y explican estos diferentes tipos, así como las funciones para calcularlas, enfocándose más en datos cuantitativos continuos. 9.4.1 Medidas de tendencia central Esta estadística es la más conocida, ya que representa el centro de la distribución de los datos, siendo la más conocida y usada la media aritmética o promedio (\\(\\bar{x}\\)). Además de la media, otras medidas de tendencia central son la media ponderada, media geométrica (\\(\\bar{x}_g\\)), mediana, y moda. 9.4.1.1 Media La media aritmética o promedio se muestra en la Ecuación (9.2) y tiene la característica de ser sensible a valores extremos (atípicos), por lo que no es muy recomendable para distribuciones asimétricas. \\[\\begin{equation} \\bar{x} = \\frac{\\sum_{i=1}^{N}{x_i}}{N} \\tag{9.2} \\end{equation}\\] La media ponderada (Ecuación (9.3)) se usa cuando los elementos del vector tienen diferentes pesos (\\(f\\)). La media aritmética es de hecho una simplificación de la media ponderada donde todos los elementos tienen el mismo peso. \\[\\begin{equation} \\bar{x} = \\frac{\\sum_{i=1}^{k}{f_i m_i}}{\\sum f} \\tag{9.3} \\end{equation}\\] 9.4.1.2 Media geométrica La media geométrica (Ecuación (9.4)) es típicamente usada para distribuciones con asimetría positiva (hacia la derecha), y es menos sensible a valores extremos (atípicos). Tiene la condición de que únicamente aplica para valores positivos, no pueden ser cero ni negativos. \\[\\begin{equation} \\bar{x}_g = \\left(x_1 * x_2 * \\cdots * x_N \\right)^\\frac{1}{N} \\tag{9.4} \\end{equation}\\] Los anteriores conceptos se muestran con un ejemplo donde se tiene las notas de exámenes (70, 50, 80, 95) y cada examen tiene diferente peso (20, 20, 20, 40). La media sería \\(\\bar{x} = \\frac{70+50+80+95}{4} = 73.75\\), la media ponderada sería \\(\\bar{x} = \\frac{70*20+50*20+80*20+95*40}{4} = 78\\), y la media geométrica sería \\(\\bar{x}_g = \\left(70 * 50 * 80 * 95 \\right)^\\frac{1}{4} = 71.81\\). En R se realizaría de la siguiente manera: notas = c(70, 50, 80, 95) pesos = c(20, 20, 20, 40) mean(notas) ## [1] 73.75 weighted.mean(notas, pesos) ## [1] 78 Gmean(notas) ## [1] 71.81587 9.4.1.3 Mediana La mediana (Ecuación (9.5)) es el valor de los datos que los divide en dos partes iguales. No se ve afectada por valores extremos, lo que la hace una medida robusta para distribuciones asimétricas. \\[\\begin{equation} \\text{si N es impar} \\rightarrow Me = X_\\frac{N+1}{2}\\\\ \\text{si N es par} \\rightarrow Me = 0.5 \\left(X_\\frac{N}{2} + X_\\frac{N+1}{2} \\right)\\\\ \\tag{9.5} \\end{equation}\\] Ejemplos de la mediana y cómo no se ve afectada por valores extremos serían: \\(1, 2, 4, 5, 6, 6, 8 \\rightarrow Me = 5\\) \\(1, 2, 4, 5, 6, 6, 6, 8 \\rightarrow Me = (5+6)/2 = 5.5\\) \\(1, 2, 4, 5, 6, 6, 80 \\rightarrow Me = 5\\) 9.4.1.4 Moda La moda es el valor más frecuente o que se repite más, es más utilizada para datos discretos, aunque para datos continuos se puede calcular la curva de densidad de los datos y encontrar el pico de dicha curva. También existe la posibilidad de que los datos presenten más de una moda (Figura 9.5). Figura 9.5: Distribuciones de datos con más de una moda (Trauth, 2015). En variables con distribución simétrica los valores de media, mediana y moda coinciden. En las distribuciones asimétricas se van distanciando conforme incrementa la asimetría (Figura 9.6). Figura 9.6: Diferencias entre medidas de tendencia central conforme la distribución presenta mayor asimetría (Trauth, 2015). 9.4.2 Medidas de dispersión Estas medidas brindan una idea de la dispersión o variabilidad que presentan los datos con respecto a una medida de tendencia central, típicamente la media. Siempre son positivas e idealmente para reducir la incertidumbre se busca que estas medidas sean lo más pequeñas posibles. 9.4.2.1 Rango (\\(R\\)) El rango toma en cuenta el valor mínimos y máximo de la variable, sin tomar en cuenta el resto de valores intermedios. Se puede expresar de dos maneras: como un vector indicando los valores máximo y mínimo, o como la diferencia entre estos valores (Ecuación (9.6)). Una desventaja de presentarlo como al diferencia es que se pierde el contexto de la escala de los datos, ya que una misma diferencia puede estar presente a diferentes escalas. Ejemplo: Los datos pueden tener un rango de 5, pero los datos pueden estar entre 100 y 95 o 25 y 20. Para ambos casos aunque el rango es el mismo no confiere la misma información; en el primer caso la dispersión se puede considerar menor ya que representa un 5% con respecto al valor máximo, mientras que en el segundo caso representa un 20%. \\[\\begin{equation} (a) R = (x_{min},x_{max})\\\\ (b) R = x_{max}-x_{min}\\\\ \\tag{9.6} \\end{equation}\\] Una ventaja de esta medida es la facilidad de calcularla e interpretarla, mientras que la desventaja es que considera únicamente los datos extremos y no la totalidad de los mismos, por lo que pudiera verse afectada por valores atípicos. Usando los datos 2, 6, 11, 8, 11, 4, 7, 5 como ejemplo se puede determinar que \\(R=(2,11)=9\\). En R la función range calcula el rango dando los valores mínimo y máximo, si se quiere la diferencia se puede usar diff, donde toma el segundo elemento y le resta el primero. vec = c(2, 6, 11, 8, 11, 4, 7, 5) range(vec) ## [1] 2 11 diff(range(vec)) ## [1] 9 9.4.2.2 Varianza (\\(s^2\\)) La varianza toma en cuenta todos los datos y su diferencia al cuadrado con respecto a la media, esto se puede visualizar en la Figura 9.7, donde la línea vertical roja representa la media, los puntos azules los datos, y las lineas horizontales azules las diferencias cuadradas entre la media y los datos. Una desventaja es que al elevar las diferencias al cuadrado la medida NO se encuentra en las mismas unidades (escala) de la variable original. Figura 9.7: Representación de la varianza La varianza se calcula mediante la Ecuación (9.7): \\[\\begin{equation} s^2 = \\frac{\\sum_{i=1}^{N}{\\left(x_i - \\bar{x}\\right)^2}}{N-1}\\\\ \\sigma^2 = \\frac{\\sum_{i=1}^{N}{\\left(x_i - \\mu\\right)}}{N} \\tag{9.7} \\end{equation}\\] En la ecuación anterior \\(s^2\\) es la varianza muestral, \\(\\sigma^2\\) la varianza poblacional, \\(\\bar{x}\\) la media muestral, y \\(\\mu\\) la media poblacional. En el caso de la varianza muestral se usa en el denominador \\(N-1\\), lo que se conoce como grados de libertad (\\(df \\ o \\ \\nu\\)). Porqué \\(N-1\\), porque para poder calcular la varianza se necesita saber la media, y si se conoce la media no importa la cantidad de datos que tenga el último dato va a estar condicionado para satisfacer el valor de la media, o sea todos los datos menos uno pueden variar libremente. Un ejemplo de puede mostrar con los datos 8, 10, 5, 12, 10, 15. En este caso \\(N=6,\\ \\bar{x}=60/6=10,\\ \\sum_{i=1}^{N}{\\left(x_i - \\bar{x}\\right)^2}=58,\\ s^2=\\frac{58}{6-1}=11.6 \\ unidades^2\\). En R se usa la función var para obtener la varianza de un vector. vec = c(8, 10, 5, 12, 10, 15) var(vec) ## [1] 11.6 9.4.2.3 Desviación estándar (\\(s\\)) Esta medida es la más utilizada, ya que se encuentra en las mismas unidades (escala) de los datos originales, dado que es simplemente la raíz cuadrada de la varianza (Ecuación (9.8)), lo que la hace más fácil de interpretar y comprender. \\[\\begin{equation} s = \\sqrt{s^2} = \\sqrt{\\frac{\\sum_{i=1}^{N}{\\left(x_i - \\bar{x}\\right)^2}}{N-1}}\\\\ \\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{\\sum_{i=1}^{N}{\\left(x_i - \\mu\\right)}}{N}} \\tag{9.8} \\end{equation}\\] Siguiendo con los datos del ejemplo de la varianza se tiene que \\(s = \\sqrt{s^2} = \\sqrt{11.6} = 3.4 \\ unidades\\). En R se tiene la función sd para calcular esta medida directamente. sd(vec) ## [1] 3.405877 Visualmente (Figura 9.8) se puede mostrar como al incrementar la desviación estándar la curva de densidad de los datos se aplana más. Figura 9.8: Curvas de densidad con diferentes desviaciones estándar, donde a mayor desviación estándar más aplanada la curva y a menor desviación estándar más empinada la curva Como se va a ver más adelante hay una relación entre la desviación estándar y la media que se ve reflejada en la distribución normal (Figura 9.9). Típicamente se asocia que el 68% de los datos se encuentran a \\(1\\ s\\) de la media, el 95% de los datos se encuentran a \\(2\\ s\\) de la media, y el 99% de los datos se encuentran a \\(3\\ s\\) de la media. Figura 9.9: Relación entre desviación estándar y media en la distribución normal. Tomado de: http://algebra2.thinkport.org/module3/images/xyz-page2-graph.jpg 9.4.2.4 Rango intercuartil (\\(IQR\\)) Mide la dispersión central de los datos, ya que se toma únicamente el 50% central de los datos, al ser la diferencia entre el tercer cuartil (\\(Q_3\\)) y primer cuartil (\\(Q_1\\)), ver Ecuación (9.9). Se considera más robusta que la desviación estándar al no tomar en cuenta valores extremos. De hecho, valores extremos o atípicos suelen identificarse cuando se desvían más de 3 veces el \\(IQR\\) con respecto a la mediana. \\[\\begin{equation} IQR = Q_3 - Q_1 \\tag{9.9} \\end{equation}\\] En R se tiene el comando IQR para calcular esta medida, como se muestra a continuación. set.seed(10) vec = rchisq(60,3) IQR(vec) ## [1] 2.533682 9.4.2.5 Mediana de la desviación absoluta (\\(mad\\)) Se calcula usando la Ecuación (9.10) y en general se considera más robusta que la desviación estándar y el rango intercuartil cuando se presentan datos extremos o atípicos y cuando la distribución es asimétrica. Se le aplica un factor de conversión de 1.4826 para cumplir con supuestos de normalidad. \\[\\begin{equation} mad = Me\\left(\\left|x_i - Me(x)\\right|\\right) * 1.4826 \\tag{9.10} \\end{equation}\\] Usando los datos generados para \\(IQR\\) se puede calcular esta medida usando mad. mad(vec) ## [1] 1.769578 9.4.2.6 Coeficiente de variación (\\(cv\\)) Esta es una medida estandarizada (Ecuación (9.11)), lo que nos va a permitir comparar la dispersión entre dos poblaciones distintas e incluso, comparar la variación producto de dos variables diferentes (que pueden provenir de una misma población). Si comparamos la dispersión en varios conjuntos de observaciones tendrá menor dispersión aquella que tenga menor coeficiente de variación. \\[\\begin{equation} cv = \\frac{s}{\\bar{x}}*100 \\tag{9.11} \\end{equation}\\] Un ejemplo se muestra a continuación. Para un grupo de datos \\(\\bar{x}=20, s = 4\\), entonces \\(cv = 4/20 = 0.2 = 20\\%\\). Para un segundo grupo \\(\\bar{x} = 48, s = 6\\), entonces \\(cv = 6/48 = 0.125 = 12.5\\%\\). Se concluye que el primer grupo tiene mayor variabilidad relativa con respecto a su media, lo que no se podría concluir de solo fijarse en la desviación estándar. En R el paquete DescTools (Signorell, 2020) tiene la función CoefVar, y si la aplicamos a los datos generados para \\(IQR\\) tenemos: CoefVar(vec) ## [1] 0.7334606 9.4.3 Medidas de posición Estas medidas corresponden con números que distribuyen los datos ordenados de la muestra en grupos de aproximadamente del mismo tamaño, con el propósito de resaltar su ubicación relativa. Estos números se denominan cuantiles en forma genérica. 9.4.3.1 Cuartiles Los cuartiles son los tres valores de la variable que dividen a un conjunto de datos ordenados en cuatro partes iguales (Figura 9.10). Primer Cuartil (\\(Q1\\)): A la izquierda de \\(Q1\\) están incluidos 25% de los datos (aproximadamente), y a la derecha de \\(Q1\\) están el 75% de los datos (aproximadamente). Segundo Cuartil (\\(Q2\\)): Igual que la mediana divide al grupo de datos en dos partes, cada una con el 50% de los datos (aproximadamente). Tercer Cuartil (\\(Q3\\)): A la izquierda de \\(Q3\\) están incluidos 75% de los datos (aproximadamente), y a la derecha de \\(Q3\\) están el 25% de los datos (aproximadamente). Figura 9.10: Representación de los cuartiles. Tomado de: http://dieumsnh.qfb.umich.mx/estadistica/medidasd%20de%20posicion.htm 9.4.3.2 Deciles Los deciles son los nueve valores de la variable que dividen a un conjunto de datos ordenados en diez partes iguales (Figura 9.11). Primer Decil (D1): A la izquierda de D1 están incluidos 10% de los datos (aproximadamente), y a la derecha de D1 están el 90% de los datos (aproximadamente). Segundo Decil (D2): A la izquierda de D1 están incluidos 20% de los datos (aproximadamente), y a la derecha de D1 están el 80% de los datos (aproximadamente) D5 coincide con la mediana. Figura 9.11: Representación de los deciles. Tomado de: http://dieumsnh.qfb.umich.mx/estadistica/medidasd%20de%20posicion.htm En R los cuantiles se pueden obtener con la función quantile y definiendo el argumento probs, donde corresponde con un vector de valores entre 0 y 1, para los cuantiles deseados. quantile(vec, probs = c(.25,.5,.75)) # para cuartiles ## 25% 50% 75% ## 1.212222 2.156949 3.745904 quantile(vec, probs = seq(.1,.9,.1)) # para deciles ## 10% 20% 30% 40% 50% 60% 70% 80% ## 0.6815646 0.8654207 1.3201193 1.8411737 2.1569488 2.4933732 3.1549380 4.0521953 ## 90% ## 5.4816144 9.4.4 Medidas de forma Estas medidas se usan para describir numéricamente la forma aproximada de la distribución de los datos. 9.4.4.1 Asimetría (skewness) Se dice que una distribución es asimétrica cuando la media, mediana y moda no coinciden y la distribución muestra una forma diferente a la “campana”, con una cola más alargada que la otra. Se pueden identificar dos tipos de asimetría: positiva (hacia la derecha) o negativa (hacia la izquierda), ver Figuras 9.12 y 9.13. Figura 9.12: Asimetría con el nombre de la dirección: hacia la izquierda o derecha. Tomado de: http://2.bp.blogspot.com/_bXZg80tWNts/S4SBFgzljbI/AAAAAAAAAd0/72BnnqmoA7g/s320/asimetria.gif Figura 9.13: Asimetría con el nombre del signo: positiva o negativa. Tomado de: http://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/ Si la asimetría es positiva se denomina a la derecha (media a la derecha de la mediana y moda) y si es negativa se denomina a la izquierda (media a la izquierda de la mediana y moda). Se pueden considerar como asimetrías importantes y que pueden afectar los datos aquellas que anden cerca de 1 y definittivamente las superiores a 1. Se puede eliminar o reducir significativamente la asimetría usando transformaciones sobre los datos (Figura 9.14). La transformación logarítmica se puede usar cuando la distribución es asimétrica a la derecha (positiva), y la transformación exponencial se puede usar cuando la distribución es asimétrica a la izquierda (negativa). Figura 9.14: Transformaciones que se pueden aplicar sobre los datos para reducir la asimetría. Tomado de: http://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/ 9.4.4.2 Curtosis (kurtosis) Caracteriza el apuntalamiento (puntiagudez) de la distribución, si la distribución tiene un pico distintivo o es relativamente plana, y la concentración de valores alrededor de la media. Los nombres que puede tomar se presentan en la Figura 9.15 y ejemplos de datos con diferentes curtosis se presentan en la Figura 9.16. Figura 9.15: Nombres de la curtosis que se le asigna a la distribución de los datos según el valor estimado de la curtosis. Leptocúrtica cuando \\(k&gt;0\\), mesocúrtica cuando \\(k \\approx 0\\), y platicúrtica cuando \\(k&lt;0\\). Tomado de: http://www.spssfree.com/curso-de-spss/curso/5-19.gif. Figura 9.16: Ejemplo de alta curtosis (izquierda) y baja curtosis (derecha) (Trauth, 2015). En R násico no hay una función para calcular la asimetría ni la curtosis, pero el paquete DescTools tiene la función Skew para asimetría y Kurt para curtosis. Usando los datos anteriores tenemos: Skew(vec) ## [1] 1.06636 Kurt(vec) ## [1] 0.5057107 Con los resultados anteriores se tiene una asimetría positiva (hacia la derecha) bastante importante, y una curtosis positiva indicando que los datos presentan un apuntalamiento importante en vez de que sean más dispersos. 9.5 Resumen general En R hay varias funciones que presentan resúmenes generales presentando varias medidas (revisar Descripciones generales (globales)), aquí se presenta la función descr del paquete summarytools (Comtois, 2019), que se usa para datos cuantitativos, si se tuvieran datos cualitativos se puede utilizar freq o la tabla de frecuencias como se presentó anteriormente (Tablas de frecuencias). descr(vec) ## Descriptive Statistics ## vec ## N: 60 ## ## vec ## ----------------- -------- ## Mean 2.62 ## Std.Dev 1.92 ## Min 0.31 ## Q1 1.19 ## Median 2.16 ## Q3 3.75 ## Max 8.56 ## MAD 1.77 ## IQR 2.53 ## CV 0.73 ## Skewness 1.07 ## SE.Skewness 0.31 ## Kurtosis 0.51 ## N.Valid 60.00 ## Pct.Valid 100.00 Usando esta función (descr) podemos comparar las diferentes medidas entre un vector de datos sin valores extremos y otro con 1 valor extremo, para ver cómo afecta o no este dato las diferentes medidas. original = c(22,10,10,16,15,15,24,19,15,19,18,16,18,12,17,17,11,18) outlier = c(22,10,10,16,15,15,24,19,15,19,18,16,18,12,17,17,11,180) DF = tibble(original, outlier) descr(DF) ## Descriptive Statistics ## DF ## N: 18 ## ## original outlier ## ----------------- ---------- --------- ## Mean 16.22 25.22 ## Std.Dev 3.81 38.81 ## Min 10.00 10.00 ## Q1 15.00 15.00 ## Median 16.50 16.50 ## Q3 18.00 19.00 ## Max 24.00 180.00 ## MAD 2.22 2.97 ## IQR 3.00 3.75 ## CV 0.24 1.54 ## Skewness 0.02 3.50 ## SE.Skewness 0.54 0.54 ## Kurtosis -0.64 11.06 ## N.Valid 18.00 18.00 ## Pct.Valid 100.00 100.00 Viendo el resumen con las diferentes medidas y comparándolas, se puede observar como las medidas de media, desviación estándar, coeficiente de variación, asimetría y curtosis se ven muy afectadas por un único valor extremo, mientras que los cuantiles (incluyendo la mediana) y el mad no se ven afectadas del todo o muy poco. Referencias "],
["estadística-descriptiva-bivariable.html", "Capítulo 10 Estadística Descriptiva Bivariable 10.1 Introducción 10.2 Covarianza 10.3 Correlación 10.4 Regresión", " Capítulo 10 Estadística Descriptiva Bivariable 10.1 Introducción En el capítulo anterior (Estadística Descriptiva Univariable) se enfocó en analizar una variable por separado, sin considerar otra(s) variable(s) que pudiera(n) estar relacionada(s). A veces ese puede ser el enfoque de un análisis o estudio, pero en la mayoría de los casos, muy probablemente, se cuenta con más de una variable y esa(s) variable(s) puede(n) ser importante(s), ya sea para efectos de simple correlación o para efectos de predicción. Esta relación entre dos variables es el enfoque de este capítulo. De manera general se pueden mencionar tres formas de analizar una variable con respecto a otra: Covarianza (lineal), Correlación (lineal), y Regresión (lineal, no lineal). 10.2 Covarianza El objetivo de la covarianza (Ecuación (10.1)) es determinar si hay o no asociación entre 2 variables continuas, y si la hay cómo se comporta una con respecto a la otra, siempre que la relación entre las variables sea lineal. Es el homologo a la varianza pero para dos variables. Ésta puede ser positiva cuando una aumenta conforme la otra aumenta, o negativa cuando una aumenta conforme la otra disminuye. \\[\\begin{equation} s_{xy} = \\frac{\\sum_{i=1}^{N}(x_i - \\bar{x})(y_i - \\bar{y})}{N-1} \\tag{10.1} \\end{equation}\\] donde \\(\\bar{x}\\) es la media de la variable \\(x\\) y \\(\\bar{y}\\) es la media de la variable \\(y\\). Similar a la varianza de una variable, el valor de la covarianza va a depender de la escala de las variables, por lo que NO es ideal para comparar la magnitud de la relación entre las variables. Esta idea se puede demostrar en la Figura 10.1, donde se muestra que aunque los datos tengan tienen una correlación perfecta, la covarianza va a cambiar de acuerdo a la cantidad y escala de los datos. Figura 10.1: Visualización de la covarianza para diferentes datos donde se tiene una correlación perfecta pero la varianza cambia de acuerdo a la escala y cantidad de los datos. En R la función para la covarianza es cov. Si se le pasan dos vectores el resultado es un único valor, pero si se le pasa una matriz o tabla con diferentes variables numéricas el resultado es una matriz de varianza-covarianza. set.seed(101) vec1 = rnorm(n = 30, mean = 40, sd = 5) vec2 = rnorm(n = 30, mean = 20, sd = 3) cov(vec1,vec2) ## [1] 0.2220198 cov(tibble(vec1,vec2)) ## vec1 vec2 ## vec1 17.4519551 0.2220198 ## vec2 0.2220198 9.4703668 cov(rock) ## area peri shape perm ## area 7203044.71232 3160367.49330 -40.820823047 -466063.55213 ## peri 3160367.49330 2049653.68934 -51.775231267 -463032.47715 ## shape -40.82082 -51.77523 0.006971657 20.35164 ## perm -466063.55213 -463032.47715 20.351635275 191684.79915 Del resultado de la matriz de covarianza se observa cómo la escala de las variables afecta el valor de la varianza-covarianza. La diagonal de la matriz es la varianza de la variable respectiva, y las entradas fuera de la diagonal son las covarianzas entre las diferentes variables. 10.3 Correlación El objetivo de la correlación de Pearson (Ecuación (10.2)) es determinar la magnitud de la asociación entre dos variables cuantitativas continuas que tengan una relación lineal. En el caso de que la relación entre las variables no sea lineal se pueden utilizar los coeficientes de correlación de Spearman o Kendall, los cuales son homólogos no-paramétricos que se cubrirán en el capítulo de Estadística No Paramétrica. \\[\\begin{equation} r = \\frac{s_{xy}}{s_{x}s_{y}} \\tag{10.2} \\end{equation}\\] donde \\(s_{xy}\\) es la covarianza entre las variables, \\(s_{x}\\) es la desviación estándar de la variable \\(x\\), y \\(s_{y}\\) es la desviación estándar de la variable \\(y\\). El coeficiente de correlación corresponde con una medida estandarizada ya que tiene la propiedad de que va a estar entre -1 y 1, sin importar la escala y rango de las variables originales (Figura 10.2). Como todas las medidas que dependen de la media, el coeficiente de correlación de Pearson se va a ver afectado por valores extremos (atípicos) y en este caso por la linealidad o no de la relación (Figura 10.3). Figura 10.2: Visualización de coeficiente de correlación de Pearson, con los datos de la figura enterior. Figura 10.3: Ejemplos de la correlación de Pearson y cómo se ve afectado en casos de presencia de valores atípicos (d) y de no linealidad (e, f), estos últimos 3 siendo casos donde no es válido o apropiado usar este coeficiente de correlación (Trauth, 2015). Algo para notar en las Figuras 10.2 y 10.3 es que la correlación se presenta por medio de gráficos de dispersión sin línea de mejor ajuste, ya que si se agrega una línea mejor ajuste se asume regresión y dependencia de una variable con respecto a otra. En R la función para calcular el coeficiente de correlación es cor, donde por defecto estima la correlación de Pearson. de manera similar a la covarianza, si se le pasan dos vectores el resultado es un único valor, pero si se le pasa una matriz o tabla el resultado es una matriz de correlaciones. cor(vec1,vec2) ## [1] 0.01726976 cor(tibble(vec1,vec2)) ## vec1 vec2 ## vec1 1.00000000 0.01726976 ## vec2 0.01726976 1.00000000 cor(rock) ## area peri shape perm ## area 1.0000000 0.8225064 -0.1821611 -0.3966370 ## peri 0.8225064 1.0000000 -0.4331255 -0.7387158 ## shape -0.1821611 -0.4331255 1.0000000 0.5567208 ## perm -0.3966370 -0.7387158 0.5567208 1.0000000 En los resultados se puede ahora sí estimar la magnitud de las relaciones, donde valores cercanos a 1 y -1 indican una asociación mayor y valores cercanos a 0 indican poca o nula asociación. Más delante en el capítulo de Pruebas Estadísticas se podrá determinar si la correlación es nula o no. 10.4 Regresión A diferencia de la correlación donde se busca únicamente establecer la presencia y magnitud de las asociación entre variables, sin importar el orden, la regresión pretende establecer la dependencia de una variable con respecto a otra(s), con el fin de predecir dicha variable y/o entender su relación, ya sea porque es difícil de medir o porque se asume y/o conoce la dependencia de otra variables. El hecho de que las variables estén correlacionadas y se establezca un modelo de regresión, no significa que ambas variables no dependen de una tercera o más variables, por lo que hay que tener cuidado y NO interpretar los resultados como causa y efecto, si ese no fue el diseño de la investigación. De manera general la regresión es el ajuste de un modelo matemático a los datos observados, Ecuación (10.3). \\[\\begin{equation} y = f \\left( x; \\beta \\right) + \\epsilon \\tag{10.3} \\end{equation}\\] donde \\(f \\left( x; \\beta \\right)\\) es el modelo de ajuste definido por el analista, \\(\\beta\\) son los parámetros desconocidos a estimar, y \\(\\epsilon\\) es el error del ajuste. 10.4.1 Nomenclatura En el ámbito de modelos de regresión se manejan diferentes nombres para las partes del modelo. Variable dependiente o respuesta: Es la variable que se pretende predecir, lo que comúnmente se conoce como \\(y\\). Variable independiente o predictor: El o las variables que se van a utilizar para predecir la variable respuesta, lo que comúnmente se conoce como \\(x\\). Coeficiente: El valor estimado del efecto de la variable independiente sobre la dependiente, \\(\\beta\\) en la ecuación del modelo estimado. 10.4.2 Supuestos De manera general los modelos de regresión deben cumplir ciertos supuestos para poder considerarlos como válidos. Para eso se pueden usar lo que se denominan gráficos diagnóstico. Entre los supuestos están: Linealidad: No se refiere la linealidad de los datos originales sino a la relación entre los valores residuales y los valores ajustados (Figura 10.4), donde lo ideal es que no se observen fuertes desviaciones ni tendencias entre estos valores. Figura 10.4: Supuesto de linealidad para un modelo de regresión. Lo ideal es que no se observen fuertes desviaciones ni tendencias. Normalidad: No se refiere la normalidad de los datos originales sino a la normalidad de los residuales, donde lo ideal es que estos residuales sigan una distribución normal, ésto se puede representar por medio del gráfico QQ (Figura 10.5). Figura 10.5: Supuesto de normalidad para un modelo de regresión. Lo ideal es que la mayoría de los puntos caigan cerca de la linea 1:1, y no hayan fuertes desviaciones. Homosquedasticidad (varianza constante): La idea es que los residuales y por ende el ajuste mantenga una varianza o error constante para todo el rango de valores, donde lo lo ideal es que no hayan tendencias ni formas de abanico (Figura 10.6). Figura 10.6: Supuesto de varianza constante para un modelo de regresión. Lo ideal es que no hayan fuertes desviaciones ni tendencias. 10.4.3 Tipos Lineal simple: Es la más básica, cuando se trabaja únicamente con dos variables cuantitativas continuas (Figura 10.7), y el ajuste es una línea. Figura 10.7: Ejemplo de regresión lineal simple. Lineal múltiple: Es cuando se trabaja con 3 o más variables, donde pueden ser de diferentes tipos (cualitativa o cuantitativa) (Figura 10.8), y el ajuste es un plano. Figura 10.8: Ejemplo de regresión lineal multiple. Tomado de: https://dlegorreta.files.wordpress.com/2015/09/regression_lineal.png. No lineal: Es cuando la relación entre las variables sigue una forma más compleja a la lineal (Figura 10.9), la cual puede ser polinomial, exponencial, potencia, logarítmica, o cualquier otro modelo o ecuación. Figura 10.9: Ejemplo de regresión no lineal. Logística: Es cuando la variable dependiente (respuesta) es cualitativa y puede tener dos clases o niveles (regresión logística binomial), tres o más clases nominales (regresión logística multinomial), o tres o más clases ordinales (regresión logística ordinal). La regresión logística binomial es la más común (Figura 10.10). En todos estos casos el resultado se puede dar como la clase predecida, o las probabilidades de pertenencia a cada clase, donde generalmente se asigna la clase predecida a la clase con mayor probabilidad de pertenencia. Por esto el eje vertical (y) se representa como probabilidades. Figura 10.10: Ejemplo de regresión logística binomial. 10.4.3.1 Lineal Simple Como se mencionó anteriormente es el tipo de regresión más básica y sencilla ya que lidia únicamente con dos variables cuantitativas continuas. Esta regresión presenta el modelo que se muestra en la Ecuación (10.4), y la forma típica de realizar el ajuste del modelo a los datos es por medio del método de mínimos cuadrados (OLS - ordinary least squares, en inglés), donde se busca minimizar el error (los residuos) en la dirección vertical (Figura 10.11). \\[\\begin{equation} \\hat{y} = \\hat{b}_0 + \\hat{b}_1 x + \\epsilon \\tag{10.4} \\end{equation}\\] donde \\(\\hat{y}\\) son los valores predecidos, \\(\\hat{b}_0\\) es el intercepto, \\(\\hat{b}_1\\) es la pendiente, \\(x\\) es la variable predictora, y \\(\\epsilon\\) es el error de ajuste. En este tipo de regresión lo más importante es la pendiente (\\(\\hat{b}_1\\)) que representa esa relación entre las variables de cómo cambia \\(y\\) con respecto a \\(x\\), donde el valor de la pendiente indica cuanto cambia (incrementa, disminuye) \\(y\\) en promedio por cada unidad de incremento de \\(x\\). El intercepto (\\(\\hat{b}_0\\)) por lo general no es de importancia ya que es un parámetro de ajuste y en la mayoría de las ocasiones carece de sentido práctico. Figura 10.11: Ajuste de modelo lineal simple mostrando los errores como lineas verticales que unen los valores predecidos con los observados. Errores positivos aparecen en color rojo, y errores negativos aparecen en color azul. Cuando la relación entre las dos variables es lineal y se tiene un modelo lineal simple se puede estimar la correlación a partir de la pendiente y viceversa, como se muestra en la Ecuación (10.5): \\[\\begin{equation} r_{xy} = \\hat{b}_1 \\frac{s_x}{s_y}\\\\ \\hat{b}_1 = r_{xy} \\frac{s_y}{s_x} \\tag{10.5} \\end{equation}\\] Más adelante se va a introducir el concepto de estandarización, que es un tipo de transformación que se puede aplicar a una variable o a un modelo. En la regresión lineal simple el coeficiente de correlación, entre las variables \\(x\\) y \\(y\\), es igual a la pendiente (\\(\\hat{b}_1\\)) estandarizada. Si se realiza una regresión lineal simple sobre variables estandarizadas la pendiente va a ser igual al coeficiente de correlación entre la variables. En R para ajustar modelos lineales se usa a función lm, donde los argumentos son primero una formula del tipo y~x (\\(y\\) en función de \\(x\\)), y data la tabla donde se encuentran los datos. Se muestra como ejemplo al ajuste del modelo que se presenta en la Figura 10.7, que es base a un set de datos que trae R por defecto. fit_lin = lm(Temp ~ Wind, airquality) fit_lin %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 90.1 2.05 43.9 6.69e-88 ## 2 Wind -1.23 0.194 -6.33 2.64e- 9 Se muestra el ajuste de la temperatura en función de la velocidad del viento, donde el modelo resultante tiene la forma \\(\\hat{Temp} = 90.13 - 1.23 Wind\\). Este resultado se puede interpretar de la siguiente manera: por cada unidad que incrementa la velocidad del viento en millas por hora (mph), la temperatura decrece en 1.23 grados Fahrenheit. Otra interpretación podría ser que por cada 10 mph que incrementa la velocidad del viento, la temperatura decrece 12.3 grados Fahrenheit. Al multiplicar las unidades de \\(x\\) por un factor la pendiente se va a ver afectada por ese mismo factor, en el ejemplo anterior siendo el factor 10. Por el momento en la tabla de resultados que se muestra únicamente es de interés el coeficiente (term) y su valor (estimate), más adelante se explicará el resto de las columnas. La función tidy del paquete broom (Robinson &amp; Hayes, 2020) y del metapaquete tidymodels (Kuhn &amp; Wickham, 2020) es una función que ordena el resultado del ajuste en una tabla para mayor facilidad de interpretación y manipulación. 10.4.4 Medidas de ajuste y error Una vez ajustado un modelo es importante saber la calidad del ajuste, ya que se pueden generar varios modelos y se quisiera saber cuál modelo se ajusta mejor o representa mejor los datos. Para este fin se pueden usar varias métricas. 10.4.4.1 RMSE El Root Mean Square Error (RMSE en inglés) o error cuadrático medio (Ecuación (10.6)) mide qué tan diferentes (alejados) son los residuos de la línea de mejor ajuste. Tiene la propiedad de que se encuentra en la misma escala de la variable respuesta, por lo que va a depender de la misma. En general a menor RMSE mejor ajuste, pero esta métrica es más útil para comparar modelos. \\[\\begin{equation} RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i - \\hat{y}\\right)^2} \\tag{10.6} \\end{equation}\\] 10.4.4.2 Coeficiente de determinación (\\(R^2\\)) El coeficiente de determinación (\\(R^2\\)) es una métrica de ajuste estandarizada, ya que varia entre 0 y 1, donde mientras más cercano a 1 mejor el ajuste. Esta métrica se puede interpretar como el porcentaje de variación en la variable respuesta que puede ser explicado por la variable predictora. Para el caso de la regresión lineal simple (únicamente) se puede relacionar el coeficiente de determinación con el coeficiente de correlación de la siguiente manera: \\(R^2 = r^2\\), o lo que es lo mismo \\(r = \\sqrt{R^2}\\). En R hay diferentes funciones para obtener diferentes métricas de ajuste. La función glance del paquete broom extrae del modelo varias medidas de ajuste, entre ellas el \\(R^2\\). La función RMSE del paquete DescTools extrae el valor de esta métrica. fit_lin %&gt;% glance() ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.210 0.205 8.44 40.1 2.64e-9 2 -542. 1091. 1100. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; RMSE(fit_lin) ## [1] 8.386689 Con este valor de \\(R^2\\) se puede estimar y corroborar el coeficiente de correlación. Tener en cuenta el signo de la pendiente, en este caso negativo, ya que el \\(R^2\\) siempre es positivo y la raíz siempre va a ser positiva, por lo que hay que asignarle el signo de la pendiente. sqrt(fit_lin %&gt;% glance() %&gt;% pull(r.squared)) * -1 ## [1] -0.4579879 with(airquality, cor(Temp,Wind)) ## [1] -0.4579879 De manera similar, se puede estimar la correlación a partir de la Ecuación (10.5). airquality %&gt;% select_at(vars(Temp,Wind)) %&gt;% map_df(sd) %&gt;% mutate(b1 = coef(fit_lin)[2], r = b1 * Wind/Temp) ## # A tibble: 1 x 4 ## Temp Wind b1 r ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.47 3.52 -1.23 -0.458 Referencias "],
["probabilidad.html", "Capítulo 11 Probabilidad 11.1 Introducción 11.2 Axiomas y Nomenclatura 11.3 Reglas de probabilidad 11.4 Variables aleatorias", " Capítulo 11 Probabilidad 11.1 Introducción La probabilidad permite cuantificar la frecuencia de un acontecimiento determinado mediante la realización de un experimento aleatorio, del que se conocen todos los resultados posibles, bajo condiciones suficientemente estables. La probabilidad de un suceso es un numero, comprendido entre 0 y 1 (o 0 y 100), que indica las posibilidades que tiene de verificarse cuando se realiza un experimento aleatorio. Un experimento aleatorio es aquella experiencia que trae un resultado que no se sabe a ciencia cierta cuál será, es decir no hay certeza del resultado. Como ejemplos comunes se pueden mencionar tirar una moneda al aire, tirar los dados. El conjunto de todos los resultados posibles arrojados por un experimento aleatorio se denomina espacio muestral (\\(\\Omega\\)). En el caso de tirar una moneda al aire seria \\(\\Omega=\\lbrace escudo,corona \\rbrace\\), en el caso de un dado \\(\\Omega=\\lbrace 1,2,3,4,5,6 \\rbrace\\). Un evento o suceso es cualquier subconjunto del espacio muestral, cada uno de los resultados posibles de una experiencia aleatoria (Walpole et al., 2012). 11.2 Axiomas y Nomenclatura Hay ciertos conceptos que son propios del tema de probabilidad y es necesario conocer para poder comprenderlos y aplicarlos, éstos se mencionan con más detalle en Walpole et al. (2012). \\(P[A]\\) es la probabilidad de que el evento o suceso \\(A\\) ocurra, \\(P[A&#39;]=1-P[A]\\) es la probabilidad de que el evento o suceso \\(A\\) no ocurra, lo que se conoce como el complemento del evento, \\(P[A \\cap B]\\) es la probabilidad de que el evento \\(A\\) y el evento \\(B\\) ocurran (intersección), \\(P[A \\cup B]\\) es la probabilidad de que el evento \\(A\\) o el evento \\(B\\) ocurran (unión), \\(P[A | B]\\) es la probabilidad de que el evento \\(A\\) ocurra dado el evento \\(B\\) ha ocurrido (condicional), \\(P[A \\cap B] = 0\\) indica eventos mutuamente excluyentes, que no tienen elementos en común, \\(0&lt;P[A]&lt;1\\), la probabilidad siempre se encuentra en 0 y 1. 11.3 Reglas de probabilidad Lo que se expone aquí proviene en su mayoría de Walpole et al. (2012). 11.3.1 Regla de la suma De manera general esta regla aplica cuando se quiere obtener la probabilidad de que \\(A\\) o \\(B\\) ocurran, o sea que al menos uno de los eventos de interés debe ocurrir. La forma general de la regla de la suma se muestra en la Ecuación (11.1). \\[\\begin{equation} P[A \\cup B] = P[A] + P[B] - P[A \\cap B] \\tag{11.1} \\end{equation}\\] Cuando los eventos son excluyentes (no pueden ocurrir al mismo tiempo) se tiene que \\(P[A \\cap B] = 0\\) y la ecuación se reduce a \\(P[A] + P[B]\\). Si los eventos no son excluyentes (pueden ocurrir al mismo tiempo) se utiliza toda la ecuación donde \\(P[A \\cap B]\\) va a ser la intersección (cantidad de datos en común) entre los eventos y debe ser sustraída para que no se contabilice dos veces. Un ejemplo de eventos excluyentes puede ser el sacar un As (\\(A\\)) o un Rey rojo (\\(B\\)) de una baraja de 52 cartas, donde los eventos son excluyentes porque no hay intersección entre As y Rey, una representación del problema se muestra en la Figura 11.1. Se tiene \\(P[A]=\\frac{4}{52}\\), \\(P[B]=\\frac{2}{52}\\), entonces \\(P[A \\cup B]=\\frac{4}{52}+\\frac{2}{52}=\\frac{6}{52}=0.115\\) o \\(11.5\\%\\). Figura 11.1: Ejemplo de regla de la suma para eventos excluyentes. Un ejemplo de eventos no excluyentes puede ser el sacar un As (\\(A\\)) o una carta de espadas negra (\\(B\\)) de una baraja de 52 cartas, donde los eventos son no excluyentes porque hay un As de espadas negra, una representación del problema se muestra en la Figura 11.2. Se tiene \\(P[A]=\\frac{4}{52}\\), \\(P[B]=\\frac{13}{52}\\), \\(P[A \\cap B] = \\frac{1}{52}\\), entonces \\(P[A \\cup B]=\\frac{4}{52}+\\frac{13}{52}-\\frac{1}{52}=\\frac{4}{13}=0.308\\) o \\(30.8\\%\\). Figura 11.2: Ejemplo de regla de la suma para eventos no excluyentes. 11.3.2 Regla de la multiplicación De manera general esta regla aplica cuando se quiere obtener la probabilidad de que \\(A\\) y \\(B\\) ocurran, o sea que todos los eventos de interés deben ocurrir. La forma general de la regla de la suma se muestra en la Ecuación (11.2). \\[\\begin{equation} P[A \\cap B] = P[A] * P[B|A] \\tag{11.2} \\end{equation}\\] Cuando los eventos son independientes (ocurrencia de A no afecta la ocurrencia de B) se tiene que \\(P[B|A] = P[B]\\) y la ecuación se reduce a \\(P[A] * P[B]\\). Si los eventos no son independientes (ocurrencia de A afecta la ocurrencia de B) y se utiliza toda la ecuación. \\(P[B|A]\\) es la probabilidad condicional de \\(B\\) dado \\(A\\). La Ecuación (11.2) se puede arreglar para encontrar esta probabilidad condicional como se muestra en la Ecuación (11.3), donde la principal idea es que al condicionar por un evento, ahora el total (denominador) no va a ser el total de las observaciones sino el total de observaciones del evento condicionante. \\[\\begin{equation} P[B|A] = \\frac{P[A \\cap B]}{P[A]} \\tag{11.3} \\end{equation}\\] Un ejemplo de eventos independientes puede ser el siguiente. Consideremos un experimento en el que se sacan 2 cartas, una después de la otra, de una baraja ordinaria, con reemplazo. Los eventos se definen como \\(A\\): la primera carta es un As, \\(B\\): la segunda carta es una espada. Como la primera carta se reemplaza, nuestro espacio muestral para la primera y segunda cartas consta de 52 cartas, que contienen 4 ases y 13 espadas. Se tiene \\(P[A]=\\frac{4}{52}\\), \\(P[B]=\\frac{13}{52}\\), entonces \\(P[A \\cap B]=\\frac{4}{52}*\\frac{2}{52}=\\frac{52}{2704}=0.019\\) o \\(1.9\\%\\); que sería lo mismo que la intersección \\(P[A \\cap B]\\) donde solo hay una opción de que se den los dos eventos entonces \\(P[A \\cap B]=\\frac{1}{52}=0.019\\). Un ejemplo de eventos no independientes puede ser el siguiente. Suponga que se tiene una caja de fusibles con 20 unidades, de las cuales 5 están defectuosas. Si se seleccionan 2 fusibles al azar y se retiran de la caja, uno después del otro, sin reemplazar el primero, ¿cuál es la probabilidad de que ambos fusibles estén defectuosos? Se tiene \\(P[A]=\\frac{5}{20}\\), \\(P[B|A]=\\frac{4}{19}\\), entonces \\(P[A \\cap B]=\\frac{5}{20}*\\frac{4}{19}=\\frac{20}{380}=\\frac{1}{19}=0.053\\) o \\(5.3\\%\\) Un ejemplo de probabilidad condicional puede ser el siguiente. Consideremos ahora una baraja ordinaria con 52 cartas, de 4 tipos, 13 de cada tipo y 2 colores (Figura 11.3). Figura 11.3: Cartas de una baraja de naipes. ¿Cuál es la probabilidad de sacar 1 As si la carta que se saca es Roja \\(P[As|Roja]\\)? Se puede resolver de dos maneras. Primero, y la que resulta más intuitiva, se puede condicionar y el nuevo total cambia, con lo que el nuevo total es el total de cartas Rojas (26), y de ese total cuántas son As (2), entonces \\(P[As|Roja]=\\frac{2}{26}=0.077\\) o \\(7.7\\%\\). Segundo, se puede usar la ecuación de la probabilidad condicional, usando siempre el total de observaciones, \\(P[As|Roja]=\\frac{P[Roja \\ \\cap \\ As]}{P[Roja]}=\\frac{2/52}{26/52}=0.077\\) o \\(7.7\\%\\). 11.4 Variables aleatorias Una variable aleatoria es una variable X a la que le puede corresponder cada uno de los eventos del espacio muestral un número real (x). Si el número real sólo se puede expresar en enteros, a la variable aleatoria se le denomina variable aleatoria discreta, si puede tener decimales y por ende un gran número de posibilidades, es una variable aleatoria continua (Walpole et al., 2012). Ejemplos de estas variables aleatorias se presentan en la Figura 11.4 (Swan &amp; Sandilands, 1995). Figura 11.4: Ejemplos de variables aleatorias mostrando casos discretos y continuos (Swan &amp; Sandilands, 1995). Tanto para las variables aleatorias discretas como continuas existen dos funciones de densidad o probabilidad que las pueden caracterizar: función de densidad de probabilidad (ésta realmente aplica más para variables discretas) y función de densidad acumulada (Walpole et al., 2012). La función de densidad de probabilidad (Figura 11.5) brinda la probabilidad puntual de cada uno de los posibles resultados para una variable aleatoria discreta; para una variable aleatoria continua el valor de esta función de probabilidad va a ser muy pequeño dada la gran cantidad de valores que puede tomar esta variable. La función de densidad acumulada brinda la probabilidad de obtener un valor menor o igual (mayor o igual) al de interés (Figuras 11.6, 11.8), y siempre comprende de 0 a 1 (0 a 100). Para un ejemplo discreto, en el caso de lanzar una moneda al aire y de contar la corona como un éxito, se pueden tener 3 resultados 0 coronas y 2 escudos, 1 corona y 1 escudo, o 2 coronas. La probabilidad de cada uno de los posibles resultados (0, 1, 2) se muestra en la Figura 11.5, y la probabilidad acumulada se muestra en la Figura 11.6. Figura 11.5: Función de densidad de probabilidad para una variable aleatoria discreta. Figura 11.6: Función de densidad acumulada para una variable aleatoria discreta Para un ejemplo de variable continua, se puede tener una variable con media de 30 y desviación estándar de 3, y se quiere encontrar la probabilidad de que esta variable sea menor a 25 (Figura 11.7 A), de que sea mayor a 35 (Figura 11.7 B), o de que se encuentre entre 25 y 35 (Figura 11.7 C). La curva de densidad acumulada se presenta en la Figura 11.8, a partir de la cual se pueden responder las mismas preguntas anteriores, lo único que cambia es la presentación de la distribución. Figura 11.7: Ejemplos de función de densidad de probabilidad para una variable aleatoria continua. A Para X menor a un valor. B Para X mayor a un valor. C Para X entre entre dos valores. Figura 11.8: Función de densidad acumulada para una variable aleatoria continua. Referencias "],
["distribuciones-de-probabilidad.html", "Capítulo 12 Distribuciones de Probabilidad 12.1 Introducción 12.2 Distribución Binomial 12.3 Distribución de Poisson 12.4 Distribución Normal o Gaussiana 12.5 Distribuciones de probabilidad en R", " Capítulo 12 Distribuciones de Probabilidad 12.1 Introducción En el capítulo anterior se introdujeron los principios de probabilidad y las variables aleatorias (discretas y continuas), y se mencionaron las funciones (distribuciones) de probabilidad, así como unos ejemplo básicos de lo que representan. Estas distribuciones permiten aproximar de forma analítica los fenómenos que dan orígenes a las variables de interés, permitiendo obtener probabilidades para todos los posibles resultados. En este capítulo se van a introducir las distribuciones de probabilidad más utilizadas para variables discretas y continuas (haciendo referencia a una población), y que serán ampliadas en capítulos posteriores tomando en cuenta una muestra. 12.2 Distribución Binomial Esta distribución aplica para variables discretas, de ensayos repetidos cuyo resultado se puede clasificar como de éxito (existe, está presente, está por encima, etc.) o fracaso (no existe, no está presenta, está por debajo, etc.), y la probabilidad de éxito se conoce y mantiene constante para cada ensayo (repetición) (Trauth, 2015; Triola, 2004; Walpole et al., 2012). La Ecuación (12.1) muestra cómo se pueden obtener probabilidades para esta distribución, donde se tiene una variable \\(X\\) distribuida de forma binomial con \\(n\\) cantidad de intentos (ensayos), una probabilidad de éxito \\(p\\) y probabilidad de fracaso \\(1-p\\) para cada ensayo, y \\(x \\ (0,1,2,\\cdots,n)\\) representa el número de éxitos para el cual obtener la probabilidad. \\[\\begin{equation} X \\sim Bin(n,p) = \\frac{n!}{(n-x)!x!} p^x q^{n-x} \\tag{12.1} \\end{equation}\\] Para demostrar y visualizar esta distribución se presenta el siguiente ejemplo, modificado de Trauth (2015). En el caso de una perforación para petróleo, la probabilidad de éxito está dada por 0.1 (10%). ¿Cuál es la probabilidad de tener 1 pozo exitoso de un total de 6 intentos? ¿Cuál es la probabilidad de tener al menos 2 pozos exitosos? Lo que se tiene: \\(p=0.1\\), \\(n=6\\), y se pide la probabilidad de \\(x=1\\) pozo exitoso y la probabilidad de al menos 2 pozos exitosos, que es lo mismo que decir 2 o más. Una representación de la función (distribución) de densidad para este caso se muestra en la Figura 12.1. De manera manual se puede resolver de la siguiente manera. \\[\\begin{align} Bin(n,p) &amp;= \\frac{n!}{(n-x)!x!} p^x q^{n-x}\\\\ Bin(6,0.1) &amp;= \\frac{6!}{(6-1)!1!} 0.1^1 0.9^{6-1} = 0.35 = 35 \\% \\end{align}\\] En R se puede usar dbinom para obtener la probabilidad puntual y pbinom para obtener la probabilidad acumulada. p = 0.1 n = 6 x = 1 dbinom(x = x, size = n, prob = p) # probabilidad de x = 1 ## [1] 0.354294 pbinom(q = 1, size = n, prob = p, lower.tail = F) # probabilidad de x &gt;= 2 ## [1] 0.114265 Figura 12.1: Función de probabilidad binomial para los datos del ejemplo. Esta distribución se puede caracterizar por medio de los parámetros \\(\\mu=pq\\) y \\(\\sigma=\\sqrt{npq}\\), donde éstos corresponden con la media y desviación estándar de una aproximación a la distribución normal (Triola, 2004; Walpole et al., 2012), la cual se va a introducir más adelante. El efecto de estos parámetros se puede visualizar en la Figura 12.2, donde se grafica la distribución de probabilidad y se observa como se puede aproximar a la distribución normal definida por dichos parámetros. Figura 12.2: Aproximación de la distribución binomial a la normal por medio de los parámetros de la media y desviación estándar, mostrando la ubicación de la media y una desviación estándar por debajo y encima de la misma. El caso de la Figura 12.2 A corresponde con una variable distribuida binomialmente con 10 intentos y una probabilidad de éxito de 0.5 (\\(Bin(10,0.5)\\)), dando como resultado los siguientes parámetros: \\[\\begin{equation} \\mu = np = 10 \\cdot 0.5 = 5\\\\ \\sigma = \\sqrt{npq} = \\sqrt{10 \\cdot 0.5 \\cdot 0.5} = 1.58 \\approx 2 \\end{equation}\\] El caso de la Figura 12.2 B corresponde con una variable distribuida binomialmente con 100 intentos y una probabilidad de éxito de 0.2 (\\(Bin(100,0.2)\\)), dando como resultado los siguientes parámetros: \\[\\begin{equation} \\mu = np = 100 \\cdot 0.2 = 20\\\\ \\sigma = \\sqrt{npq} = \\sqrt{100 \\cdot 0.2 \\cdot 0.8} = 4 \\end{equation}\\] 12.3 Distribución de Poisson Esta distribución aplica para variables discretas, para eventos que se presentan en una región (distancia, área, volumen) o intervalo de tiempo. Estos eventos, aleatorios e independientes, se consideran poco probables en largos intervalos de ocurrencia, y la cantidad puede ser infinita. Ejemplos de eventos que se pueden caracterizar serían: terremotos, erupciones volcánicas, inundaciones, etc. (Trauth, 2015; Triola, 2004; Walpole et al., 2012). La Ecuación (12.2) muestra cómo se pueden obtener probabilidades para esta distribución, donde se tiene una variable \\(X\\) distribuida de forma Poisson con \\(\\lambda\\) como único parámetro, que corresponde con un promedio de eventos por unidad de tiempo o espacio (\\(t\\)), y \\(x \\ (0,1,2,\\cdots)\\) representa el número de eventos. \\[\\begin{equation} X \\sim Poiss(\\lambda) = \\frac{e^{-\\lambda t}(\\lambda t)^x}{x!} \\tag{12.2} \\end{equation}\\] Para demostrar y visualizar esta distribución se presenta el siguiente ejemplo, modificado de Triola (2004). Al analizar los impactos de las bombas V1 en la Segunda Guerra Mundial, el sur de Londres se subdividió en 576 regiones, cada una con un área de 0.25 \\(km^2\\). En total, 535 bombas estallaron en el área combinada de 576 regiones. Si se selecciona una región aleatoriamente, calcule la probabilidad de que fuese blanco de impactos exactamente en dos (\\(x=2\\)) ocasiones. Con base en la probabilidad que se calculó en el inciso a, ¿cuántas de las 576 regiones se espera que reciban impactos exactamente dos veces? Se tiene que calcular \\(\\lambda\\) el promedio de impactos por región (\\(t=1\\)), y se pide la probabilidad de \\(x=2\\) impactos y la probabilidad de 3 o más impactos. Una representación de la función (distribución) de densidad para este caso se muestra en la Figura 12.3. De manera manual se puede resolver de la siguiente manera. \\[\\begin{equation} \\lambda = \\frac{535}{576} = 0.929\\\\ Poiss(0.292) = \\frac{e^{-0.929 * 1}(0.929 * 1)^2}{2!} = 0.17 = 17 \\%\\\\ \\text{Número de regiones que pudieran recibir 2 impactos} = 576 * 0.17 = 97.9 \\end{equation}\\] En R se puede usar dpois para obtener la probabilidad puntual y ppois para obtener la probabilidad acumulada. lambda = 535/576 x = 2 dpois(x = x, lambda = lambda) # probabilidad de x = 2 ## [1] 0.1703929 ppois(q = 2, lambda = lambda, lower.tail = F) # probabilidad de x &gt;= 3 ## [1] 0.06768529 Figura 12.3: Función de probabilidad de Poisson para los datos del ejemplo. Esta distribución se puede caracterizar por medio de los parámetros \\(\\mu=\\lambda\\) y \\(\\sigma=\\sqrt{\\lambda}\\), donde éstos corresponden con la media y desviación estándar de una aproximación a la distribución normal (Triola, 2004; Walpole et al., 2012). El efecto de estos parámetros se puede visualizar en la Figura 12.4, donde se grafica la distribución de probabilidad y se observa como se puede aproximar a la distribución normal definida por dichos parámetros. Figura 12.4: Aproximación de la distribución de Poisson a la normal por medio de los parámetros de la media y desviación estándar, mostrando la ubicación de la media y una desviación estándar por debajo y encima de la misma. El caso de la Figura 12.4 A corresponde con una variable distribuida como Poisson con \\(\\lambda=3\\) (\\(Poiss(3)\\)), dando como resultado los siguientes parámetros: \\[\\begin{equation} \\mu = \\lambda = 3\\\\ \\sigma = \\sqrt{\\lambda} = \\sqrt{3} = 1.73 \\approx 2 \\end{equation}\\] El caso de la Figura 12.4 B corresponde con una variable distribuida como Poisson con \\(\\lambda=10\\) (\\(Poiss(10)\\)), dando como resultado los siguientes parámetros: \\[\\begin{equation} \\mu = \\lambda = 10\\\\ \\sigma = \\sqrt{\\lambda} = \\sqrt{10} = 3.16 \\approx 3 \\end{equation}\\] Para las distribución binomial conforme incrementa la probabilidad de éxito (\\(p\\)) más se asemeja a una distribución normal, y para la distribución de Poisson conforme incrementa el promedio de eventos (\\(\\lambda\\)) más se asemeja a una distribución normal. 12.4 Distribución Normal o Gaussiana Esta distribución es utilizada para variables continuas, y es la más utilizada en estadística porque “describe de manera aproximada muchos fenómenos que ocurren en la naturaleza, la industria y la investigación” (Walpole et al., 2012). Tiene una forma de campana donde los valores más frecuentes o probables se encuentran en la parte central y los valores menos frecuentes o probables se encuentran en las colas (Figura 12.5) Figura 12.5: Forma de la distribución normal mostrando donde se presentan los valores más y menos frecuentes. Esta distribución se puede describir por medio de los parámetros de la media (\\(\\mu\\)) y desviación estándar (\\(\\sigma\\)), donde la media representa el valor más probable (Trauth, 2015). Para indicar que una variable se distribuye normalmente se usa la siguiente sintaxis: \\[\\begin{equation} X \\sim N(\\mu,\\sigma) \\tag{12.3} \\end{equation}\\] Las características principales de esta distribución son (Walpole et al., 2012): Simétrica alrededor de \\(\\mu\\) Centrada y con valor máximo en \\(\\mu\\) Puntos de inflexión en \\(x = \\mu \\pm \\sigma\\) El área bajo la curva es 1 La Figura 12.6 demuestra cómo afectan la media y desviación estándar la forma de la curva comparando dos curvas. Figura 12.6: Comparación entre curvas normales con diferentes parámetros. A con diferente media (\\(\\mu_1 &lt; \\mu_2\\)) pero misma desviación estándar (\\(\\sigma_1 = \\sigma_2\\)). B con misma media (\\(\\mu_1 = \\mu_2\\)) pero diferente desviación estándar (\\(\\sigma_1 &lt; \\sigma_2\\)). C con diferente media (\\(\\mu_1 &lt; \\mu_2\\)) y diferente desviación estándar (\\(\\sigma_1 &lt; \\sigma_2\\)). Tomado de (Walpole et al., 2012). 12.4.1 Distribución Normal Estándar (Z) Un caso especial de la distribución normal es la distribución normal estándar. Ésta se caracteriza por tener una media de o y desviación estándar de 1 (Triola, 2004; Walpole et al., 2012). Para transformar (estandarizar) una variable se usa la Ecuación (12.4) y se describe que esta nueva variable estandarizada está distribuida normalmente con media 0 y desviación estándar 1. \\[\\begin{equation} Z = \\frac{x - \\mu}{\\sigma}\\\\ Z \\sim N(0,1) \\tag{12.4} \\end{equation}\\] \\(Z\\) es una medida estandarizada y lo que indica es cuántas desviaciones estándar por encima o por debajo de la media se encuentra el valor \\(x\\). Si \\(Z &gt; 0\\) entonces \\(x\\) es mayor que la media y si \\(Z &lt; 0\\) entonces \\(x\\) es menor que la media. Imaginemos tenemos una población con media 45 y desviación estándar 5. Para un \\(x=50\\) tenemos \\(Z = \\frac{50 - 45}{5} = 1\\), entonces \\(x\\) está 1 \\(\\sigma\\) por encima de \\(\\mu\\). Para un \\(x=35\\) tenemos \\(Z = \\frac{35 - 45}{5} = -2\\), entonces \\(x\\) está 2 \\(\\sigma\\) por debajo de \\(\\mu\\). El área bajo la curva original y la curva estandarizada es la misma (Figura 12.7), entonces las probabilidades son las mismas, además es útil para comparar distribuciones aún cuando tienen diferentes escalas y/o unidades (Walpole et al., 2012). Figura 12.7: El área bajo la curva original es la misma que bajo la curva estandarizada, por ende las probabilidades son las mismas (Walpole et al., 2012). Se va a demostrar cómo utilizar la curva normal estándar (\\(Z\\)) para obtener probabilidades por medio de ejemplos tomados de Walpole et al. (2012). El procedimiento manual general es calcular \\(Z\\) y buscar el área a la izquierda para este valor (tablas de distribuciones en Walpole et al. (2012)), y dependiendo de lo que se pida se realiza algún cálculo adicional. Dada una variable aleatoria X que tiene una distribución normal con \\(\\mu=50\\) y \\(\\sigma=10\\), calcule la probabilidad de que X tome un valor entre 45 y 62. La idea es obtener al área a la izquierda del valor mayor y restarle el área a la izquierda del valor menor, obteniendo así la intersección (Figura 12.8). \\[\\begin{equation} X \\sim N(50,10)\\\\ z_1 = \\frac{45 - 50}{10} = -0.5\\\\ z_2 = \\frac{62 - 50}{10} = 1.2\\\\ P(45 &lt; X &lt; 62) = P(-0.5 &lt; Z &lt; 1.2) = P(Z &lt; 1.2) - P(Z &lt; -0.5)\\\\ P(Z &lt; 1.2) - P(Z &lt; -0.5) = 0.8849 - 0.3085 = 0.5764 = 57.6 \\% \\end{equation}\\] Figura 12.8: Curva normal para el ejemplo mostrando el área entre los valores de 45 y 62. En R se realiza como se muestra, usando pnorm, donde esta función va a calcular, por defecto, el área a la izquierda para \\(x=q\\) con una media \\(\\mu\\) y desviación estándar \\(\\sigma\\). La función pnorm y el resto de funciones relacionadas con la distribución normal tienen por defecto una media de 0 y desviación estándar de 1, correspondiendo con la distribución normal estándar, por lo que se pueden usar valores de \\(z\\) directamente y obtener los mismos resultados. mu = 50 sigma = 10 x1 = 45 x2 = 62 z1 = (x1 - mu) / sigma z2 = (x2 - mu) / sigma pnorm(q = x2,mean = mu,sd = sigma) - pnorm(q = x1,mean = mu,sd = sigma) ## [1] 0.5763928 pnorm(q = z2) - pnorm(q = z1) ## [1] 0.5763928 Dado que X tiene una distribución normal con \\(\\mu=300\\) y \\(\\sigma=50\\), calcule la probabilidad de que X tome un valor mayor que 362. Aquí la idea es obtener el área a la derecha de 362 (Figura 12.9), por lo que hay que encontrar el área la izquierda y restárcela a 1. \\[\\begin{equation} X \\sim N(300,50)\\\\ z = \\frac{362 - 300}{50} = 1.24\\\\ P(X &gt; 362) = P(Z &gt; 1.24) = 1 - P(Z &gt; 1.24)\\\\ 1 - P(Z &gt; 1.24) = 1 - 0.8925 = 0.1075 = 10.7 \\% \\end{equation}\\] Figura 12.9: Curva normal para el ejemplo mostrando el área a la derecha de 362. En R se realiza como se muestra, usando pnorm, donde esta función va a calcular, por defecto, el área a la izquierda para \\(x=q\\) con una media \\(\\mu\\) y desviación estándar \\(\\sigma\\). Para calcular el área a la derecha se puede restar el resultado a 1 o se puede usar el argumento lower.tail = F para obtener directamente el área a la derecha. mu = 300 sigma = 50 x = 362 z = (x - mu) / sigma pnorm(q = x,mean = mu,sd = sigma,lower.tail = F) ## [1] 0.1074877 1 - pnorm(q = z) ## [1] 0.1074877 Así como se puede calcular la probabilidad para un \\(x\\) se puede a partir de una probabilidad dada encontrar el \\(x\\) que corresponde con dicha probabilidad. Se muestra un ejemplo a continuación. Dada una distribución normal con \\(\\mu = 40\\) y \\(\\sigma = 6\\), calcule el valor de \\(x\\) que tiene: 45% del área a la izquierda, y 14% del área a la derecha (o sea 86% área a la izquierda), ver Figura 12.10. Para ésto se despeja \\(x\\) de la Ecuación (12.4) y se tiene que encontrar el valor de \\(z\\) que corresponde con la probabilidad dada. \\[\\begin{equation} p = 0.45, \\text{resultando en un } z = -0.13\\\\ x = \\sigma * z + \\mu = 6 * -0.13 + 40 = 39.24\\\\ p = 0.86, \\text{resultando en un } z = 1.08\\\\ x = \\sigma * z + \\mu = 6 * 1.08 + 40 = 46.48 \\end{equation}\\] En R se usa la función qnorm para encontrar el valor de la variable para una probabilidad dada. qnorm(.45,40,6) ## [1] 39.24603 qnorm(.14,40,6,lower.tail = F) ## [1] 46.48192 qnorm(.86,40,6) ## [1] 46.48192 Figura 12.10: Curva normal para el ejemplo mostrando el valor para un área a la izquierda de 45% (coral) y el valor para un área a la derecha de 14% (celeste). 12.5 Distribuciones de probabilidad en R R tiene funciones para las distribuciones más usadas (Tabla 12.1). Existen 4 funciones generales para todas las distribuciones, donde empiezan con la misma letra y después le sigue el nombre de la distribución: d*: Las funciones que empiezan con d calculan la probabilidad puntual. Éstas aplican más para distribuciones discretas (binomial, poisson). p*: Las funciones que empiezan con p calculan la probabilidad acumulada inferior (por defecto) o superior. Éstas aplican para todas pero especialmente para las distribuciones continuas (normal, \\(Z\\), \\(t\\), \\(\\chi^2\\), \\(F\\)). q*: Las funciones que empiezan con q calculan el valor de la variable o estadístico (cuantil) que corresponde con cierta probabilidad. r*: Las funciones que empiezan con r generan datos aleatorio (random) para la distribución deseada y con los parámetros deseados. Tabla 12.1: Funciones de R para las distribuciones más usadas Probabilidad Valores Distribucion Puntual Acumulada Cuantil Aleatorios Binomial dbinom pbinom qbinom rbinom Poisson dpois ppois qpois rpois Normal dnorm pnorm qnorm rnorm t dt pt qt rt chi2 dchisq pchisq qchisq rchisq F df pf qf rf Referencias "],
["introducción-a-estadística-inferencial.html", "Capítulo 13 Introducción a Estadística Inferencial 13.1 Introducción 13.2 Distribuciones muestrales", " Capítulo 13 Introducción a Estadística Inferencial 13.1 Introducción En los capítulos anteriores se han venido construyendo las bases para poder, ahora sí, responder a preguntas acerca de poblaciones a partir de datos muestrales, esta es la esencia de la estadística inferencial, ir más allá de describir los datos recolectados para poder hacer generalizaciones con fundamento teórico y cuantitativo. Este flujo de análisis se resume en la Figura 13.1. En este capítulo se introducen conceptos fundamentales de la estadística inferencial, para posteriormente expandir sobre éstos utilizando estimación y pruebas de hipótesis para responder formalmente a una pregunta de investigación. Figura 13.1: Diagrama del análisis en estadística inferencial. Es necesario introducir algunos conceptos nuevos que son muy usados y conocidos en estadística inferencial (Walpole et al., 2012): Parámetro: Es una medida estadística poblacional, cuyo valor es de interés conocer. Por ejemplo, la media poblacional \\(\\mu\\) y la varianza poblacional \\(\\sigma^2\\) son parámetros. Todos los parámetros se representan por medio de letras griegas. Estadístico o Estimador: Es una variable aleatoria definida con las variables de la muestra aleatoria. Por ejemplo, la media muestral \\(\\bar{x}\\) y la varianza muestral \\(s^2\\) son estadísticos. Es lo que se mide a partir de la muestra recolectada. Distribución muestral de un estadístico: Es la distribución de probabilidad de un estadístico. Va a ser diferente a la distribución poblacional ya que toma en cuenta el tamaño de la muestra. Los diferentes estadísticos van a aproximarse a diferentes curvas de distribución. Estadística paramétrica: Es la estadística convencional, en la que se basan la mayoría de técnicas de análisis. Hace suposiciones, siendo la más importante que los datos siguen una distribución conocida, por lo general siendo esta la distribución normal. Estadística no-paramétrica: A diferencia de la paramétrica, ésta no hace suposiciones respecto a distribuciones y se emplean otros métodos de análisis, que en algunos casos se pueden realizar únicamente por medios computacionales. 13.2 Distribuciones muestrales 13.2.1 Media \\((\\bar{x})\\) Es la distribución de probabilidad de medias muestrales, donde las diferentes muestras presentan el mismo tamaño o número de observaciones (Triola, 2004). La distribución de medias muestrales va a tener la misma media de la población pero la desviación estándar va a ser el error estándar de la media. A continuación se exponen dos conceptos muy importantes en estadística que permiten aproximar una gran variedad de problemas a la distribución normal (McKillup &amp; Darby Dyar, 2010; Walpole et al., 2012). Teorema del Límite Central: La distribución de las medias de las muestras tenderá a ser normalmente distribuida aún cuando la población de la cual se muestreó no es normal. La media de la distribución de medias es la media de la población y la distribución de medias tiene menos dispersión que la población. Ley de los números grandes: Si tomamos muestras de una población con distribución desconocida, ya sea finita o infinita, la distribución muestral de \\(\\bar{x}\\) aún será aproximadamente normal con media \\(\\mu\\) y varianza o , siempre que el tamaño de la muestra sea grande (\\(n ≥ 30\\)) y la distribución no sea muy asimétrica. En función de si se conoce o no la varianza poblacional (\\(\\sigma^2\\)), el error estándar va a ser \\(\\sigma_\\bar{x} = \\frac{\\sigma}{\\sqrt{n}}\\) o \\(s_\\bar{x} = \\frac{s}{\\sqrt{n}}\\), respectivamente, donde cuando no se conoce \\(\\sigma^2\\) se usa la varianza muestral (\\(s^2\\)) como estimador de la varianza poblacional (McKillup &amp; Darby Dyar, 2010; Triola, 2004; Walpole et al., 2012). Lo anterior también afecta la distribución a usar para la inferencia. Si se conoce \\(\\sigma^2\\) se usa la distribución normal estándar (\\(Z\\)), pero si no se conoce \\(\\sigma^2\\) se usa la distribución \\(t\\) de Student. Esta distribución es similar a al normal en términos de que tiene forma de campana y es simétrica alrededor de la media, pero tiene la característica de que va a estar en función del tamaño de la muestra (\\(n\\), y por ende de los grados de libertad \\(v\\)), donde a menor tamaño de la muestra va a permitir más probabilidad en las colas y a mayor tamaño de la muestra se va a parecer más a la curva normal (McKillup &amp; Darby Dyar, 2010; Triola, 2004; Walpole et al., 2012). Una comparación entre la distribución normal y la distribución \\(t\\) se muestra en la Figura 13.2. Figura 13.2: Comparación entre la distribución normal Z y la distribución t, esta última con diferentes grados de libertad. Dado lo anterior, la distribución toma la forma de la Ecuación (13.1) para la distribución normal y la Ecuación (13.2) para la distribución \\(t\\): \\[\\begin{equation} Z = \\frac{\\bar{x} - \\mu}{\\sigma_\\bar{x}}\\\\ \\sigma_\\bar{x} = \\frac{\\sigma}{\\sqrt{n}} \\tag{13.1} \\end{equation}\\] \\[\\begin{equation} t = \\frac{\\bar{x} - \\mu}{s_\\bar{x}}\\\\ s_\\bar{x} = \\frac{s}{\\sqrt{n}} \\tag{13.2} \\end{equation}\\] Conforme el tamaño de la muestra incrementa el error estándar decrece y las inferencias/conclusiones se vuelven más precisas. Este concepto se puede observar en la Figura 13.3 (McKillup &amp; Darby Dyar, 2010). Figura 13.3: Efecto del tamaño de la muestra en el error estándar. Al aumentar el tamaño de la muestra, manteniendo el resto de parámetros poblacionales constantes, el error estándar decrece, incrementando la precisión de las conclusiones sobre la media poblacional. Tomado de McKillup &amp; Darby Dyar (2010). El siguiente ejemplo tomado de Swan &amp; Sandilands (1995) muestra cómo se puede tomar en cuenta una muestra para calcular probabilidades y empezar a hacer inferencias. Se realiza primero con la distribución \\(Z\\) (asumiendo que se conoce \\(\\sigma\\)) y luego con la distribución \\(t\\). La longitud de especímenes de belemnite tiene una distribución normal con \\(\\mu = 3.5 \\ cm\\) y \\(\\sigma = 0.2 \\ cm\\). Se obtiene una muestra al azar de 8 belemnites. Cuál es la probabilidad de que cuando se selecciona una muestra al azar, le media de la muestra sea de 3.7 cm? Si se asume \\(\\sigma = 0.2\\) se puede usar \\(Z\\) \\[\\begin{equation} \\bar{x} = 3.7, \\mu = 3.5, \\sigma = 0.2, n = 8\\\\ \\sigma_\\bar{x} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{0.2}{\\sqrt{8}} = 0.0707\\\\ Z = \\frac{\\bar{x} - \\mu}{\\sigma_\\bar{x}} = \\frac{3.7 - 3.5}{0.0707} = 2.82\\\\ P(X &gt; 3.7) = P(Z &gt; 2.82) = 1 - P(Z &lt; 2.82)\\\\ 1 - P(Z &lt; 2.82) = 1 - 0.9976 = 0.0024 \\approx 0.2\\% \\end{equation}\\] Si no se asume \\(\\sigma = 0.2\\) se usa \\(t\\) \\[\\begin{equation} \\bar{x} = 3.7, \\mu = 3.5, s = 0.2, n = 8, v = 8 - 1 = 7\\\\ s_\\bar{x} = \\frac{s}{\\sqrt{n}} = \\frac{0.2}{\\sqrt{8}} = 0.0707\\\\ t = \\frac{\\bar{x} - \\mu}{s_\\bar{x}} = \\frac{3.7 - 3.5}{0.0707} = 2.82\\\\ P(X &gt; 3.7) = P(t_7 &gt; 2.82) = 1 - P(t_7 &lt; 2.82)\\\\ 1 - P(t_7 &lt; 2.82) = 1 - 0.9873 = 0.0127 \\approx 1.27\\% \\end{equation}\\] En R se usan las funciones de las distribuciones. En el caso de que conozca \\(\\sigma\\) se usa la distribución la normal, si no se conoce \\(\\sigma\\) se usa la distribución \\(t\\). En general se ajusta la desviación estándar y se usa el error estándar respectivo. x = 3.7 mu = 3.5 sigma = 0.2 n = 8 pnorm(q = x, mean = mu, sd = sigma/sqrt(n), lower.tail = F) ## [1] 0.002338867 s = 0.2 v = n - 1 pt(q = (x-mu)/(s/sqrt(n)), df = v, lower.tail = F) ## [1] 0.01273178 Con los ejemplos anteriores se observa como al usar la distribución \\(t\\) la probabilidad en los extremos es mayor, especialmente cuando el tamaño de la muestra es pequeño. 13.2.2 Diferencia de medias \\((\\bar{x}_1-\\bar{x}_2)\\) Es la distribución de probabilidad de la diferencia entre medias muestrales, donde las muestras pueden o no presentar el mismo tamaño o número de observaciones. Ésta se utiliza cuando se desean comparar dos posibles poblaciones, dos métodos de análisis, etc. Asumiendo que se cumple el Teorema del Límite Central, se asume que las muestras van a estar normalmente distribuidas y por ende la diferencia de medias tendrá esta forma. La distribución toma la forma presentada en la Ecuación (13.3) si se conoce \\(\\sigma\\), sino se puede ajustar y usar la distribución \\(t\\) usando la Ecuación (13.4) (Walpole et al., 2012): \\[\\begin{equation} Z = \\frac{(\\bar{x}_1-\\bar{x}_2) - (\\mu_1-\\mu_2)}{\\sigma_{\\bar{x}_1-\\bar{x}_2}}\\\\ \\sigma_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}} \\tag{13.3} \\end{equation}\\] \\[\\begin{equation} t = \\frac{(\\bar{x}_1-\\bar{x}_2) - (\\mu_1-\\mu_2)}{s_{\\bar{x}_1-\\bar{x}_2}}\\\\ s_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}} \\tag{13.4} \\end{equation}\\] El contenido de humedad del sitio A tiene una media 35% y una desviación estándar de 5%, el contenido de humedad del sitio B tiene una media 30% y una desviación estándar de 6%. Cuál es la probabilidad de que una muestra de tamaño 25 del sitio A tenga un 7% más de contenido de humedad que una muestra de tamaño 30 del sitio B? \\[\\begin{equation} \\mu_1-\\mu_2 = 35 - 30 = 5\\\\ \\sigma_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}} = \\sqrt{\\frac{5^2}{25}+\\frac{6^2}{30}} = 1.48\\\\ Z = \\frac{(\\bar{x}_1-\\bar{x}_2) - (\\mu_1-\\mu_2)}{\\sigma_{\\bar{x}_1-\\bar{x}_2}}\\\\ Z = \\frac{7 - 5}{1.48} = 1.35\\\\ P(\\bar{X}_1-\\bar{X}_2 &gt; 7) = P(Z &gt; 1.35) = 1 - P(Z &lt; 1.35)\\\\ 1 - P(Z &lt; 1.35) = 1 - 0.911 = 0.088 \\approx 8.85 \\% \\end{equation}\\] En R se hace de manera similar al caso de una media. Se puede realizar de dos maneras, calculando el \\(z\\) (o \\(t\\)) y usándolo directamente, o en el caso de usar \\(z\\), incluyendo en la función la diferencia de medias muestrales como el valor de interés, la diferencia de medias poblaciones como la media, y el error estándar como la desviación estándar. x = 7 mu1 = 35 mu2 = 30 sigma1 = 5^2 sigma2 = 6^2 n1 = 25 n2 = 30 z = (x - (mu1-mu2)) / sqrt(sigma1/n1 + sigma2/n2) z ## [1] 1.3484 pnorm(q = z, lower.tail = F) ## [1] 0.08876493 pnorm(q = x, mean = mu1-mu2, sd = sqrt(sigma1/n1 + sigma2/n2), lower.tail = F) ## [1] 0.08876493 s1 = 5^2 s2 = 6^2 v = n1 + n2 -2 t = (x - (mu1-mu2)) / sqrt(s1/n1 + s2/n2) t ## [1] 1.3484 pt(q = t, df = v, lower.tail = F) ## [1] 0.09163208 En el ejemplo anterior se cumple todavía que la probabilidad usando \\(t\\) es mayor que usando \\(z\\), pero la diferencia no es tan grande ya que el tamaño de muestra combinado (por ende los grados de libertad) es grande y se empieza a aproximar a la distribución normal. Triola (2004) muestra una figura (13.4) que puede servir para decidir qué distribución/método utilizar cuando el parámetro de interés es la media. Figura 13.4: Diagrama de flujo para ayudar a decidir qué distribución usar cuando se quiere hacer inferencia sobre la media poblacional. Tomado de Triola (2004). 13.2.3 Varianza \\((s^2)\\) Las distribuciones de estadísticos anteriores se enfocaron en la media, cuando es de interés sacar conclusiones para el comportamiento típico de un fenómeno o proceso. Cuando el interés es la variabilidad de un fenómeno o proceso el estadístico de interés es la varianza muestral, como proxy de la varianza poblacional. Dado que esta medida es siempre positiva se ocupa una distribución que imponga esta condición, por lo que se usa la distribución \\(\\chi^2\\), con \\(v = n - 1\\) grados de libertad, Ecuación (13.5) (Triola, 2004; Walpole et al., 2012). \\[\\begin{equation} \\chi^2 = \\frac{(n-1)s^2}{\\sigma^2} \\tag{13.5} \\end{equation}\\] La Figura 13.5 (Walpole et al., 2012) muestra la forma de la distribución \\(\\chi^2\\), donde la probabilidad de que el estadístico tome un valor mayor o igual al calculado corresponde con el área a la derecha de dicho estadístico. Esta distribución no es simétrica, pero conforme incrementan los grados de libertad se vuelve más simétrica (Triola, 2004), Figura 13.6. Figura 13.5: Distribución \\(\\chi^2\\) mostrando el área a la derecha (probabilidad de un valor mayor) para un valor específico de \\(\\chi^2\\). Tomada de Walpole et al. (2012). Figura 13.6: Distribución \\(\\chi^2\\) con diferentes grados de libertad. Ejemplo: Una población con distribución aproximadamente normal tiene varianza especificada de 0.8. Calcule la probabilidad que una muestra aleatoria de tamaño 6 tenga una varianza mayor o igual a 1.2. \\[\\begin{equation} \\chi^2 = \\frac{(n-1)s^2}{\\sigma^2} = \\frac{(6-1)1.2}{0.8} = 7.5\\\\ P(s^2 &gt; 1.2) = P(\\chi^2 &gt; 7.5) = 0.186 \\approx 18.6 \\% \\end{equation}\\] En R se puede usar la función pchisq con el estadístico y grados de libertad específicos. sigma2 = 0.8 s2 = 1.2 n = 6 v = n - 1 chi = (v * s2) / sigma2 chi ## [1] 7.5 pchisq(q = chi, df = v, lower.tail = F) ## [1] 0.1860298 13.2.4 Dos varianzas \\(\\left( \\frac{s_1^2}{s_2^2} \\right)\\) Así como se pueden hacer inferencias sobre dos medias, se pueden hacer inferencias sobre la relación entre dos varianzas (desviaciones estándar). De igual manera a la distribución \\(\\chi^2\\) para una varianza, se ocupa una distribución que sea positiva, pero al tratarse de dos muestras (poblaciones) esta distribución ocupa dos grados de libertad, uno para cada muestra, este es el caso de la distribución \\(F\\). Esta distribución tiene una amplia aplicación en la comparación de varianzas muestrales, así como en problemas que implican dos o más muestras (Walpole et al., 2012), lo que se cubrirá en los próximos capítulos. Si \\(s_1^2\\) y \\(s_2^2\\) son las varianzas de muestras aleatorias independientes de tamaño \\(n_1\\) y \\(n_2\\) tomadas de poblaciones normales con varianzas \\(\\sigma_1^2\\) y \\(\\sigma_1^2\\), respectivamente, entonces el estadístico \\(F\\) se define como: \\[\\begin{equation} F = \\frac{s_1^2}{s_2^2} \\tag{13.6} \\end{equation}\\] La distribución \\(F\\) depende de \\(v_1 = n_1 - 1\\) y \\(v_2 = n_2 - 1\\), como se muestra en la Figura 13.7. Figura 13.7: Distribución \\(F\\) con diferentes grados de libertad. Ejemplo: Se tienen dos muestras de tamaño \\(n_1 = 6\\) y \\(n_2 = 9\\). Cuál es la probabilidad de que la razón de varianzas sea mayor a 2? \\[\\begin{equation} F = \\frac{s_1^2}{s_2^2} = 2\\\\ P(F_{5,8} &gt; 2) = 1 - P(F_{5,8} &lt; 2) = 1 - 0.817 = 0.183 \\approx 18.3 \\% \\end{equation}\\] En R se usa pf donde se requiere el valor de \\(F\\) y los dos grados de libertad (\\(df1, df2\\)). f = 2 n1 = 6 n2 = 9 v1 = n1 - 1 v2 = n2 - 1 pf(q = f, df1 = v1, df2 = v2, lower.tail = F) ## [1] 0.1829977 Lo presentado en este capítulo es la base de lo que se va a cubrir en los próximos capítulos, donde se introducirán temas y técnicas de inferencia muy utilizadas que permiten responder a preguntas de investigación, como lo son Estimación e Hipótesis, Pruebas Estadísticas y Estadística No Paramétrica. Estas a su vez van a sentar la base para posteriormente aplicar conceptos y técnicas similares a datos típicos en geociencias. Referencias "],
["estimación-e-hipótesis.html", "Capítulo 14 Estimación e Hipótesis 14.1 Introducción 14.2 Estimación 14.3 Hipótesis", " Capítulo 14 Estimación e Hipótesis 14.1 Introducción En los capítulos anteriores (Distribuciones de Probabilidad e Introducción a Estadística Inferencial) se introdujeron principios básicos de la estadística inferencial: el uso de distribuciones y distribuciones muestrales. Estos conceptos serán ampliados en este capítulo donde se introducirán los dos métodos de estadística inferencial más usados: estimación e hipótesis. En ambos casos se trabaja a partir de una muestra o muestras para indicar la incertidumbre que se tiene sobre un parámetro poblacional (estimación), o para responder a una pregunta de investigación (hipótesis); ambos métodos se pueden complementar, aunque la tendencia va más hacia la estimación, junto con tamaños de efecto. 14.2 Estimación Existen dos tipos de estimación principales: puntual y por intervalo. Un estimador puntual es un valor único que se considera representa o aproxima de mejor manera a un parámetro poblacional (Triola, 2004; Walpole et al., 2012). Una estimación por intervalo representa un rango de posibles valores para el parámetro poblacional (Cumming, 2014, 2012; Cumming &amp; Calin-Jageman, 2017; Cumming &amp; Finch, 2005; Triola, 2004). La ventaja de un intervalo de confianza, sobre una prueba de hipótesis (como se va a ver más adelante y en el próximo capítulo), es que brinda una medida de la incertidumbre que se tiene sobre un parámetro específico, brindando más información que responder sí o no a una pregunta de hipótesis. Lo que se obtiene a partir de la estimación por intervalo es lo que se conoce como intervalo de confianza (IC). Éste se construye a partir de un nivel de confianza (\\(1 - \\alpha\\), \\(\\alpha\\) = nivel de significancia), el cual “es la proporción de veces que el intervalo de confianza realmente contiene el parámetro de población, suponiendo que el proceso de estimación se repite un gran número de veces” (Triola, 2004). Los valores más comunes para el nivel de confianza son 90% (\\(\\alpha = 0.1\\)), 95% (\\(\\alpha = 0.05\\)), y 99% (\\(\\alpha = 0.01\\)) (Triola, 2004; Walpole et al., 2012). Interpretación correcta del intervalo de confianza (frecuentista) Si se tiene una cantidad infinita de muestras con sus respectivos intervalos de confianza, el \\(100(1 - \\alpha)\\%\\) de esos intervalos va a contener el valor poblacional (Cumming, 2014, 2012; Cumming &amp; Finch, 2005; Nolan &amp; Heinzen, 2014; Triola, 2004). El intervalo que se obtiene puede ser uno del \\(100(1 - \\alpha)\\%\\) o del \\(100(\\alpha)\\%\\), esto no se sabe. El intervalo de confianza es el rango de valores más probables para el parámetro poblacional (Cumming, 2014, 2012; Cumming &amp; Calin-Jageman, 2017; Cumming &amp; Finch, 2005). La nota anterior se puede visualizar o imaginar a como lo definen Cumming &amp; Finch (2005) y se muestra en la Figura 14.1, donde el intervalo que se calcula es uno de muchos posibles intervalos que se pudo obtener y contiene o no el parámetro poblacional, pero ésto no se sabe. Figura 14.1: Intervalos de confianza al 95% para una media poblacional conocida, donde los puntos representan la media de la muestra. Los puntos sólidos contienen \\(\\mu\\) y los puntos abiertos no contienen \\(\\mu\\). Aquí 18 de los 20 (90%) IC contienen a \\(\\mu\\). A la larga, el 95% de los intervalos va a contener \\(\\mu\\). Tomado de Cumming &amp; Finch (2005). Para distribuciones simétricas (\\(Z, t\\)) los intervalos de confianza siguen la forma que se muestra en la Ecuación (14.1), mientras que para distribuciones asimétricas (\\(\\chi^2, F\\)) los intervalos de confianza siguen la forma que se muestra en la Ecuación (14.2). \\[\\begin{equation} \\hat{\\theta} \\pm MoE\\\\ \\hat{\\theta} = \\text{estadístico o estimador}\\\\ MoE = \\text{margen de error} = SE \\cdot stat_{crit,\\alpha/2,v}\\\\ \\alpha/2 \\text{ va a estar en función del nivel de confianza elegido}\\\\ SE = \\text{error estándar}\\\\ v = \\text{grados de libertad} \\tag{14.1} \\end{equation}\\] \\[\\begin{equation} \\hat{\\theta}_i &lt; \\hat{\\theta} &lt; \\hat{\\theta}_s\\\\ \\hat{\\theta}_i = \\text{límite inferior}\\\\ \\hat{\\theta}_s = \\text{límite superior} \\tag{14.2} \\end{equation}\\] El estadístico crítico (\\(stat_{crit,\\alpha/2,v}\\)) se puede encontrar en tablas de valores críticos (Davis, 2002; Nolan &amp; Heinzen, 2014; Swan &amp; Sandilands, 1995; Triola, 2004; Walpole et al., 2012), o a partir de la función q respectiva en R. A excepción de \\(Z\\) todos van a estar en función de los grados de libertad, que es lo mismo que decir el tamaño de la(s) muestra(s). 14.2.1 Media Cuando se quiere estimar una media poblacional (\\(\\mu\\)) a partir de intervalos de confianza se puede usar la distribución normal estándar (\\(Z\\)) si se conoce la desviación estándar poblacional (\\(\\sigma\\)), que es la menor cantidad del tiempo, o se puede usar la distribución \\(t\\) si no se conoce \\(\\sigma\\). Esto se introdujo en la sección Distribuciones muestrales del capítulo anterior. 14.2.1.1 Se conoce \\(\\sigma\\) Si se conoce \\(\\sigma\\) o se tiene una muestra grande (por lo general \\(n &gt; 30\\)) se puede usar la distribución \\(Z\\) para construir el intervalo de confianza. Modificando la Ecuación (14.1) y ajustándola a \\(Z\\) se tiene la Ecuación (14.3), donde \\(z_{\\alpha/2}\\) corresponde con el valor crítico de \\(Z\\) para el nivel de confianza escogido, que se puede encontrar usando qnorm. Para este caso se puede visualizar conforme la Figura 14.2. \\[\\begin{equation} \\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\\\ MoE = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\tag{14.3} \\end{equation}\\] Figura 14.2: Representación de los valores críticos en la distribución normal estándar para un dado nivel de confianza. Como la distribución \\(Z\\) no depende del tamaño de la muestra los valores críticos van a ser constantes para un dado nivel de confianza. Un resumen de los valores críticos de \\(Z\\) se muestra en la Tabla 14.1. Tabla 14.1: Valores críticos para \\(Z\\) Nivel de confianza \\(\\alpha\\) Valor crítico, \\(z_{\\alpha/2}\\) 90% 0.10 1.645 95% 0.05 1.960 99% 0.01 2.575 El siguiente ejemplo de Walpole et al. (2012) demuestra lo expuesto anteriormente. Se encuentra que la concentración promedio de zinc que se obtiene en una muestra de mediciones en 36 sitios diferentes de un río es de 2.6 gramos por mililitro. Calcule los intervalos de confianza del 95% y 99% para la concentración media de zinc en el río. Suponga que la desviación estándar de la población es de 0.3 gramos por mililitro. \\[\\begin{equation} \\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\\\ Para \\ (1-\\alpha) = 95\\% \\ se \\ tiene \\ z_{\\alpha/2} = z_{0.05/2} = 1.96\\\\ 2.6 \\pm 1.96 \\frac{0.3}{\\sqrt{36}} = 2.6 \\pm 0.1 \\to 95\\% \\ IC [2.50,2.70]\\\\ Para \\ (1-\\alpha) = 99\\% \\ se \\ tiene \\ z_{\\alpha/2} = z_{0.01/2} = 2.575\\\\ 2.6 \\pm 2.575 \\frac{0.3}{\\sqrt{36}} = 2.6 \\pm 0.13 \\to 99\\% \\ IC [2.47,2.73] \\end{equation}\\] Dado que este tipo de inferencia es poco común, ya que se requiere conocer \\(\\sigma\\), no hay funciones nativas en R, pero los paquetes DescTools (Signorell, 2020), OneTwoSamples (Zhang, 2013), y GMisc (Garnier-Villarreal, 2020), ofrecen diferentes funciones que pueden ser implementadas. A diferencia de la mayoría de funciones que requieren vectores de datos para realizar los cálculos, el paquete GMisc ofrece funciones para estimar intervalos de confianza partir de datos puntuales. x = 2.6 sig = 0.3 n = 36 ci_z(x = x, sig = sig, n = n, conf.level = 0.95) # Para 95% ## # A tibble: 1 x 3 ## mean lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.6 2.5 2.7 ci_z(x = x, sig = sig, n = n, conf.level = 0.99) # Para 99% ## # A tibble: 1 x 3 ## mean lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.6 2.47 2.73 Simulando un vector de datos con estos valores se pueden demostrar funciones de DescTools y OneTwoSamples (Los resultados no van a ser exactos, dado que es una simulación, pero la idea es demostrar funciones a base de vectores). En la función MeanCI si se pasa el argumento sd y en la función interval_estimate1 si se pasa el argumento sigma se usa \\(Z\\) para el intervalo de confianza. set.seed(123) vec = rnorm(n = n, mean = x, sd = sig) MeanCI(x = vec, sd = sig, conf.level = 0.95) # DescTools ## mean lwr.ci upr.ci ## 2.616681 2.518683 2.714680 interval_estimate1(x = vec, sigma = sig, alpha = 0.05) # OneTwoSamples ## # A tibble: 1 x 4 ## mean df a b ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.62 36 2.52 2.71 14.2.1.2 No se conoce \\(\\sigma\\) Si no se conoce \\(\\sigma\\) o se tiene una muestra pequeña se puede usar la distribución \\(t\\) para construir el intervalo de confianza. Modificando la Ecuación (14.1) y ajustándola a \\(t\\) se tiene la Ecuación (14.4), donde \\(t_{\\alpha/2,v}\\) corresponde con el valor crítico de \\(t\\) para el nivel de confianza escogido y los grados de libertad respectivos, que se puede encontrar usando qt. Para este caso se puede visualizar conforme la Figura 14.3. En general los intervalos de confianza usando \\(t\\) son más conservadores (amplios) que usando \\(Z\\). \\[\\begin{equation} \\bar{x} \\pm t_{\\alpha/2,v} \\frac{s}{\\sqrt{n}}\\\\ MoE = t_{\\alpha/2,v} \\frac{s}{\\sqrt{n}} \\tag{14.4} \\end{equation}\\] Figura 14.3: Representación de los valores críticos en la distribución t para un dado nivel de confianza. El siguiente ejemplo de Swan &amp; Sandilands (1995) demuestra lo expuesto anteriormente. Se tiene el porcentaje de cuarzo estimado en secciones delgadas de una roca ígnea: 23.5, 16.6, 25.4, 19.1, 19.3, 22.4, 20.9, 24.9. Estime el intervalo de confianza al 95% para el contenido de cuarzo. \\[\\begin{equation} \\bar{x} \\pm t_{\\alpha/2,v} \\frac{s}{\\sqrt{n}}\\\\ \\bar{x} = 21.512, s = 3.083, n = 8, v = 7\\\\ 21.512 \\pm 2.365 \\frac{3.083}{\\sqrt{8}} = 21.512 \\pm 2.578 \\to 95\\% \\ IC [18.93,24.09] \\end{equation}\\] En R se pueden de nuevo usar funciones de los paquetes DescTools, OneTwoSamples, y GMisc, donde para los primeros dos ahora no se brinda el argumento de la desviación estándar poblacional, y para el último se pueden pasar las estadísticas a partir del vector. cuarzo = c(23.5, 16.6, 25.4, 19.1, 19.3, 22.4, 20.9, 24.9) a = 0.05 MeanCI(x = cuarzo, conf.level = 1-a) # DescTools ## mean lwr.ci upr.ci ## 21.51250 18.93477 24.09023 interval_estimate1(x = cuarzo, alpha = a) # OneTwoSamples ## # A tibble: 1 x 4 ## mean df a b ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.5 7 18.9 24.1 ci_t(x = mean(cuarzo), s = sd(cuarzo), n = length(cuarzo), conf.level = 1-a) # GMisc ## # A tibble: 1 x 4 ## mean df lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.5 7 18.9 24.1 Triola (2004) presenta un diagrama de flujo (Figura 14.4) que puede servir como guía de cuando usar \\(Z\\), cuando usar \\(t\\), o cuando usar métodos no-paramétricos (bootstrap), los cuales serán cubiertos en el capítulo Estadística No Paramétrica. Figura 14.4: Diagrama de flujo para la elección de la distribución (o método) a usar en el cálculo de intervalos de confianza. Tomado de Triola (2004). 14.2.2 Diferencia de medias En el caso de que se quieren comparar dos muestras independientes para determinar su similitud o no, o dos muestras dependientes (apareadas) para determinar el efecto de un fenómeno o evento, se pueden calcular intervalos de confianza para la diferencia de medias. Muestras independientes son aquellas que no tienen relación entre ellas, por lo general indicando que son muestras tomadas a partir de la misma o diferente población pero en diferentes momentos o sitios. Muestras dependientes son aquellas donde se muestrean y miden las mismas observaciones (puntos) antes y después de una intervención (fenómeno, evento, proceso, intervención, etc.) (Nolan &amp; Heinzen, 2014; Triola, 2004). En este caso el estadístico o estimador va a ser la diferencia de medias muestrales (\\(\\bar{x}_1-\\bar{x}_2\\)) y el parámetro al diferencia de medias poblacionales (\\(\\mu_1-\\mu_2\\)). 14.2.2.1 Se conoce \\(\\sigma\\) Si se conoce \\(\\sigma\\) (de nuevo, es lo menos común) se puede usar la distribución \\(Z\\). Modificando la Ecuación (14.1) y ajustándola a \\(Z\\), para dos muestras con \\(n_1\\) y \\(n_2\\) como los tamaño de muestra y \\(\\sigma_1^2\\) y \\(\\sigma_1^2\\) como las varianzas poblacionales, se tiene la Ecuación (14.5), donde \\(z_{\\alpha/2}\\) corresponde con el valor crítico de \\(Z\\) para el nivel de confianza escogido (Walpole et al., 2012), que se puede encontrar usando qnorm. \\[\\begin{equation} (\\bar{x}_1-\\bar{x}_2) \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}\\\\ MoE = z_{\\alpha/2} \\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}} \\tag{14.5} \\end{equation}\\] Lo anterior se demuestra con el siguiente ejemplo. Se tienen dos rocas ígneas, A y B, a las cuales se les realizan diferentes mediciones de la densidad, con tamaños 20 y 25 respectivamente. La densidad promedio de A se estima en 2.75 \\(g/cm^3\\), mientras que la densidad promedio de B se estima en 2.85 \\(g/cm^3\\). Construya un intervalo de confianza, al 95%, para la diferencia entre B y A, asumiendo varianzas poblacionales de 0.01 para A y 0.05 para B. \\[\\begin{equation} (\\bar{x}_B-\\bar{x}_A) \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_B^2}{n_B}+\\frac{\\sigma_A^2}{n_A}}\\\\ (2.85-2.75) \\pm 1.96 \\sqrt{\\frac{0.05}{25}+\\frac{0.1}{20}}\\\\ 0.1 \\pm 0.164 \\to 95\\% \\ IC [-0.06,0.26] \\end{equation}\\] En R se pueden usar funciones de OneTwoSamples y GMisc. En este caso como se dan valores puntuales es más fácil usar ci_z2 de GMisc. Para mostrar como usar interval_estimate2 de OneTwoSamples, se simularon datos de acuerdo a la información (lo más cercano posibles), por lo que hay una diferencia ya que no son exactos. xB = 2.85 xA = 2.75 varB = 0.05 varA = 0.1 nB = 25 nA = 20 a = 0.05 set.seed(125) A = rnorm(n = nB, mean = xA, sd = sqrt(varA)) B = rnorm(n = nA, mean = xB, sd = sqrt(varB)) interval_estimate2(x = B, y = A, sigma = c(sqrt(varB), sqrt(varA)), alpha = a) ## # A tibble: 1 x 4 ## mean df a b ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0802 45 -0.0779 0.238 ci_z2(x1 = xB, sig1 = sqrt(varB), n1 = nB, x2 = xA, sig2 = sqrt(varA), n2 = nA, conf.level = 1-a) ## # A tibble: 1 x 3 ## mean_diff lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.1 -0.06 0.26 14.2.2.2 No se conoce \\(\\sigma\\) Como se ha mencionado anteriormente, y corresponde con la mayoría de las situaciones, cuando no se conoce \\(\\sigma\\) (o \\(n &lt; 30\\)) se usa la distribución \\(t\\). Aquí se pueden encontrar diferentes escenarios: Muestras independientes con varianzas iguales, Muestras independientes con varianzas diferentes, Muestras dependientes. 14.2.2.2.1 Muestras independientes Como se definió en la sección 14.2.2, las muestras independientes no tienen relación entre ellas. En esta situación se pueden encontrar dos escenarios dependiendo de si las varianzas son iguales o no. Lo que va a cambiar para los dos casos es el error estándar. Varianzas iguales En el caso de que se pueda determinar que las varianzas (desviaciones estándar) de las dos muestras son iguales o podrían ser iguales (sección 14.2.4), se usa lo que se conoce como varianza agrupada (pooled variance en inglés) que se muestra en la Ecuación (14.6), con lo que el error estándar se convierte en lo que se muestra en la Ecuación (14.7), y el intervalo de confianza para la diferencia de medias cuando las varianzas se pueden considerar iguales queda como se muestra en la Ecuación (14.8), con \\(t_{\\alpha/2,v}\\) como el estadístico crítico a los \\(v = n_1 + n_2 - 2\\) grados de libertad (Davis, 2002; Nolan &amp; Heinzen, 2014; Swan &amp; Sandilands, 1995; Triola, 2004; Walpole et al., 2012). \\[\\begin{equation} s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2} \\tag{14.6} \\end{equation}\\] \\[\\begin{equation} s_e = s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} \\tag{14.7} \\end{equation}\\] \\[\\begin{equation} (\\bar{x}_1-\\bar{x}_2) \\pm MoE\\\\ MoE = t_{\\alpha/2,v} \\cdot s_e\\\\ v = n_1 + n_2 - 2 \\tag{14.8} \\end{equation}\\] El siguiente ejemplo de Swan &amp; Sandilands (1995) demuestra lo expuesto anteriormente. Se recolectaron braquiópodos en dos capas (A, B) y se les midió la longitud (cm), con los siguientes resultados: A \\(\\to\\) 3.2, 3.1, 3.1, 3.3, 2.9, 2.9, 3.5, 3.0; B \\(\\to\\) 3.1, 3.1, 2.8, 3.1, 3.0, 2.6, 3.0, 3.0, 3.1, 2.8. Calcule el intervalo de confianza, al 95%, para la diferencia de medias. Un resumen de los datos se muestra en la Tabla 14.2. Tabla 14.2: Datos ejemplo Capa \\(\\bar{x}\\) \\(s^2\\) n A 3.125 0.042 8 B 2.960 0.029 10 \\[\\begin{equation} s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}\\\\ s_p^2 = \\frac{(8-1)0.042 + (10-1)0.029}{8 + 10 - 2} = 0.0348\\\\ s_e = s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\\\ s_e = 0.1865 \\sqrt{\\frac{1}{8} + \\frac{1}{10}} = 0.0885\\\\ MoE = t_{\\alpha/2,v} \\cdot s_e = t_{0.05/2,16} \\cdot s_e\\\\ MoE = 2.12 \\cdot 0.0885 = 0.1876\\\\ (\\bar{x}_1-\\bar{x}_2) \\pm MoE\\\\ (3.125-2.96) \\pm 0.1876\\\\ 0.165 \\pm 0.1876 \\to 95\\% \\ IC [-0.023,0.353] \\end{equation}\\] En R de nuevo se utilizan funciones de los paquetes OneTwoSamples y GMisc. En interval_estimate1 y ci_t hay que especificar que las varianzas son iguales (var.equal = T) ya que no es el comportamiento por defecto. A = c(3.2, 3.1, 3.1, 3.3, 2.9, 2.9, 3.5, 3.0) B = c(3.1, 3.1, 2.8, 3.1, 3.0, 2.6, 3.0, 3.0, 3.1, 2.8) a = 0.05 interval_estimate2(x = A, y = B, var.equal = T, alpha = a) ## # A tibble: 1 x 4 ## mean df a b ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.165 16 -0.0230 0.353 ci_t2(x1 = mean(A), s1 = sd(A), n1 = length(A), x2 = mean(B), s2 = sd(B), n2 = length(B), var.equal = T, conf.level = 1-a, digits = 3) ## # A tibble: 1 x 4 ## mean_diff df lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.165 16 -0.023 0.353 Varianzas diferentes En el caso de que las varianzas no se puedan considerar iguales el error estándar se convierte en lo que se muestra en la Ecuación (14.9) (el error estándar del estadístico Welch), y el intervalo de confianza para la diferencia de medias cuando las varianzas no se pueden considerar iguales queda como se muestra en la Ecuación (14.10), con \\(t_{\\alpha/2,v}\\) como el estadístico crítico a los grados de libertad mostrados en la Ecuación (14.11), donde se redondean al entero más cercano (Cumming &amp; Calin-Jageman, 2017; McKillup &amp; Darby Dyar, 2010; Swan &amp; Sandilands, 1995; Triola, 2004; Walpole et al., 2012). \\[\\begin{equation} s_e = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}} \\tag{14.9} \\end{equation}\\] \\[\\begin{equation} (\\bar{x}_1-\\bar{x}_2) \\pm MoE\\\\ MoE = t_{\\alpha/2,v} \\cdot s_e\\\\ \\tag{14.10} \\end{equation}\\] \\[\\begin{equation} v = \\frac{(s_1^2/n_1 + s_2^2/n_2)^2}{[(s_1^2/n_1)^2/(n_1-1)] + [(s_2^2/n_2)^2/(n_2-1)]} \\tag{14.11} \\end{equation}\\] El siguiente ejemplo de McKillup &amp; Darby Dyar (2010) demuestra lo expuesto anteriormente. Se recolectaron 15 braquiópodos en cada uno de dos afloramientos (A, B) y se les midió la longitud de la concha (mm), con los siguientes resultados: A \\(\\to\\) 25, 40, 34, 37, 38, 35, 29, 32, 35, 44, 27, 33, 37, 38, 36; B \\(\\to\\) 45, 37, 36, 38, 49, 47, 32, 41, 38, 45, 33, 39, 46, 47, 40. Calcule el intervalo de confianza, al 95%, para la diferencia de medias. Un resumen de los datos se muestra en la Tabla 14.3. Tabla 14.3: Datos ejemplo Afloramiento \\(\\bar{x}\\) \\(s^2\\) n A 34.67 24.67 15 B 40.87 28.69 15 \\[\\begin{equation} s_e = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\\\ s_e = \\sqrt{\\frac{24.67}{15} + \\frac{28.69}{15}} = 1.886\\\\ MoE = t_{\\alpha/2,v} s_e = t_{0.05/2,28} \\cdot s_e\\\\ MoE = 2.05 \\cdot 1.886 = 3.863\\\\ (\\bar{x}_1-\\bar{x}_2) \\pm MoE\\\\ (34.67-40.87) \\pm 3.863\\\\ -6.2 \\pm 3.863 \\to 95\\% \\ IC [-10.063,-2.337] \\end{equation}\\] En R de nuevo se utilizan funciones de los paquetes DescTools, OneTwoSamples, y GMisc. MeanDiffCI asume varianzas diferentes, para interval_estimate2 y ci_t2 se especifica que las varianzas son diferentes (el comportamiento por defecto). A = c(25, 40, 34, 37, 38, 35, 29, 32, 35, 44, 27, 33, 37, 38, 36) B = c(45, 37, 36, 38, 49, 47, 32, 41, 38, 45, 33, 39, 46, 47, 40) a = 0.05 MeanDiffCI(x = A, y = B, conf.level = 1-a) ## meandiff lwr.ci upr.ci ## -6.20000 -10.06454 -2.33546 interval_estimate2(x = A, y = B, var.equal = F, alpha = a) ## # A tibble: 1 x 4 ## mean df a b ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -6.2 27.8 -10.1 -2.34 ci_t2(x1 = mean(A), s1 = sd(A), n1 = length(A), x2 = mean(B), s2 = sd(B), n2 = length(B), var.equal = F, conf.level = 1-a, digits = 3) ## # A tibble: 1 x 4 ## mean_diff df lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -6.2 27.8 -10.1 -2.34 Cumming &amp; Finch (2005) dan algunas pautas de cómo se pueden interpretar los intervalos de confianza, al 95%, para el caso de dos muestras independientes (Figura 14.5), con el fin de determinar si las muestras se pueden consideras iguales o no (Esto es similar a realizar pruebas de hipótesis que serán cubiertas en el próximo capítulo). El punto principal es qué tanto se traslapan los intervalos, conforme más se traslapen más probable que las muestras sean similares y viceversa. Esto es más fácil y directo de interpretar con la diferencia de medias y su respectivo intervalo de confianza. Figura 14.5: Regla, según Cumming &amp; Finch (2005), de cómo interpretar intervalos de confianza al 95%, con la finalidad de determinar si las muestras se pueden considerar iguales o no. Si el traslape es igual o superior al 50% entre los brazos (márgenes de error) es probable que sean similares a un nivel de significancia \\(\\alpha = 0.05\\), pero si el traslape es menor o nulo es poco probable que sean similares. Esto se puede representar para los ejemplo anteriores. La Figura 14.6 muestra los intervalos de confianza para las muestras y para la diferencia de medias del ejemplo de muestras independientes con varianzas iguales, donde 0 cae dentro del intervalo de confianza para la diferencia, por lo que se pueden considerar iguales. La Figura 14.7 muestra los intervalos de confianza para las muestras y para la diferencia de medias del ejemplo de muestras independientes con varianzas diferentes, donde 0 no cae dentro del intervalo de confianza para la diferencia, por lo que no se pueden considerar iguales. Estas figuras se generaron con la función NewStats_2samples del paquete GMisc. Figura 14.6: Comparación de los intervalos de confianza para el ejemplo de muestras independientes con varianza igual. El valor de 0 cae dentro del intervalo de confianza para la diferencia, por lo que se pueden considerar iguales. Figura 14.7: Comparación de los intervalos de confianza para el ejemplo de muestras independientes con varianza diferente. El valor de 0 no cae dentro del intervalo de confianza para la diferencia, por lo que no se pueden considerar iguales. 14.2.2.2.2 Muestras dependientes (apareadas) Aunque se tratan de dos muestras, por estar relacionadas el análisis es muy similar al de una muestra, donde la diferencia es que no se trabaja con los datos de las muestras sino con la diferencia entre las muestras. Por lo anterior es que en vez de trabajar con la media y la desviación estándar de una muestra, ahora se trabaja con la media de las diferencias (\\(\\bar{d}\\)) y la desviación estándar de las diferencias (\\(s_d\\)). El intervalo de confianza entonces queda como se presenta en la Ecuación (14.12), donde el parámetro de interés es la media de las diferencias (\\(\\mu_d\\)), no la diferencia de medias, y los grados de libertad son \\(v=n-1\\), ya que se analizan las mismas observaciones dos veces (McKillup &amp; Darby Dyar, 2010; Nolan &amp; Heinzen, 2014; Triola, 2004; Walpole et al., 2012). \\[\\begin{equation} \\bar{d} \\pm MoE\\\\ MoE = t_{\\alpha/2,v} \\cdot s_e = t_{\\alpha/2,v} \\frac{s_d}{\\sqrt{n}}\\\\ v = n-1 \\tag{14.12} \\end{equation}\\] El siguiente ejemplo de McKillup &amp; Darby Dyar (2010) se usa para demostrar lo expuesto arriba. Se tiene el contenido de \\(FeO\\) en porcentaje de peso para 10 granitos que fueron preparados a diferentes tamaños de grano: \\(&lt; 25 \\ \\mu m\\) y \\(&lt; 125 \\ \\mu m\\). Los datos se muestran en la Tabla 14.4, donde \\(\\bar{d} = 0.1\\), \\(s_d = 0.1247\\), \\(n = 10\\), \\(v = 9\\). Tabla 14.4: Datos ejemplo muestras dependientes Granito &lt; 25 \\(\\mu m\\) &lt; 125 \\(\\mu m\\) Diferencia 1 13.5 13.6 0.1 2 14.6 14.6 0.0 3 12.7 12.6 -0.1 4 15.5 15.7 0.2 5 11.1 11.1 0.0 6 16.4 16.6 0.2 7 13.2 13.2 0.0 8 19.3 19.5 0.2 9 16.7 16.8 0.1 10 18.4 18.7 0.3 \\[\\begin{equation} \\bar{d} \\pm MoE\\\\ MoE = t_{\\alpha/2,v} \\cdot s_e = t_{\\alpha/2,v} \\frac{s_d}{\\sqrt{n}}\\\\ MoE = t_{0.05/2,9} \\frac{0.1247}{\\sqrt{10}} = 2.262 \\cdot 0.0394 = 0.0891\\\\ 0.1 \\pm 0.0891 \\to 95\\% \\ IC [0.0108,0.189] \\tag{14.12} \\end{equation}\\] En R de nuevo se utilizan funciones de los paquetes DescTools, OneTwoSamples, y GMisc. En el caso de MeanDiffCI, esta función tiene el argumento paired que es para indicar que las muestras son dependientes (apareadas). Para interval_estimate1 y ci_t, se trabaja con el vector de diferencias como el caso de una muestra. m1 = c(13.5,14.6,12.7,15.5,11.1,16.4,13.2,19.3,16.7,18.4) m2 = c(13.6,14.6,12.6,15.7,11.1,16.6,13.2,19.5,16.8,18.7) diferencia = m2 - m1 a = 0.05 MeanDiffCI(x = m2, y = m1, paired = T, conf.level = 1-a) ## meandiff lwr.ci upr.ci ## 0.10000000 0.01077932 0.18922068 interval_estimate1(x = diferencia, alpha = a) ## # A tibble: 1 x 4 ## mean df a b ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.1 9 0.0108 0.189 ci_t(x = mean(diferencia), s = sd(diferencia), n = length(diferencia), conf.level = 1-a, digits = 4) ## # A tibble: 1 x 4 ## mean df lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.1 9 0.0108 0.189 En este caso el MoE es muy pequeño dada la alta correlación entre las muestras \\(r = 0.9996\\). A diferencia de las muestras independientes, donde se pueden analizar los intervalos de confianza para las dos muestras y sacar conclusiones, ésto no es posible para las muestras dependientes (apareadas), donde la única forma de analizar la relación es por medio del intervalo de confianza para la media de las diferencias. Lo anterior se debe a que usualmente las muestras están correlacionadas, y conforme la correlación incrementa el margen de error del intervalo de confianza se reduce, a como se muestra en la Ecuación (14.13). Gráficamente las muestras dependientes se representan por medio de una línea que une las medias de las muestras (Cumming &amp; Finch, 2005). \\[\\begin{equation} MoE_d^2 = MoE_1^2 + MoE_2^2 - 2rMoE_1MoE_2\\\\ MoE_1 = \\text{margen de error de la muestra 1}\\\\ MoE_2 = \\text{margen de error de la muestra 2}\\\\ r = \\text{coeficiente de correlación de Pearson} \\tag{14.13} \\end{equation}\\] La Figura 14.8 muestra los intervalos de confianza para las muestras y para la media de las diferencias del ejemplo de muestras dependientes, donde 0 no cae dentro del intervalo de confianza para la diferencia, por lo que no se pueden considerar iguales. Esta figura se generó con la función NewStats_2samples del paquete GMisc. Figura 14.8: Comparación de los intervalos de confianza para el ejemplo de muestras dependientes. El análisis se debe enfocar en el intervalo de confianza para media de las diferencias, donde aquí el valor de 0 no cae dentro del intervalo de confianza para la diferencia, por lo que no se pueden considerar iguales. 14.2.3 Varianza A veces es el caso que se quieren hacer inferencias, sacar conclusiones para la dispersión (variabilidad) de un proceso o fenómeno (normalmente distribuido), por lo que el parámetro de interés es la varianza poblacional \\(\\sigma^2\\) y el estadístico o estimador es \\(s^2\\). En estos casos, y como se introdujo y mencionó en la sección 13.2.3, se usa la distribución \\(\\chi^2\\), ya que el parámetro de interés es siempre positivo. Para este caso la Ecuación (14.2) se puede ajustar a \\(\\chi^2\\), con lo que se tiene la Ecuación (14.14), donde \\(\\chi^2_{\\alpha/2}\\) y \\(\\chi^2_{1-\\alpha/2}\\) corresponden con el valores críticos de \\(\\chi^2\\) para el nivel de confianza escogido y los grados de libertad respectivos, que se puede encontrar usando qchi. Para este caso se puede visualizar conforme la Figura 14.9 (Triola, 2004; Walpole et al., 2012). \\[\\begin{equation} \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2,v}} &lt; \\sigma^2 &lt; \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2,v}} \\tag{14.14} \\end{equation}\\] Figura 14.9: Representación de los valores críticos en la distribución \\(\\chi^2\\) para un dado nivel de confianza y grados de libertad. Utilizando de nuevo el ejemplo de Swan &amp; Sandilands (1995) donde se tiene el porcentaje de cuarzo estimado en secciones delgadas de una roca ígnea: 23.5, 16.6, 25.4, 19.1, 19.3, 22.4, 20.9, 24.9, se puede utilizar para estimar el intervalo de confianza al 95% para la variabilidad del contenido de cuarzo. En R las funciones VarCI de DescTools, interval_var1 de OneTwoSamples, y ci_chisq de GMisc, proporcionan opciones para calcular el intervalo de confianza para \\(\\sigma^2\\). De éstas ci_chisq es la única que también muestra el intervalo de confianza para \\(\\sigma\\), que es simplemente sacar la raíz cuadrada del intervalo de confianza para \\(\\sigma^2\\). \\[\\begin{equation} \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2,v}} &lt; \\sigma^2 &lt; \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2,v}}\\\\ \\frac{(8-1)9.507}{\\chi^2_{0.05/2,7}} &lt; \\sigma^2 &lt; \\frac{(8-1)9.507}{\\chi^2_{1-0.05/2,7}}\\\\ \\frac{(8-1)9.507}{16.012} &lt; \\sigma^2 &lt; \\frac{(8-1)9.507}{1.689}\\\\ 4.16 &lt; \\sigma^2 &lt; 39.38 \\end{equation}\\] cuarzo = c(23.5, 16.6, 25.4, 19.1, 19.3, 22.4, 20.9, 24.9) a = 0.05 VarCI(x = cuarzo, conf.level = 1-a) ## var lwr.ci upr.ci ## 9.506964 4.155981 39.381007 interval_var1(x = cuarzo, alpha = a) ## # A tibble: 1 x 4 ## var df a b ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.51 7 4.16 39.4 ci_chisq(s = sd(cuarzo), n = length(cuarzo), conf.level = 1-a) ## # A tibble: 2 x 5 ## stat value df lower upper ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sd 3.08 7 2.04 6.28 ## 2 var 9.51 7 4.16 39.4 14.2.4 Dos varianzas Cuando se desea estimar la similitud entre dos varianzas se usa la distribución \\(F\\), que se usa para la razón de varianzas \\(\\frac{\\sigma^2_1}{\\sigma^2_2}\\) poblaciones. Para este escenario la Ecuación (14.2) se puede ajustar a \\(F\\), con lo que se tiene la Ecuación (14.15), donde \\(F_{\\alpha/2}\\) y \\(F_{1-\\alpha/2}\\) corresponden con el valores críticos de \\(F\\) para el nivel de confianza escogido y los grados de libertad respectivos, que se puede encontrar usando qf. Para este caso se puede visualizar conforme la Figura 14.10 (Davis, 2002; Swan &amp; Sandilands, 1995; Triola, 2004; Walpole et al., 2012). Aquí la idea es que si el valor de 1 cae dentro del intervalo, entonces las varianzas se pueden considerar iguales, ya que es un valor posible para la razón de varianzas. Lo típico es usar la varianza mayor como la primera para que la razón esté por encima de 1, pero no es necesario. \\[\\begin{equation} \\frac{s^2_1}{s^2_2} \\frac{1}{F_{\\alpha/2(v_1,v_2)}} &lt; \\frac{\\sigma^2_1}{\\sigma^2_2} &lt; \\frac{s^2_1}{s^2_2} F_{\\alpha/2(v_2,v_1)} \\tag{14.15} \\end{equation}\\] Figura 14.10: Representación de los valores críticos en la distribución \\(F\\) para un dado nivel de confianza y grados de libertad. Para ilustrar este concepto se utilizan de nuevo los datos del ejemplo de Swan &amp; Sandilands (1995), que se usó en la sección de muestras independientes con varianzas iguales, donde se recolectaron braquiópodos en dos capas (A, B), por lo que aquí se usa para comprobar si la suposición de varianzas iguales estaría correcta. \\[\\begin{equation} \\frac{s^2_1}{s^2_2} \\frac{1}{F_{\\alpha/2(v_1,v_2)}} &lt; \\frac{\\sigma^2_1}{\\sigma^2_2} &lt; \\frac{s^2_1}{s^2_2} F_{\\alpha/2(v_2,v_1)}\\\\ \\frac{0.042}{0.029} \\frac{1}{F_{0.05/2(7,9)}} &lt; \\frac{\\sigma^2_1}{\\sigma^2_2} &lt; \\frac{0.042}{0.029} F_{0.05/2(9,7)}\\\\ \\frac{0.042}{0.029} \\frac{1}{4.197} &lt; \\frac{\\sigma^2_1}{\\sigma^2_2} &lt; \\frac{0.042}{0.029} 4.823\\\\ 0.345 &lt; \\frac{\\sigma^2_1}{\\sigma^2_2} &lt; 6.985 \\end{equation}\\] A = c(3.2, 3.1, 3.1, 3.3, 2.9, 2.9, 3.5, 3.0) B = c(3.1, 3.1, 2.8, 3.1, 3.0, 2.6, 3.0, 3.0, 3.1, 2.8) a = 0.05 interval_var2(x = A, y = B, alpha = a) ## # A tibble: 1 x 5 ## rate df1 df2 a b ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.44 7 9 0.342 6.93 ci_F(s1 = sd(A), n1 = length(A), s2 = sd(B), n2 = length(B), conf.level = 1-a, digits = 3) ## # A tibble: 2 x 6 ## stat ratio df1 df2 lower upper ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sd 1.20 7 9 0.585 2.63 ## 2 var 1.44 7 9 0.342 6.93 Como se observa en este caso 1 cae dentro del intervalo, por lo que las varianzas se pueden considerar iguales, y se consideraría correcto suponer ésto para la construcción del intervalo de confianza para la diferencia de medias. 14.3 Hipótesis Como se mencionó al principio del capítulo, en estadística inferencial se usan los métodos de estimación e hipótesis. En la sección anterior se introdujeron los conceptos de estimación y varios de los casos donde se suelen utilizar. Esta sección y el resto del capítulo se va a enfocar en introducir lo que es una hipótesis estadística, cuáles son de forma general las hipótesis que se usan, y qué conclusiones (resultados) se obtienen a partir de éstas. En el próximo capítulo (Pruebas Estadísticas) se presentarán diferentes pruebas que se pueden realizar y que complementan a la estimación. De acuerdo con Triola (2004) y Walpole et al. (2012) una hipótesis es una aseveración o conjetura respecto a una o más poblaciones, y nunca se va a saber con certeza si es correcta o no. Para intentar responder a la hipótesis planteada se toma una muestra representativa y los datos de ésta se usan en pruebas estadísticas para llegar a una conclusión (Walpole et al., 2012). 14.3.1 Nula (\\(H_0\\)) y Alterna (\\(H_1\\)) Las pruebas de hipótesis, que se van a mostrar más específicamente en el próximo capítulo, se formalizan por medio de lo que se conoce como hipótesis nula e hipótesis alterna. La hipótesis alterna es típicamente la aseveración de interés o lo que quiere probar, y se simboliza con \\(H_1\\), mientras que la hipótesis nula es lo que se opone a la alterna y se simboliza con \\(H_0\\). La hipótesis nula por lo general hace referencia a la ausencia de una relación o diferencia, mientras que la hipótesis alterna hace referencia a la existencia de una relación o diferencia, un efecto medible (Swan &amp; Sandilands, 1995; Trauth, 2015; Triola, 2004; Walpole et al., 2012). 14.3.2 Resultados y Errores Dado que no se sabe con certeza si las aseveraciones propuestas son correctas o no es que los dos posibles resultados o conclusiones después de realizar una prueba de hipótesis son: rechazar \\(H_0\\) en favor de \\(H_1\\) hay suficiente evidencia, resultado erróneo al azar (\\(\\alpha\\)) no rechazar \\(H_0\\) no hay suficiente evidencia, muestra no representativa, es verdadera, resultado erróneo al azar (\\(\\beta\\)) De nuevo, dada la incertidumbre en los datos es que no se puede aceptar \\(H_0\\), solo se puede no rechazar, ya que la muestra que se utilizó puede que no fuera lo suficientemente representativa o grande (Swan &amp; Sandilands, 1995; Triola, 2004; Walpole et al., 2012). La Tabla 14.5 muestra los posibles resultados que se pueden obtener al realizar una prueba de hipótesis (Tabla 14.6 muestra la misma tabla pero con términos estadísticos). Dentro de éstos se incluyen los mencionado anteriormente, y se incluyen los posibles errores que se pueden cometer, dado que hay incertidumbre en los datos. El error tipo I es cuando se rechaza \\(H_0\\) cuando ésta es correcta y corresponde con \\(\\alpha\\) el nivel de significancia. El error tipo II es cuando no se rechaza \\(H_0\\) cuando ésta es falsa y corresponde con \\(\\beta\\). El complemento de \\(\\beta\\) (\\(1-\\beta\\)) corresponde con el poder (potencia) de la prueba y corresponde con rechazar \\(H_0\\) cuando es falsa (decisión correcta), y en la mayoría de los círculos científicos se usa 80% como mínimo valor aceptable. Ambos errores se pueden reducir incrementando el tamaño de la muestra (Nolan &amp; Heinzen, 2014; Swan &amp; Sandilands, 1995; Walpole et al., 2012). Tabla 14.5: Posibles resultados de una prueba de hipótesis \\(H_0\\) es verdadera \\(H_0\\) es falsa No rechazar \\(H_0\\) Decisión correcta Error Tipo II Rechazar \\(H_0\\) Error Tipo I Decisión correcta Tabla 14.6: Términos estadísticos para posibles resultados de una prueba de hipótesis \\(H_0\\) es verdadera \\(H_0\\) es falsa No rechazar \\(H_0\\) Nivel de confianza (1 - \\(\\alpha\\)) \\(\\beta\\) Rechazar \\(H_0\\) \\(\\alpha\\) Potencia (\\(1-\\beta\\)) 14.3.3 Estadístico de prueba y crítico El estadístico de prueba (\\(z\\), \\(t\\), \\(\\chi^2\\), \\(F\\), etc.) es el que se obtiene a partir de los datos muestrales, asumiendo que \\(H_0\\) es verdadera, y es el que se va a usar para decidir si se rechaza o no la hipótesis nula. El estadístico o valor crítico es el que va a delimitar el valor mínimo de la región de rechazo, la cuál va a comprender todos los valores posibles que van a resultar en un rechazo de la hipótesis nula. El valor crítico está en función del nivel de significancia \\(\\alpha\\), y debe ser escogido con anterioridad a la recolección de los datos (Cumming &amp; Calin-Jageman, 2017; Swan &amp; Sandilands, 1995; Triola, 2004; Walpole et al., 2012). Este estadístico crítico era el que se usó en la sección de Estimación para construir los intervalos de confianza. 14.3.4 Contrastes y valor-p Al formular las hipótesis (especialmente la alterna \\(H_1\\)) éstas pueden ser de tres tipos (Figura 14.11) (Nolan &amp; Heinzen, 2014; Triola, 2004; Walpole et al., 2012): Dos colas (bilateral): La región de rechazo se divide en dos partes iguales en ambos extremos de la distribución \\(H_0: \\theta = \\theta_0\\) \\(H_1: \\theta \\ne \\theta_0\\) Cola izquierda (unilateral izquierda): La región de rechazo se encuentra completamente en el extremo izquierdo (inferior) de la distribución. \\(H_0: \\theta \\ge \\theta_0\\) \\(H_1: \\theta &lt; \\theta_0\\) Cola derecha (unilateral derecha): La región de rechazo se encuentra completamente en el extremo derecho (superior) de la distribución. \\(H_0: \\theta \\le \\theta_0\\) \\(H_1: \\theta &gt; \\theta_0\\) Figura 14.11: Posibles contrastes para una prueba de hipótesis. Superior es bilateral, Medio es unilateral izquierda, Inferior es unilateral derecha. Tomado de Triola (2004). El valor-p (\\(p\\)) es la probabilidad de obtener un resultado tan extremo como el observado si \\(H_0\\) es verdadera (NO es la probabilidad de que \\(H_0\\) sea verdad), en notación probabilística sería \\(P(resultado|H_0)\\), la probabilidad del resultado dado \\(H_0\\). Este valor va a depender del tamaño de la muestra, ya que a mayor tamaño de la muestra menor el error estándar (\\(s/\\sqrt(n)\\)) y menor el valor-p y más extremo el resultado (Nolan &amp; Heinzen, 2014; Triola, 2004; Walpole et al., 2012). El procedimiento de cómo calcular el valor-p se muestra en la Figura 14.12. Figura 14.12: Procedimiento para calcular el valor-p. Tomado de Triola (2004). 14.3.5 Decisión De manera general se rechaza \\(H_0\\) si: \\(p &lt; \\alpha\\), estadístico de prueba es mayor que el estadístico crítico (ej: \\(t &gt; t_{crit}\\)), parámetro poblacional cae fuera del intervalo de confianza. Referencias "],
["pruebas-estadísticas.html", "Capítulo 15 Pruebas Estadísticas 15.1 Introducción 15.2 Pruebas paramétricas 15.3 Tamaño del Efecto 15.4 \\(z\\) de 1 muestra 15.5 \\(t\\) de 1 muestra 15.6 \\(t\\) de 2 muestras independientes 15.7 \\(t\\) de 2 muestras dependientes 15.8 ANOVA de 1-factor entre-grupos 15.9 \\(t\\) de correlación 15.10 \\(\\chi^2\\) para 1 varianza 15.11 \\(F\\) para 2 varianzas", " Capítulo 15 Pruebas Estadísticas 15.1 Introducción En la sección 14.3 del capítulo anterior se introdujeron las bases teóricas de lo que son las pruebas de hipótesis, las partes o elementos que la componen, y cómo tomar una decisión una vez llevada a cabo la prueba. Este capítulo se centra en la parte práctica de cómo realizar una prueba estadística paramétrica (los pasos generales). Además se introduce el concepto de tamaño del efecto (effect size - ES - en inglés), y de cómo puede éste agregar información después de realizada la prueba, así como la importancia práctica que representa. 15.2 Pruebas paramétricas De manera general se van a cubrir las pruebas estadísticas homólogas con las estimaciones que se presentaron en el capítulo anterior en la sección Estimación (14.2), donde la práctica en general es usar un contraste bilateral a menos que sea muy justificado un contraste unilateral. Las pruebas se resumen a continuación y serán detalladas más adelante: Prueba \\(z\\) de 1 muestra (15.4) \\(H_0: \\mu = \\mu_0\\) Prueba \\(t\\) de 1 muestra (15.5) \\(H_0: \\mu = \\mu_0\\) Prueba \\(t\\) de 2 muestras independientes (15.6) \\(H_0: \\mu_1 = \\mu_2\\) Prueba \\(t\\) de 2 muestras dependientes (15.7) \\(H_0: \\mu_D = 0\\) Prueba \\(F\\) para 2 o más muestras (15.8) \\(H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\cdots = \\mu_k\\) Prueba \\(t\\) de correlación (15.9) \\(H_0: r = 0\\) Prueba \\(\\chi^2\\) para 1 varianza (15.10) \\(H_0: \\sigma^2 = \\sigma^2_0\\) Prueba \\(F\\) para 2 varianzas (15.11) \\(H_0: \\sigma^2_1 = \\sigma^2_2 \\to \\frac{\\sigma^2_1}{\\sigma^2_2} = 1\\) 15.2.1 Supuestos y Pasos Las pruebas presentadas en este capítulo se conocen como pruebas paramétricas, que a diferencia de las pruebas no-parámetricas que se presentan en el próximo capítulo, hacen supuestos sobre los datos que se van a analizar, siendo el mayor supuesto de que los datos siguen una distribución aproximadamente normal (o se tiene una muestra grande, \\(n &gt; 30\\)), y que las muestras son aleatoreas independientes (excepto en el caso de datos apareados) (Nolan &amp; Heinzen, 2014; Triola, 2004); en algunos casos pueden haber otros supuestos (parámetros conocidos, varianzas iguales, etc.) pero en general éstos son los más importantes. Para evaluar la “normalidad” de los datos se pueden generar gráficos como el de caja, histogramas, y QQ (Figura 15.1). El gráfico de caja muestra la mediana y los cuartiles, mientras la mediana no se encuentre muy cerca de alguno de los extremos de la caja se puede considerar normalmente distribuida; en caso de haber valores atípicos (extremos) éstos aparecerían como puntos en los extremos. El gráfico QQ hace una comparación de los valores originales con los cuantiles teóricos, la idea es que si los puntos caen cerca de la línea se puede considerar normalmente distribuida; puede que en las colas haya cierta desviación pero eso es normal siempre y cuando no sea muy extrema. El histograma se puede comparar con una curva normal y ver asimetrías, similar al de caja, mientras las asimetría sea nula o mínima se puede considerar normalmente distribuida. Hay pruebas específicas para evaluar la normalidad (Shapiro-Wilk), pero éstas tienden a ser muy sensibles a desviaciones por lo que en general es más recomendado hacer una evaluación visual Figura 15.1: Gráficos que se pueden usar para evaluar si los datos siguen una distribución aproximandamente normal. A Gráfico de caja. B Gráfico QQ. C Histograma. Nolan &amp; Heinzen (2014) establecen una serie de pasos que se pueden aplicar de manera general a las pruebas estadísticas: Identificar la población, distribución, y la prueba apropiada: En función del parámetro de interés (\\(\\mu, \\sigma^2\\)) Establecer las hipótesis nula y alterna: En función de la prueba escogida en el punto anterior (\\(z,t,\\chi^2,F\\)) Determinar parámetros de la distribución a comparar (\\(H_0\\)): En función del parámetro de interés y datos disponibles Determinar valores críticos En función de la distribución y el nivel de significancia (\\(\\alpha\\)) Calcular el estadístico de prueba En función de la prueba escogida y datos disponibles Aquí se puede agregar calcular los intervalos de confianza respectivos Tomar una decisión En función de valores críticos y estadístico de prueba, valor-p, o intervalos de confianza Si se rechaza \\(H_0\\) se dice que hay un resultado estadísticamente significativo. Esta aseveración se refiere a que hay poca probabilidad de que los datos observados provengan de una población representada por \\(H_0\\). El hecho de que se rechace \\(H_0\\) y se obtenga un resultado estadísticamente significativo no quiere decir que éste sea importante desde el punto de vista práctico (Nolan &amp; Heinzen, 2014). Esto último es lo que va a indicar el tamaño del efecto. 15.3 Tamaño del Efecto Cuando una prueba da un resultado estadísticamente significativo, quiere decir que se determinó que hay una diferencia o relación. Como se mencionó en el capítulo anterior, este resultado se va a ver afectado directamente por el tamaño de la muestra, por lo que en el caso de muestras grandes casí siempre se va a encontrar un resultado estadísticamente significativo, aun cuando la diferencia o relación real sea pequeña. El tamaño del efecto (ES) es lo que indica la magnitud real de esa diferencia o relación, no se ve afectada por el tamaño de la muestra, e indica la importancia práctica de un efecto, y debe ser el objetivo a la hora de realizar análisis estadísticos (Cohen, 1988; Cumming, 2012; Cumming &amp; Calin-Jageman, 2017; Nolan &amp; Heinzen, 2014). El tamaño del efecto puede presentarse en las unidades originales o de forma estandarizada (más usadas) y se reconocen dos familias: diferencia entre medias (familia \\(d\\) y sus variantes), asociación/relación entre variables (familia \\(r\\) y similares como \\(R^2, \\eta^2, V\\)), y dentro de lo posible es recomendable incluir intervalos de confianza (Cohen, 1988; Cumming, 2012; Cumming &amp; Calin-Jageman, 2017; Lakens, 2013; Nakagawa &amp; Cuthill, 2007). Para los tamaños de efecto es complicado calcular de forma manual los intervalos de confianza, en la mayoría de los casos se usa la distribución \\(t\\) no-central, por lo que se recomienda utilizar métodos computacionales (Cumming, 2012; Cumming &amp; Calin-Jageman, 2017). A parte de presentar los resultados de pruebas estadísticas, sea que se rechace o no la hipótesis nula, es necesario incluir el tamaño del efecto para brindar una mejor idea del resultado encontrado, así como para posibles usos en futuros estudios (meta-analisis, cálculos de tamaños de muestra, potencia, etc.), y generar una base de resultados resportados en un área específica del saber para determinar qué se puede considerar como un efecto pequeño, mediano o grande (American Psycological Association [APA] 2010; Cumming, 2012; Cumming &amp; Calin-Jageman, 2017; Nakagawa &amp; Cuthill, 2007; Thompson, 2007; Tomczak &amp; Tomczak, 2014). La diferencia entre medias estandarizada se conoce como Cohen \\(d\\) (Cohen, 1988), de manera general se presenta en la Ecuación (15.1), y se puede visualizar en la Figura 15.2. Se interpreta de manera similar a \\(Z\\), donde la diferencia entre medias está dada en función de una desviación estándar. La desviación estándar (denominador) es la que va a cambiar dependiendo de la prueba y condiciones de la misma (Tabla 15.1). \\[\\begin{equation} d = \\frac{\\bar{x}_1-\\bar{x}_2}{\\sigma} \\tag{15.1} \\end{equation}\\] Figura 15.2: Representación del Cohen \\(d\\). Hay dos factores que controlan el tamaño de \\(d\\), primero al incrementar la diferencia entre medias (manteniendo la dispersión constante) mayor será \\(d\\), segundo al disminuir la dispersión (manteniendo la diferencia de medias constante) mayor será \\(d\\). Estos dos factores van a influenciar el traslape entre las curvas, donde a mayor \\(d\\) menor el traslape y mayor la diferencia entre las curvas y las muestras. Al igual que \\(Z\\) puede tomar valores de \\(-∞\\) a \\(∞\\). Dada la naturaleza estadística del Cohen \\(d\\) y la dificultad de su interpretación para personas con poco conocimiento estadístico, se han generado otras métricas comparables que pueden ser más fáciles de entender en términos generales. Grissom &amp; Kim (2005), McGraw &amp; Wong (1992), y Ruscio (2008) desarrollaron el concepto de probabilidad de superioridad (PS) o lenguage común del tamaño del efecto (CL) el cuál se puede calcular de acuerdo a la Ecuación (15.2). Este concepto se interpreta como la probabilidad de que un elemento del grupo con media superior (elegido al azar) tenga un valor superior al de un elemento del grupo con media inferior, y al ser una probabilidad se encuentra entre 0 y 100%, conforme mayor sea \\(d\\) mayor será PS. Reiser &amp; Faraggi (1999) modificaron de Cohen (1988) el traslape (OVL) entre las curvas (grupos), dado por la Ecuación (15.3), donde a mayor \\(d\\) menor el traslape y más diferenciados los grupos. Cohen (1988) definió el concepto de \\(U_3\\), denominado la medida de no-traslape, presentado en Ecuación (15.4), y se puede interpretar como el porcentaje del grupo con media superior que va a estar por encima de la media del grupo con media inferior. Para todos estos casos \\(\\Phi\\) corresponde con la función de densidad acumulada de la distribución normal estándar \\(Z\\). Cabe resaltar que éstos conceptos y cálculos se basan en grupos de mismo tamaño y misma desviación estándar.Un muy buen recurso para visualizar todos estos conceptos lo brinda Magnusson (2020) en Interpreting Cohen’s d. \\[\\begin{equation} PS = CL = \\Phi \\left( \\frac{|d|}{\\sqrt{2}} \\right) \\tag{15.2} \\end{equation}\\] \\[\\begin{equation} OVL = 2 \\Phi \\left( \\frac{-|d|}{2} \\right) \\tag{15.3} \\end{equation}\\] \\[\\begin{equation} U_3 = \\Phi \\left( |d| \\right) \\tag{15.4} \\end{equation}\\] Aquí se presenta cómo calcular estos conceptos en R, donde el único dato necesario es el \\(d\\). d = 2 pnorm(abs(d)/sqrt(2))*100 # PS, CL ## [1] 92.13504 2*pnorm(-abs(d)/2)*100 # traslape ## [1] 31.73105 pnorm(abs(d))*100 # U3 ## [1] 97.72499 Muchos autores han trabajado en este tema, proponiendo mejoras a los tamaños de efecto inicialmente planteados y resumiendo los diferentes tamaños de efecto para las diferentes pruebas y condiciones (Cohen, 1988; Cumming, 2012; Cumming &amp; Calin-Jageman, 2017; Ellis, 2010; Fritz et al., 2012; Grissom &amp; Kim, 2005; Hedges &amp; Olkin, 1985; Lakens, 2013; McGrath &amp; Meyer, 2006; McGraw &amp; Wong, 1992; Nakagawa &amp; Cuthill, 2007; Thompson, 2007; Tomczak &amp; Tomczak, 2014; Zou, 2007). Aquí se presenta un resumen de los diferentes tamaños de efecto (Tabla 15.1) y se remite al lector a revisar las referencias aquí mencionadas y las que se incluyen en las mismas, en el caso de querer ahondar en alguno de los casos presentados. Tabla 15.1: Tamaños de efecto estandarizados para pruebas paramétricas Prueba Tamaño de efecto Z de 1 muestra \\[\\begin{equation} d_{pop} = \\frac{\\bar{x}-\\mu_0}{\\sigma} \\tag{15.5} \\end{equation}\\] t de 1 muestra \\[\\begin{equation} d_s = \\frac{\\bar{x}-\\mu_0}{s} \\tag{15.6} \\end{equation}\\] t de 2 muestras independientes \\[\\begin{equation} d_s = \\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}} \\tag{15.7} \\end{equation}\\] t de 2 muestras dependientes usando la desviación de las diferencias \\[\\begin{equation} d_z = \\frac{\\bar{x}_d}{s_d} \\tag{15.8} \\end{equation}\\] t de 2 muestras dependientes tomando en cuenta la correlación \\[\\begin{equation} d_{rm} = \\frac{\\bar{x}_d}{\\sqrt{s_1^2 + s_2^2 - 2 \\cdot r \\cdot s_1 \\cdot s_2}}\\sqrt{2(1-r)} \\tag{15.9} \\end{equation}\\] t de 2 muestras dependientes usando el promedio de las desviaciones \\[\\begin{equation} d_{av} = \\frac{\\bar{x}_d}{\\sqrt{\\frac{s_1^2 + s_2^2}{2}}} \\tag{15.10} \\end{equation}\\] t con correción de Hedges \\[\\begin{equation} g = d \\cdot \\left( 1 - \\frac{3}{4v-1} \\right) \\tag{15.11} \\end{equation}\\] Correlación punto biserial a partir de \\(d_s\\) para 2 muestras independientes \\[\\begin{equation} r_{pb} = \\frac{d_s}{\\sqrt{d_s^2 + \\frac{(n_1 + n_2)^2}{n_1 n_2}}} \\tag{15.12} \\end{equation}\\] Correlación \\[\\begin{equation} r \\tag{15.13} \\end{equation}\\] Regresión \\[\\begin{equation} R^2 \\tag{15.14} \\end{equation}\\] ANOVA \\[\\begin{equation} \\eta^2 = \\frac{SC_{efecto}}{SC_{total}} \\tag{15.15} \\end{equation}\\] ANOVA \\[\\begin{equation} \\eta_p^2 = \\frac{SC_{efecto}}{SC_{efecto}+SC_{error}} \\tag{15.16} \\end{equation}\\] Notas: \\(\\mu_0\\) = media poblacional \\(\\sigma\\) = desviación estándar poblacional \\(\\bar{x}_k\\) = media muestral de \\(k\\) \\(s_k\\) = desviación estándar muestral de \\(k\\) \\(s_k^2\\) = varianza muestral de \\(k\\) \\(n_k\\) = tamaño de muestra de \\(k\\) \\(v\\) = grados de libertad \\(\\bar{x}_d\\) = media de la diferencia entre muestras \\(s_d\\) = desviación estándar de la diferencia entre muestras \\(r\\) = coeficiente de correlación de Pearson \\(r_{pb}\\) = coeficiente de correlación punto biserial \\(SC\\) = suma de cuadrados en ANOVA La Tabla 15.1 presenta ambas familias de tamaños de efecto, donde \\(r\\) y \\(R^2\\) se interpretan como en las secciones 10.3 y 10.4.4.2, respectivamente. \\(\\eta^2\\) se interpreta igual a \\(R^2\\), como el porcentaje de variación en la variable respuesta explicado por la variable predictora (Cohen, 1988; Lakens, 2013; Tomczak &amp; Tomczak, 2014). Una pregunta que puede surgir es qué magnitud de los diferentes tamaños de efecto se puede considerar pequeña, mediana, o grande. Cohen (1988) sugirió unos valores que se pueden usar como guías (Tabla 15.2), los cuales han surgido y fueron determinados como tales en las ciencias sociales. El mismo autor advierte sobre utilizar estos valores como definitivos y estrictos, y menciona que se debe aplicar criterio de experto en el área específica para determinar que califica como pequeño, mediano, o grande. Tabla 15.2: Clases comunmente usadas para diferentes tamaños de efecto Clases del tamaño del efecto Prueba Tamaño de efecto Pequeño Mediano Grande Comparación de medias \\(d\\), \\(g\\) 0.20 0.50 0.80 Correlación \\(r\\) 0.10 0.30 0.50 \\(R^2\\) 0.01 0.09 0.25 \\(r_{pb}\\) 0.10 0.24 0.37 Regresión \\(R^2\\) 0.02 0.13 0.26 ANOVA \\(\\eta^2\\) 0.01 0.06 0.14 Nota: Conforme Cohen (1988) 15.4 \\(z\\) de 1 muestra El uso de la prueba \\(z\\) de 1 muestra se realiza con el ejemplo de la sección 14.2.1.1 (Walpole et al., 2012), donde se tenía una media muestral de \\(2.6 \\ g/ml\\) para una muestra de tamaño 36 y se asumía una deviación poblacional de \\(0.3 \\ g/ml\\). Es posible que esta muestra provenga de una población con media 2.8 (\\(\\mu_0=2.8\\))? Asuma \\(\\alpha = 0.05\\). Usando los pasos mencionados anteriormente (15.2.1) se tiene: Identificar la población, distribución, y la prueba apropiada: Población: concentración de zinc en un río Distribución: de medias Prueba: \\(Z\\) de 1 muestra porque se tiene 1 muestra, se quiere comparar con un valor hipotético, y se tiene \\(\\sigma\\) Establecer las hipótesis nula y alterna: \\(H_0: \\mu = \\mu_0 \\to\\) La concentración de zinc en el río es igual a un valor hipotético o conocido (2.8) \\(H_1: \\mu \\neq \\mu_0 \\to\\) La concentración de zinc en el río es diferente a un valor hipotético o conocido (2.8) Determinar parámetros de la distribución a comparar (\\(H_0\\)): \\(\\mu_0 = 2.8\\) \\(\\sigma_\\bar{x} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{0.3}{\\sqrt{36}} = 0.05\\) Determinar valores críticos \\(\\alpha = 0.05\\) \\(z_{\\alpha/2} = z_{0.05/2} = |1.96|\\) Calcular el estadístico de prueba \\(Z = \\frac{\\bar{x} - \\mu}{\\sigma_\\bar{x}} = \\frac{2.6 - 2.8}{0.05} = -4\\) \\(2.6 \\pm 0.1 \\to 95\\% \\ IC \\ [2.50,2.70]\\) Tomar una decisión El estadístico de prueba es mayor al crítico, \\(z &gt; z_{\\alpha/2}\\) El valor-p es menor a \\(\\alpha = 0.05\\), \\(p &lt; .001\\) El valor hipotético del parámetro cae fuera del intervalo de confianza, \\(IC \\ [2.50,2.70]\\) Decisión: Se rechaza \\(H_0\\) En R el paquete DescTools tiene la función ZTest para realizar esta prueba, pero necesita un vector de datos, por lo que se genera un vector aleatorio, y se demuestra a continuación. x = 2.6 n = 36 sig = 0.3 mu0 = 2.8 a = 0.05 set.seed(123) vec = rnorm(n = n, mean = x, sd = sig) z1.res = ZTest(vec, mu = mu0, # valor del parámetro a comparar sd_pop = sig, # deviación poblacional conf.level = 1-a) # nivel de confianza z1.res ## ## One Sample z-test ## ## data: vec ## z = -3.6664, Std. Dev. Population = 0.3, p-value = 0.000246 ## alternative hypothesis: true mean is not equal to 2.8 ## 95 percent confidence interval: ## 2.518683 2.714680 ## sample estimates: ## mean of x ## 2.616681 El tamaño del efecto se puede calcular de acuerdo a la Ecuación (15.5) de la siguiente manera: \\[\\begin{equation} d_{pop} = \\frac{\\bar{x}-\\mu_0}{\\sigma}\\\\ d_{pop} = \\frac{2.6-2.8}{0.3} = -0.67 \\end{equation}\\] En R se puede calcular de forma básica y el intervalo de confianza con la función d.ci del paquete psych (Revelle, 2020), donde es necesario indicar \\(d\\) y \\(n\\). dpop = (x - mu0) / sig dpop.ci = d.ci(dpop,n1=n) %&gt;% round(2) dpop.ci ## lower effect upper ## [1,] -1.02 -0.67 -0.3 Usando las medidas PS, OVL, y \\(U_3\\) (Figura 15.3). ## # A tibble: 1 x 3 ## PS OVL U3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 68.1 73.9 74.8 Figura 15.3: \\(U_3\\) para el ejemplo de la prueba \\(z\\) de 1 muestra. A Corresponde con los datos en escala original. B Corresponde con el tamaño del efecto \\(d\\). Conclusión: La concentración de zinc (\\(M = 2.62\\), 95% IC \\([2.52\\), \\(2.71]\\)) es signficativamente diferente a la media de 2.8 g/ml, \\(z(36) = -3.67\\), \\(p &lt; .001\\), \\(d = -0.67 \\ [-1.02, -0.3]\\). El efecto se puede considerar mediano, pero con un rango de pequeño hasta muy grande. Hay una probabilidad de 68.1% (PS) que un elemento de la población hipotética tenga una concentración mayor a un elemento de la muestra; las dos curvas se traslapan en un 73.9% (OVL); el 74.8% (\\(U_3\\)) de la población hipotética se encuentra por encima de la media de la muestra. 15.5 \\(t\\) de 1 muestra El uso de la prueba \\(t\\) de 1 muestra se realiza con el ejemplo de la sección 14.2.1.2 (Swan &amp; Sandilands, 1995), donde se tenía el contenido de cuarzo en secciones delgadas de una roca ígnea. Es posible que esta muestra provenga de una población con media 20% (\\(\\mu_0=20\\))? Asuma \\(\\alpha = 0.05\\). Identificar la población, distribución, y la prueba apropiada: Población: contenido de cuarzo en la roca ígnea Distribución: de medias Prueba: \\(t\\) de 1 muestra porque se tiene 1 muestra, se quiere comparar con un valor hipotético, y no se tiene \\(\\sigma\\) Establecer las hipótesis nula y alterna: \\(H_0: \\mu = \\mu_0 \\to\\) El contenido de cuarzo en la roca ígnea es igual a un valor hipotético o conocido (20) \\(H_1: \\mu \\neq \\mu_0 \\to\\) El contenido de cuarzo en la roca ígnea es diferente a un valor hipotético o conocido (20) Determinar parámetros de la distribución a comparar (\\(H_0\\)): \\(\\mu_0 = 20\\) \\(s_\\bar{x} = \\frac{s}{\\sqrt{n}} = \\frac{3.083}{\\sqrt{8}} = 1.09\\) Determinar valores críticos \\(\\alpha = 0.05\\) \\(t_{\\alpha/2,v} = t_{0.05/2,7} = |2.365|\\) Calcular el estadístico de prueba \\(t = \\frac{\\bar{x} - \\mu}{s_\\bar{x}} = \\frac{21.5 - 20}{1.09} = 1.37\\) \\(21.512 \\pm 2.578 \\to 95\\% \\ IC \\ [18.93, 24.09]\\) Tomar una decisión El estadístico de prueba es menor al crítico, \\(t &lt; t_{\\alpha/2,v}\\) El valor-p es mayor a \\(\\alpha = 0.05\\), \\(p = 0.2079\\) El valor hipotético del parámetro cae dentro del intervalo de confianza, \\(IC \\ [18.93, 24.09]\\) Decisión: No se rechaza \\(H_0\\) R trae la función t.test que puede realizar las diferentes pruebas \\(t\\). Para el caso de 1 muestra se brinda el vector de datos, la media poblacional con la cual comparar (mu), y el nivel de confianza. mu0 = 20 a = 0.05 cuarzo = c(23.5, 16.6, 25.4, 19.1, 19.3, 22.4, 20.9, 24.9) t1.res = t.test(cuarzo, mu = mu0, conf.level = 1-a) t1.res ## ## One Sample t-test ## ## data: cuarzo ## t = 1.3875, df = 7, p-value = 0.2079 ## alternative hypothesis: true mean is not equal to 20 ## 95 percent confidence interval: ## 18.93477 24.09023 ## sample estimates: ## mean of x ## 21.5125 El tamaño del efecto se puede calcular de acuerdo a la Ecuación (15.6) de la siguiente manera: \\[\\begin{equation} d_s = \\frac{\\bar{x}-\\mu_0}{s}\\\\ d_s = \\frac{21.5-20}{3.083} = 0.49 \\end{equation}\\] En R se puede calcular con la función d.single.t del paquete MOTE (Buchanan et al., 2019), donde es necesario indicar la media muestral, media poblacional a comparar, desviación estándar muestral, tamaño de la muestra, y nivel de significancia. d.t1.res = d.single.t(m = mean(cuarzo), u = mu0, sd = sd(cuarzo), n = length(cuarzo), a = a) d.t1.res[1:3] %&gt;% unlist() ## d dlow dhigh ## 0.4905400 -0.2619016 1.2128190 Usando las medidas PS, OVL, y \\(U_3\\) (Figura 15.4). ## # A tibble: 1 x 3 ## PS OVL U3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 63.6 80.6 68.8 Figura 15.4: \\(U_3\\) para el ejemplo de la prueba \\(t\\) de 1 muestra. A Corresponde con los datos en escala original. B Corresponde con el tamaño del efecto \\(d\\). Conclusión: El contenido de cuarzo de la roca ígnea (\\(M = 21.51\\), 95% IC \\([18.93\\), \\(24.09]\\)) no es signficativamente diferente a la media de 20% , \\(t(7) = 1.39\\), \\(p = .208\\), \\(d\\) = 0.49 \\([-0.26, 1.21]\\). El efecto se puede considerar mediano, pero con un rango de pequeño, en dirección opuesta, hasta muy grande. Hay una probabilidad de 63.6% (PS) que un elemento de la muestra tenga un contenido de cuarzo mayor a un elemento de la población hipotética; las dos curvas se traslapan en un 80.6% (OVL); el 68.8% (\\(U_3\\)) de la muestra se encuentra por encima de la media de la población hipotética. 15.6 \\(t\\) de 2 muestras independientes Para este caso es cuando, en teoría, se debiera cumplir no solo la normalidad de las muestras, sino también la igualdad de varianzas. Si se quiere relajar el supuesto de igualdad de varianzas se usa el estadístico Welch o Welch-Satterthwaite (14.2.2.2.1 - varianzas diferentes). En general la diferecia entre asumir igualdad de varianzas o no va a ser pequeña, siempre y cuando éstas no sean muy diferentes y los tamaños de muestra no sean muy disparejos, y es recomendado indicar qué método se usó (Cumming &amp; Calin-Jageman, 2017). El uso de la prueba \\(t\\) de 2 muestras independientes se realiza con el ejemplo de la sección 14.2.2.2.1 (Swan &amp; Sandilands, 1995), el de varianzas iguales, donde se tenían braquiópodos en dos capas (A, B) y se les midió la longitud (cm). Es posible que estas muestras provenga de una misma población (\\(\\mu_1=\\mu_2\\))? Asuma \\(\\alpha = 0.05\\). Identificar la población, distribución, y la prueba apropiada: Población: braquiópodos en capas A y B Distribución: diferencia de medias Prueba: \\(t\\) de 2 muestras independientes porque se tienen 2 muestras, y las dos provienen de sitios diferentes (independientes uno del otro) Establecer las hipótesis nula y alterna: \\(H_0: \\mu_1 = \\mu_2 \\to\\) La longitud e los braquiópodos en la capa A es igual a la longitud e los braquiópodos en la capa B \\(H_1: \\mu \\neq \\mu_0 \\to\\) La longitud e los braquiópodos en la capa A es diferente la longitud e los braquiópodos en la capa B Determinar parámetros de la distribución a comparar (\\(H_0\\)): \\(\\mu_1 - \\mu_2 = 0\\) \\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}} = \\sqrt{\\frac{(8-1)0.042 + (10-1)0.029}{8 + 10 - 2}} = 0.1865\\) \\(s_{\\bar{x}_1-\\bar{x}_2} = s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} = 0.1865 \\sqrt{\\frac{1}{8} + \\frac{1}{10}} = 0.0885\\) Determinar valores críticos \\(\\alpha = 0.05\\) \\(t_{\\alpha/2,v} = t_{0.05/2,16} = |2.12|\\) Calcular el estadístico de prueba \\(t = \\frac{(\\bar{x}_1-\\bar{x}_2) - (\\mu_1-\\mu_2)}{s_{\\bar{x}_1-\\bar{x}_2}} = \\frac{(3.125-2.96) - 0}{0.0885} = 1.86\\) \\(0.165 \\pm 0.1876 \\to 95\\% \\ IC \\ [-0.023,0.353]\\) Tomar una decisión El estadístico de prueba es menor al crítico, \\(t &lt; t_{\\alpha/2,v}\\) El valor-p es mayor a \\(\\alpha = 0.05\\), \\(p = 0.081\\) El valor de \\(0\\) cae dentro del intervalo de confianza, \\(IC \\ [-0.023,0.353]\\) Decisión: No se rechaza \\(H_0\\) R trae la función t.test que puede realizar las diferentes pruebas \\(t\\). Para el caso de 2 muestras independientes se brindan los vectores de datos, el indicador lógico de si las varianzas se deben considerar iguales, y el nivel de confianza. a = 0.05 A = c(3.2, 3.1, 3.1, 3.3, 2.9, 2.9, 3.5, 3.0) B = c(3.1, 3.1, 2.8, 3.1, 3.0, 2.6, 3.0, 3.0, 3.1, 2.8) t2.ind.res = t.test(x = A, y = B, var.equal = T, conf.level = 1-a) t2.ind.res ## ## Two Sample t-test ## ## data: A and B ## t = 1.861, df = 16, p-value = 0.08122 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.02295489 0.35295489 ## sample estimates: ## mean of x mean of y ## 3.125 2.960 Como se mencionó en la sección 14.2.2.2.1, las pruebas de hipótesis se pueden aproximar por medio de la estimación, más específicamente, los intervalos de confianza (Figura 15.5), siguiendo las guías de Cumming &amp; Finch (2005), Cumming (2012), y Cumming &amp; Calin-Jageman (2017). El intervalo de confianza para el parámetro de interés se puede comparar con \\(H_0\\), y se puede obtener la misma conclusión a realizar la prueba estadística. Figura 15.5: Intervalos de confianza para la prueba t de 2 muestras independientes. El valor de 0 cae dentro del intervalo de confianza para la diferencia, por lo que se pueden considerar iguales o que no hay diferencia entre las capas. El tamaño del efecto se puede calcular de acuerdo a las Ecuaciones (15.7) y (15.11), donde de manera general se usa la desviación agrupada (pooled standard deviation ,\\(s_p\\)) como el estandarizador, sin importar si se asumen varianzas iguales o no para la prueba, pero es buena práctica reportar qué se uso como estandarizador (Cumming &amp; Calin-Jageman, 2017). \\[\\begin{equation} d_s = \\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}}\\\\ d_s = \\frac{3.125-2.96}{\\sqrt{\\frac{(8-1)0.042 + (10-1)0.029}{8 + 10 - 2}}} = 0.88 \\end{equation}\\] \\[\\begin{equation} g_s = d_s \\cdot \\left( 1 - \\frac{3}{4v-1} \\right)\\\\ g_s = 0.88 \\cdot \\left( 1 - \\frac{3}{4(16)-1} \\right) = 0.84 \\end{equation}\\] La correción de Hedges (\\(g_s\\)) se usa ya que \\(d_s\\) se considera un estimador sesgado (biased) especialmenta para muestras pequeñas, conforme mayor sea el tamaño de muestra más similares \\(d_s\\) y \\(g_s\\) (Cumming, 2012; Cumming &amp; Calin-Jageman, 2017; Fritz et al., 2012; McGrath &amp; Meyer, 2006). En R se puede calcular \\(d_s\\) con la función d.ind.t y \\(g_s\\) con la función g.ind.t del paquete MOTE, donde es necesario indicar la media muestral de ambas muetras, desviación estándar muestral de ambas muestras, tamaño ambas muestras, y nivel de significancia. Adicionalmente existen las funciones cohens_d y hedges_g de effectsize (Ben-Shachar et al., 2020), y cohen.d de effsize (Torchiano, 2018). Usando MOTE d.t2.ind.res = d.ind.t(m1 = mean(A), m2 = mean(B), sd1 = sd(A), sd2 = sd(B), n1 = length(A), n2 = length(B), a = a) d.t2.ind.res[1:3] %&gt;% unlist() ## d dlow dhigh ## 0.8827506 -0.1075693 1.8483100 g.t2.ind.res = g.ind.t(m1 = mean(A), m2 = mean(B), sd1 = sd(A), sd2 = sd(B), n1 = length(A), n2 = length(B), a = a) g.t2.ind.res[1:3] %&gt;% unlist() ## d dlow dhigh ## 0.8407149 -0.1024469 1.7602953 Usando effectsize cohens_d(A,B, ci = 1-a) ## # A tibble: 1 x 4 ## Cohens_d CI CI_low CI_high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.883 0.95 -0.108 1.85 hedges_g(A,B, ci = 1-a) ## # A tibble: 1 x 4 ## Hedges_g CI CI_low CI_high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.841 0.95 -0.102 1.76 Usando effsize cohen.d(A,B, conf.level = 1-a, noncentral=T) ## ## Cohen&#39;s d ## ## d estimate: 0.8827506 (large) ## 95 percent confidence interval: ## lower upper ## -0.1075693 1.8483100 cohen.d(A,B, conf.level = 1-a, hedges.correction = T, noncentral=T) ## ## Hedges&#39;s g ## ## g estimate: 0.8407149 (large) ## 95 percent confidence interval: ## lower upper ## -0.1075693 1.8483100 Usando las medidas PS, OVL, y \\(U_3\\) (Figura 15.6). ## # A tibble: 1 x 3 ## PS OVL U3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 73.4 65.9 81.1 Figura 15.6: \\(U_3\\) para el ejemplo de la prueba \\(t\\) de 2 muestras independientes. A Corresponde con los datos en escala original. B Corresponde con el tamaño del efecto \\(d\\). Conclusión: La longitud de los braquiópodos en la capa A no es signficativamente diferente de la longitud de los braquiópodos en la capa B (\\(\\Delta M = 0.165\\), 95% IC \\([-0.023\\), \\(0.353]\\)), \\(t(16) = 1.86\\), \\(p = .081\\), \\(d_s\\) = 0.88 \\([-0.11, 1.85]\\). El efecto se puede considerar grande, pero con un rango de muy pequeño, en dirección opuesta, hasta muy grande. Hay una probabilidad de 73.4% (PS) que un elemento de la muestra A tenga una longitud mayor que un elemento de la muestra B; las dos curvas se traslapan en un 65.9% (OVL); el 81.8% (\\(U_3\\)) de la muestra A se encuentra por encima de la media de la muestra B. 15.7 \\(t\\) de 2 muestras dependientes Estas muestras que miden la misma observación (objeto, sujeto) más de una vez también se conocen como muestras pareadas o mediciones repetidas (repeated measures - en inglés). El uso de la prueba \\(t\\) de 2 muestras dependientes se realiza con el ejemplo de la sección 14.2.2.2.2 (McKillup &amp; Darby Dyar, 2010), donde se tenía el contenido de \\(FeO\\) en porcentaje de peso para 10 granitos que fueron preparados a diferentes tamaños de grano: \\(&lt; 25 \\ \\mu m\\) y \\(&lt; 125 \\ \\mu m\\). Hay un efecto en el tratamiento, en este caso, cambio del tamaño de grano (\\(\\mu_D=0\\))? Asuma \\(\\alpha = 0.05\\). Identificar la población, distribución, y la prueba apropiada: Población: contenido de \\(FeO\\) a diferentes tamaños de grano: \\(&lt; 25 \\ \\mu m\\) y \\(&lt; 125 \\ \\mu m\\) Distribución: media de las diferencia Prueba: \\(t\\) de 2 muestras dependientes porque se tienen los mismos elementos antes y después de un tratamiento (cambio de tamaño de grano) Establecer las hipótesis nula y alterna: \\(H_0: \\mu_D = 0 \\to\\) El contenido de \\(FeO\\) es igual a los diferentes tamaños de grano \\(H_1: \\mu_D \\neq 0 \\to\\) El contenido de \\(FeO\\) es diferente a los diferentes tamaños de grano Determinar parámetros de la distribución a comparar (\\(H_0\\)): \\(\\mu_D = 0\\) \\(s_{\\bar{x}_d} = \\frac{s_d}{\\sqrt{n}} = \\frac{0.1247}{\\sqrt{10}} = 0.0394\\) Determinar valores críticos \\(\\alpha = 0.05\\) \\(t_{\\alpha/2,v} = t_{0.05/2,9} = |2.262|\\) Calcular el estadístico de prueba \\(t = \\frac{\\bar{x}_d}{s_{\\bar{x}_d}} = \\frac{0.1}{0.0394} = 2.538\\) \\(0.1 \\pm 0.0981 \\to 95\\% \\ IC \\ [0.0108,0.189]\\) Tomar una decisión El estadístico de prueba es mayor al crítico, \\(t &gt; t_{\\alpha/2,v}\\) El valor-p es menor a \\(\\alpha = 0.05\\), \\(p = 0.032\\) El valor de \\(0\\) no cae dentro del intervalo de confianza, \\(IC \\ [0.0108,0.189]\\) Decisión: Se rechaza \\(H_0\\) R trae la función t.test que puede realizar las diferentes pruebas \\(t\\). Para el caso de 2 muestras dependientes se brindan los vectores de datos, el indicador lógico de si es una prueba dependiente (paired), y el nivel de confianza. a = 0.05 m1 = c(13.5,14.6,12.7,15.5,11.1,16.4,13.2,19.3,16.7,18.4) m2 = c(13.6,14.6,12.6,15.7,11.1,16.6,13.2,19.5,16.8,18.7) n = length(m2) r = cor(m1,m2) t2.dep.res = t.test(x = m2, y = m1, paired = T, conf.level = 1-a) t2.dep.res ## ## Paired t-test ## ## data: m2 and m1 ## t = 2.5355, df = 9, p-value = 0.03195 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.01077932 0.18922068 ## sample estimates: ## mean of the differences ## 0.1 Como se mencionó en la sección 14.2.2.2.2, y a diferencia de muestras independientes, las pruebas de hipótesis para muestras dependientes no se pueden aproximar por medio de los intervalos de confianza (Figura 15.7), siguiendo las guías de Cumming &amp; Finch (2005), Cumming (2012), y Cumming &amp; Calin-Jageman (2017). En este caso solo se puede utilizar el intervalo de confianza para la diferencia, que va a estar en función de los márgenes de error de las muestras y la correlación entre las mismas (\\(MoE_d^2 = MoE_1^2 + MoE_2^2 - 2rMoE_1MoE_2\\)), donde a mayor correlación menor el márgen de error para la diferencia y mayor precisión (Cumming &amp; Finch, 2005). Figura 15.7: Intervalos de confianza para la prueba t de 2 muestras independientes. El valor de 0 cae fuera del intervalo de confianza para la diferencia, por lo que no se pueden considerar iguales o que hay diferencia entre la concentración de \\(FeO\\) para los tamaños. El tamaño del efecto se puede calcular de acuerdo a las Ecuaciones (15.8), (15.9) y (15.10), donde la diferencia es qué se usa como estandarizador. De nuevo es buena práctica reportar qué se usó como estandarizador para claridad en lo que se reporta (Cumming &amp; Calin-Jageman, 2017). Si la correlación entre las muestras es \\(r \\sim |0.5|\\) no hay mucha diferencia entre los tamaños de efecto, pero conforme la correlación se acerque a \\(0\\) o \\(1\\) mayor será la diferencia entre \\(d_z\\) con respecto a \\(d_{rm}\\) y \\(d_{av}\\) (Lakens, 2013). Debido al punto anterior, y la típica relación entre muestras, Cumming (2012), Cumming &amp; Calin-Jageman (2017), y Lakens (2013) sugieren utilizar \\(d_{av}\\) como el tamaño del efecto. \\[\\begin{equation} d_z = \\frac{\\bar{x}_d}{s_d}\\\\ d_z = \\frac{0.1}{0.1247} = 0.80 \\end{equation}\\] \\[\\begin{equation} d_{rm} = \\frac{\\bar{x}_d}{\\sqrt{s_1^2 + s_2^2 - 2 \\cdot r \\cdot s_1 \\cdot s_2}}\\sqrt{2(1-r)}\\\\ d_{rm} = \\frac{0.1}{\\sqrt{7.33 + 6.79 - 2 \\cdot 0.9996 \\cdot 2.7 \\cdot 2.6}}\\sqrt{2(1-0.9996)} = 0.001\\\\ \\text{usando todos los decimales da } d_{rm} = 0.022) \\end{equation}\\] \\[\\begin{equation} d_{av} = \\frac{\\bar{x}_d}{\\sqrt{\\frac{s_1^2 + s_2^2}{2}}}\\\\ d_{av} = \\frac{0.1}{\\sqrt{\\frac{7.33 + 6.79}{2}}} = 0.038 \\end{equation}\\] En R se puede calcular \\(d_z\\) con la función d.dep.t.diff, \\(d_{rm}\\) con la función d.dep.t.rm, y \\(d_{av}\\) con la función d.dep.t.avg, todas del paquete MOTE. Adicionalmente existen las funciones cohens_d de effectsize, y cohen.d de effsize, donde de nuevo se tiene que definir el argumento paired, pero en los dos casos el efecto que calculan es \\(d_z\\), el cual es el menos recomendado. Además, la función cohensd_rm del paquete itns (Erceg-Hurn et al., 2017) calcula \\(d_{av}\\). Usando MOTE a = 0.05 d.t2.dep.res1 = d.dep.t.diff(mdiff = mean(m2-m1), sddiff = sd(m2-m1), n = n, a = a) d.t2.dep.res1[1:3] %&gt;% unlist() ## d dlow dhigh ## 0.80178373 0.06638463 1.50482386 d.t2.dep.res2 = d.dep.t.rm(m1 = mean(m2), m2 = mean(m1), sd1 = sd(m2), sd2 = sd(m1), r = r, n = n, a = a) d.t2.dep.res2[1:3] %&gt;% unlist() ## d dlow dhigh ## 0.02164429 -0.59882065 0.64092600 d.t2.dep.res3 = d.dep.t.avg(m1 = mean(m2), m2 = mean(m1), sd1 = sd(m2), sd2 = sd(m1), n = n, a = a) d.t2.dep.res3[1:3] %&gt;% unlist() ## d dlow dhigh ## 0.03764125 -0.58341889 0.65664471 La Tabla 15.3 muestra la comparación entre los diferentes tamaños de efecto para el ejemplo de muestras dependientes. Los resultados muestran como \\(d_z\\) sobreestima (y por mucho) a \\(d_{rm}\\) y \\(d_{av}\\), donde \\(d_{rm}\\) es el más conservador de todos. (Lakens, 2013). El hecho de que \\(d_z\\) sobreestima el tamaño del efecto es devido a la alta correlación entre las muestras \\(r = 0.9996\\). Tabla 15.3: Comparación de tamaños de efecto para muestras dependientes ES \\(d\\) \\(IC_{inf}\\) \\(IC_{sup}\\) \\(d_z\\) 0.802 0.066 1.505 \\(d_{rm}\\) 0.022 -0.599 0.641 \\(d_{av}\\) 0.038 -0.583 0.657 Un panorama general del efecto de \\(r\\) en el tamaño del efecto para muestras dependientes se muestra en la Tabla 15.4, donde se generaron tres juegos de datos, provenientes de poblaciones con \\(r=0.1\\), \\(r=0.5\\), y \\(r=0.9\\), respectivamente. Se observa como a bajos valores de \\(r\\) \\(d_z\\) subestima el tamaño del efecto (con respecto a \\(d_{rm}\\) y \\(d_{av}\\)), a valores intermedios todos son muy similares, y a altos valores de \\(r\\) \\(d_z\\) sobreestima el tamaño del efecto (con respecto a \\(d_{rm}\\) y \\(d_{av}\\)), lo que muestra la sensibilidad y el efecto de \\(r\\) en el uso de \\(d_z\\), y por lo que se recomienda usar \\(d_{rm}\\) o \\(d_{av}\\). Tabla 15.4: Comparación de los diferentes \\(d\\) para muestras dependientes \\(\\rho\\) \\(r\\) \\(d_z\\) \\(d_{rm}\\) \\(d_{av}\\) 0.1 0.254 0.511 0.624 0.625 0.5 0.492 0.637 0.642 0.646 0.9 0.932 2.107 0.778 0.804 Usando itns d.t2.dep.res4 = cohensd_rm(m2,m1, ci = 1-a) d.t2.dep.res4 ## est ll ul ## 0.03763431 0.03630516 0.03671538 Usando effectsize cohens_d(m2,m1, paired = T, ci = 1-a) ## # A tibble: 1 x 4 ## Cohens_d CI CI_low CI_high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.802 0.95 0.0700 1.59 Usando effsize cohen.d(m2,m1, conf.level = 1-a, paired = T, noncentral=T) ## ## Cohen&#39;s d ## ## d estimate: 0.8017837 (large) ## 95 percent confidence interval: ## lower upper ## 0.06996669 1.58622949 Usando las medidas PS, OVL, y \\(U_3\\) (Figura 15.8). ## # A tibble: 1 x 3 ## PS OVL U3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 51.1 98.5 51.5 Figura 15.8: \\(U_3\\) para el ejemplo de la prueba \\(t\\) de 2 muestras dependientes. A Corresponde con los datos en escala original. B Corresponde con el tamaño del efecto \\(d\\). Conclusión: La concentración de \\(FeO\\) es signficativamente diferente para los diferentes tamaños (\\(M_d = 0.100\\), 95% IC \\([0.011\\), \\(0.189]\\)), \\(t(9) = 2.54\\), \\(p = .032\\), \\(N = 10\\), \\(r = 0.9996356\\), \\(d_{av}\\) = 0.04 \\([-0.58, 0.66]\\). El efecto se puede considerar muy pequeño, con un rango de mediano, en dirección opuesta, hasta mediano en la dirección del efecto. Hay una probabilidad de 51.1% (PS) que un elemento de la muestra de \\(&lt; 125 \\ \\mu m\\) tenga una concentración mayor que un elemento de la muestra de \\(&lt; 25 \\ \\mu m\\); las dos curvas se traslapan en un 98.5% (OVL); el 51.5% (\\(U_3\\)) de la muestra de \\(&lt; 125 \\ \\mu m\\) se encuentra por encima de la media de la muestra de \\(&lt; 25 \\ \\mu m\\). 15.8 ANOVA de 1-factor entre-grupos Hasta el momento solo se han analizado casos donde se tenían 1 o 2 muestras y se desean hacer inferencias sobre la media, pero en muchos casos se pueden llegar a tener 3 o más muestras. Se podría pensar en realizar diferentes pruebas \\(t\\) de dos muestras, pero esto genera dos problemas: primero, dependiendo de la cantidad de grupos el número de comparaciones sería muy grande, y segundo y más importante al realizar diferentes purbeas \\(t\\) se va a inflar el error tipo-I (\\(\\alpha\\)), pudiendo incurrir en conclusiones equivocadas (Nolan &amp; Heinzen, 2014). La prueba del análisis de varianza (ANOVA - Analysis of Variance, en inglés), es una prueba que permite dilucidar la similitud entre muestras sin incurrir en los problemas antes mencionados, y la cual hace uso de la distribución y prueba \\(F\\) para el análisis. La base teórica y procedimental que se va a exponer aquí se puede encontrar en (Borradaile, 2003; Davis, 2002; McKillup &amp; Darby Dyar, 2010; Nolan &amp; Heinzen, 2014; Swan &amp; Sandilands, 1995; Walpole et al., 2012) Una suposición de ANOVA es que las varianzas son iguales; esta prueba es relativamente robusta a esta suposición siempre y cuando la razón entre las varianzas mayor y menor no sea superior a 2. ANOVA es una técnica que permite diferentes diseños, pero en esta sección se presenta el caso más sencillo, el cual se conoce como ANOVA de 1 factor entre-grupos (between-groups ANOVA). Este diseño quiere decir que la variable de interés (numérica continua) está en función de una variable categórica (factor o tratamiento) con \\(2+\\) grupos o clases. De hecho la prueba \\(t\\) de 2 muestras independientes es un caso especial de ANOVA con 2 grupos, y se puede comprobar que \\(F=t^2\\). De manera general las hipótesis nula y alterna serían: \\(H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\cdots = \\mu_k\\) \\(H_1: \\text{Al menos 1 de las media es diferente}\\) Para poder determinar el efecto de los diferentes grupos (muestras) en la variación de la media de la variable de interés, se puede pensar en que hay dos fuentes de variación: entre las muestras (Figura 15.9) y dentro de las muestras (Figura 15.10). Figura 15.9: Representación de la variación entre grupos. Conforme más separados estén y más distancia con respecto a la media general, mayor la variación. Figura 15.10: Representación de la variación dentro de grupos. Conforme más dispersión haya en las muestras mayor la vriación dentro de lo grupos. La variación entre grupos va a cuantificar la diferencia real entre las medias más el error (variación dentro de los grupos), por lo que de manera general se puede estimar este efecto por medio del estadístico \\(F\\) a como se muestra en la Ecuación (15.17). Conforme mayor sea la diferencia entre los grupos mayor será el valor de \\(F\\), mientras que conforme más similares sean los grupos \\(F \\sim 1\\). Para este caso la prueba \\(F\\) va a ser siempre de una cola, donde hay un único valor crítico y la región de rechazo se encuentra a la derecha. \\[\\begin{equation} F = \\frac{\\text{variación entre grupos (diferencia + error)}}{\\text{variación dentro de grupos (error)}} \\tag{15.17} \\end{equation}\\] Figura 15.11: Representación de la distribución \\(F\\), donde se muestra la región crítica o de rechazo en color rojo. El resultado de ANOVA por lo general se presenta como una tabla, donde manera general lleva la estructura presentada en la Tabla 15.5. El tamaño del efecto para ANOVA se puede obtener de los valores de la tabla: \\(\\eta^2=SCG/SCT\\), donde \\(SCT\\) es la suma de cuadrados total (\\(SCT=SCG+SCE\\)) y corresponde con el porcentaje de variación en la variable respuesta explicado por el efecto de los grupos. Tabla 15.5: Tabla de ANOVA Fuente de variación Suma de cuadrados Grados de liberta Cuadrados medios F Entre grupos \\(SCG = \\sum(\\bar{x}_g-\\bar{x}_T)^2 \\cdot n_g\\) \\(k-1\\) \\(s_1^2 = \\frac{\\sum(\\bar{x}_g-\\bar{x}_T)^2 \\cdot n_g}{k-1}\\) \\(\\frac{s_1^2}{s^2}\\) Dentro de grupos (error) \\(SCE = \\sum\\sum(x_{ig}-\\bar{x}_g)^2\\) \\(N-k\\) \\(s^2 = \\frac{\\sum\\sum(x_{ig}-\\bar{x}_g)^2}{N-k}\\) Notas: \\(\\bar{x}_g\\) = media del grupo \\(\\bar{x}_T\\) = media total (global) \\(x_{ig}\\) = valor \\(i\\) del grupo \\(g\\) \\(n_g\\) = tamaño del grupo \\(g\\) \\(k\\) = número de grupos \\(N\\) = total de observaciones Cuando el resultado de la prueba de ANOVA es significativo esto indica que al menos uno de los grupos es diferente, pero no se sabe cuál. Para poder responder hay dos opciones: Si no se tiene idea o no se ha decidido con anterioridad de cuáles grupos se quieren comparar se puedeen realizar análisis posteriores (post-hoc) comparando todos los grupos, donde se ajusta para el error tipo-I. El post-hoc más utilizado es TukeyHSD (Honestly Significant Difference), pero existen otros como Bonferroni, Holm, Scheffe, etc. Si se han decidido comparaciones (contrastes) a priori éstas son las que se analizan, pero debieron haberse especificado durante la planificación del estudio y antes de recolectar los datos. Para demostrar el ANOVA se usa un ejemplo de McKillup &amp; Darby Dyar (2010), donde se tiene el contenido de \\(MgO\\) presente en cuatro turmalinas en tres sitios diferentes: Mount Mica, Sebago Batholith, Black Mountain. Asuma \\(\\alpha=0.05\\). Identificar la población, distribución, y la prueba apropiada: Población: contenido de \\(MgO\\) presente en turmalinas en Mount Mica, Sebago Batholith, Black Mountain Distribución: F Prueba: \\(F\\) Establecer las hipótesis nula y alterna: \\(H_0: \\mu_1 = \\mu_2 = \\mu_3 \\to\\) El contenido de \\(MgO\\) presente en las turmalinas de los tres sitios es el mismo \\(H_1:\\) El contenido de \\(MgO\\) presente en las turmalinas difiere por lo menos en uno de los tres sitios Determinar parámetros de la distribución a comparar (\\(H_0\\)): \\(v_1 = k-1 = 3-1=2\\) \\(v_2 = N-k = 12-3 = 9\\) Determinar valores críticos \\(\\alpha = 0.05\\) \\(F_{\\alpha,v_1,v_2} = F_{0.05,2,9} = 4.256\\) Calcular el estadístico de prueba \\(F = \\frac{36}{3.3} = 10.8\\) El procedimiento manual no se detalla aquí por ser más extenso y elaborado de lo normal, pero se recomienda al lector revisar las referencias mencionadas para detallarlo y entenderlo Tomar una decisión El estadístico de prueba es mayor al crítico, \\(F &gt; F_{\\alpha,v_1,v_2}\\) El valor-p es menor a \\(\\alpha = 0.05\\), \\(p = 0.004\\) Decisión: Se rechaza \\(H_0\\) y por lo menos uno de los contenidos de \\(MgO\\) es diferente En R los datos tienen que estar en formato largo, lo que quiere decir una columna con los valores de la variable de interés, y otra columna con el grupo al que pertence la medición. Otro punto importante es que la variable agrupadora es mejor que sea del tipo factor. Los datos del ejemplo se encuentran en el archivo anova MgO.csv a = 0.05 dat.aov = import(&#39;data/anova MgO.csv&#39;, setclass = &#39;tibble&#39;) %&gt;% mutate(Location = as.factor(Location) %&gt;% fct_reorder(MgO)) dat.aov ## # A tibble: 12 x 2 ## Location MgO ## &lt;fct&gt; &lt;int&gt; ## 1 Mount Mica 7 ## 2 Mount Mica 8 ## 3 Mount Mica 10 ## 4 Mount Mica 11 ## 5 Sebago Batholith 4 ## 6 Sebago Batholith 5 ## 7 Sebago Batholith 7 ## 8 Sebago Batholith 8 ## 9 Black Mountain 1 ## 10 Black Mountain 2 ## 11 Black Mountain 4 ## 12 Black Mountain 5 Es recomendable hacer una inspección de los grupos, tanto numerica (Tabla 15.6) como gráfica, para darnos una idea de la media y desviaciones estándar. Aquí se observa que todos los grupos tienen la misma desviación estándar (algo poco comun), por lo que el supuesto de la homogeniedad de varianzas se mantiene. dat.aov %&gt;% group_by(Location) %&gt;% summarise_all(list(N = ~n(), Avg = ~mean(.), SD = ~sd(.))) Tabla 15.6: Resumen por grupo Location N Avg SD Black Mountain 4 3 1.826 Sebago Batholith 4 6 1.826 Mount Mica 4 9 1.826 ggplot(dat.aov, aes(Location, MgO)) + stat_summary(fun.data = mean_cl_normal, fun.args = list(conf.int = 1-a), geom = &#39;pointrange&#39;, size = .75) + labs(x=&#39;&#39;) Figura 15.12: Medias e intervalos de confianza (95%) para cada grupo. Para realizar el ANOVA se puede hacer de diferentes maneras, pero de manera general se expresa como un modelo lineal y ~ x, donde y es la variable de interes y x es la variable agrupadora. Se pueden usar diferentes funciones, dentro de ellas aov, lm, donde siempre es necesario especificar la tabla de datos. La tabla resumen se muestra en la Tabla 15.7, donde se puede confeccionar a partir de la función anova. mgo.aov = aov(MgO ~ Location, dat.aov) anova(mgo.aov) Tabla 15.7: Tabla de ANOVA para el ejemplo Variación Grados libertad Suma Cuadrados Cuadrados Medios F Valor-p Location 2 72 36.0000 10.8 0.0041 Residuals 9 30 3.3333 NA NA El resultado es significativo, indicando que por lo menos uno de los contenidos de \\(MgO\\) difiere del resto. El tamaño del efecto \\(\\eta^2\\), así como el intervalo de confianza, se puede obtener por medio de las funciones eta.F de MOTE y eta_squared de effectsize. Para eta.F es necesario indicar los grados de liberta, el valor de \\(F\\), y el nivel de significancia \\(\\alpha\\). Para eta_squared lo que se necesita es el objeto de ANOVA y el nivel se confianza. aov.eta = eta.F(dfm = 2, dfe = 9, Fvalue = 10.8, a = a) aov.eta[1:3] %&gt;% unlist() ## eta etalow etahigh ## 0.7058824 0.1574104 0.8872368 eta_squared(mgo.aov,ci = 1-a) ## # A tibble: 1 x 5 ## Parameter Eta_Sq_partial CI CI_low CI_high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Location 0.706 0.95 0.193 0.859 Como se encontró un resultdo significativo, es necesario indicar dónde se encuentran esas diferencias. Aquí no se definirieron contrastes a priori, por lo que se usa Tukey para ajustar el error tipo-I para todas las comparaciones. Se usa la función TukeyHSD donde los argumentos necesarios son el objeto de ANOVA y el nivel de confianza. Aquí se puede interpretar el valor-p con respecto al nivel de significancia escogido, ya que es un valor-p ajustado, de igual manera se puede interpretar el intervalo de confianza. El resumen de la prueba de Tukey se presenta en la Tabla 15.8 y la Figura 15.13. tukey = TukeyHSD(mgo.aov, conf.level = 1-a) %&gt;% tidy() %&gt;% dplyr::select(-term) Tabla 15.8: Resumen de la prueba de Tukey Comparación Diferencia \\(IC_{inf}\\) \\(IC_{sup}\\) Valor-p ajustado Sebago Batholith-Black Mountain 3 -0.604 6.604 0.103 Mount Mica-Black Mountain 6 2.396 9.604 0.003 Mount Mica-Sebago Batholith 3 -0.604 6.604 0.103 ggplot(tukey, aes(comparison, estimate, ymin = conf.low, ymax = conf.high)) + geom_pointrange(size = 1) + geom_hline(yintercept = 0, col = &#39;red&#39;) + coord_flip() + theme_bw() + labs(y = &#39;Diferencia en medias&#39;, x = &#39;Comparacion&#39;) Figura 15.13: Intervalos de confianza (95%) para las comparaciones usando la prueba de Tukey. Conclusión: El contenido de \\(MgO\\) varía significativamentre entre las localidades, \\(F\\)(2, 9) = 10.80, \\(p\\) = .004. El tamano del efecto es grande, \\(\\eta^2\\) = 0.71, 95% IC [0.16, 0.89], pero con un rango muy amplio. Analisis posteriores con Tukey HSD indican que hay una diferencia entre Mount Mica - Black Mountain (p = .003), pero no asi entre Mount Mica - Sebago Batholit (p = .103), ni Sebago Batholith - Black Mountain (p = .103). 15.9 \\(t\\) de correlación En la sección de Correlación (10.3) se introdujo el concepto de correlación, más específicamente la correlación de Pearson. Esta es una medida de asosiación entre dos variables (en este caso numéricas) que indica la magnitud y dirección de dicha asociación. La idea de la prueba para la correlación es determinar si la correlación encontrada es significativa o no, o sea, si difiere de cero (Borradaile, 2003; Davis, 2002; Nolan &amp; Heinzen, 2014; Swan &amp; Sandilands, 1995; Trauth, 2015). El uso de la prueba \\(t\\) de correlación se realiza con un ejemplo de Davis (2002), donde se tiene la longitud de los ejes de cantos en una playa (Tabla 15.9). Se quiere determinar si la correlación entre los ejes \\(a\\) y \\(b\\) es significativa, asumiendo \\(\\alpha = 0.1\\). Tabla 15.9: Longitudes de los ejes de cantos en una playa Canto Eje-a Eje-b Eje-c 1 8 7 3 2 16 8 5 3 12 10 9 4 13 12 5 5 16 14 5 6 14 9 8 7 16 13 13 8 11 6 3 9 15 9 9 10 13 10 9 Fuente: Davis (2002) Identificar la población, distribución, y la prueba apropiada: Población: relación entre ejes \\(a\\) y \\(b\\) Distribución: de correlaciones Prueba: \\(t\\) de correlación Establecer las hipótesis nula y alterna: \\(H_0: \\rho = 0 \\to\\) No hay relación entre los ejes \\(H_1: \\rho \\neq 0 \\to\\) Hay una relación entre los ejes Nota: \\(\\rho\\) es el parámetro poblacional para la correlación Determinar parámetros de la distribución a comparar (\\(H_0\\)): \\(r_{a,b} = 0.597\\) \\(v = N-2 = 10-2 = 8\\) Aquí el único parámetro del que depende la prueba son los grados de libertad \\(v\\), pero se muestra el valor de \\(r\\) encontrado Determinar valores críticos \\(\\alpha = 0.1\\) \\(t_{\\alpha/2,v} = t_{0.1/2,8} = |1.860|\\) Calcular el estadístico de prueba \\(t = \\frac{r \\sqrt{v}}{\\sqrt{1-r^2}} = \\frac{0.597 \\cdot \\sqrt{8}}{\\sqrt{1-0.597^2}} = 2.10\\) \\(90\\% \\ IC \\ [0.066,0.864]\\) Tomar una decisión El estadístico de prueba es mayor al crítico, \\(t &gt; t_{\\alpha/2,v}\\) El valor-p es menor a \\(\\alpha = 0.1\\), \\(p = 0.068\\) El valor de \\(0\\) no cae dentro del intervalo de confianza, \\(IC \\ [0.0108,0.189]\\) Decisión: Se rechaza \\(H_0\\) Nota: Tomar en cuenta que si se escogiera \\(\\alpha=0.05\\), no se rechazaría \\(H_0\\) En R así como está la función cor para calcular el coeficiente de correlación, se tiene la función cor.test para la prueba específica. Se pueden especificar el típo de correlación (method) y el nivel de significancia. El coeficiente de correlación es ya un tamaño de efecto, por lo que no es necesario realizar ningun cálculo adicional. cantos = tibble(Canto = 1:10, a = c(8,16,12,13,16,14,16,11,15,13), b = c(7,8,10,12,14,9,13,6,9,10), c = c(3,5,9,5,5,8,13,3,9,9)) a = 0.1 r.res = cor.test(~a+b,data = cantos, method = &#39;pearson&#39;, conf.level = 1-a) r.res ## ## Pearson&#39;s product-moment correlation ## ## data: a and b ## t = 2.1031, df = 8, p-value = 0.06861 ## alternative hypothesis: true correlation is not equal to 0 ## 90 percent confidence interval: ## 0.0661825 0.8641924 ## sample estimates: ## cor ## 0.5966799 Conclusión: Los ejes \\(a\\) y \\(b\\) están correlacionados, \\(r = .60\\), 90% IC \\([.07\\), \\(.86]\\), \\(t(8) = 2.10\\), \\(p = .069\\). El efecto se puede considerar grande, con un rango de pequeño, hasta muy grande. 15.10 \\(\\chi^2\\) para 1 varianza Así como se pueden realizar pruebas estadísticas sobre la media poblacional (\\(\\mu\\)), se pueden realizar pruebas estadísticas sobre la varianza poblacional (\\(\\sigma^2\\)), aunque son menos comunes. Por esto último es que para estas pruebas no hay tamaños de efecto estandarizados, sino más bien se usan los datos en escala original. El uso de la prueba \\(\\chi^2\\) de 1 muestra se realiza con el ejemplo de la sección 14.2.3 (Swan &amp; Sandilands, 1995), donde se tenía el contenido de cuarzo en secciones delgadas de una roca ígnea. Es posible que esta muestra provenga de una población con varianza 12 (\\(\\sigma^2_0=12\\))? Asuma \\(\\alpha = 0.05\\). Identificar la población, distribución, y la prueba apropiada: Población: variabilidad del contenido de cuarzo en la roca ígnea Distribución: de varianza Prueba: \\(\\chi^2\\) de 1 muestra porque se tiene 1 muestra, se quiere hacer inferencia sobre la variabilidad, y se quiere comparar con un valor hipotético Establecer las hipótesis nula y alterna: \\(H_0: \\sigma^2 = \\sigma^2_0 \\to\\) La variabilidad en el contenido de cuarzo en la roca ígnea es igual a un valor hipotético o conocido (12) \\(H_1: \\sigma^2 \\neq \\sigma^2_0 \\to\\) La variabilidad en el contenido de cuarzo en la roca ígnea es diferente a un valor hipotético o conocido (12) Determinar parámetros de la distribución a comparar (\\(H_0\\)): \\(\\sigma^2_0 = 20\\) \\(v = N-1 = 8-1 = 7\\) Determinar valores críticos \\(\\alpha = 0.05\\) \\(\\chi^2_{\\alpha/2,v} = \\chi^2_{0.05/2,7} = 12.83\\) \\(\\chi^2_{1-\\alpha/2,v} = \\chi^2_{1-0.05/2,7} = 0.83\\) Calcular el estadístico de prueba \\(\\chi^2 = \\frac{(n-1)s^2}{\\sigma_0^2} = \\frac{(8-1)9.507}{12} = 5.546\\) \\(4.16 &lt; \\sigma^2 &lt; 39.38 \\to 95\\% \\ IC \\ [4.16,39.38]\\) Tomar una decisión El estadístico de prueba cae entre los valores críticos El valor-p es mayor a \\(\\alpha = 0.05\\), \\(p = 0.8127\\) El valor hipotético del parámetro cae dentro del intervalo de confianza, \\(IC \\ [4.16, 39.38]\\) Decisión: No se rechaza \\(H_0\\) En R el paquete DescTools trae la función VarTest para realizar esta prueba, donde ocupa brindar el vector de datos, la varianza poblacional (sigma.squared), y el nivel de confianza. sigma0 = 12 a = 0.05 cuarzo = c(23.5, 16.6, 25.4, 19.1, 19.3, 22.4, 20.9, 24.9) n = length(cuarzo) chi.res = VarTest(x = cuarzo, sigma.squared = sigma0, conf.level = 1-a) chi.res ## ## One Sample Chi-Square test on variance ## ## data: cuarzo ## X-squared = 5.5457, df = 7, p-value = 0.8127 ## alternative hypothesis: true variance is not equal to 12 ## 95 percent confidence interval: ## 4.155981 39.381007 ## sample estimates: ## variance of x ## 9.506964 Conclusión: La variabilidad del contenido de cuarzo no difiere significativamente del valor propuesto, \\(s^2 = 9.51\\), 95% IC \\([4.16\\), \\(39.38]\\), \\(\\chi^2(7, n = 8) = 5.55\\), \\(p = .813\\). 15.11 \\(F\\) para 2 varianzas En algunas de las pruebas se hace la suposicion de igualdad de varianzas. Lo anterior se puede evaluar empiricamente comparando la menor y mayor desviacion estandar, donde la relacion entre ellas no debiera ser mayor a 2 (Cumming &amp; Calin-Jageman, 2017). Si se quiere realizar de manera formal se puede realizar la prueba aqui descrita. El uso de la prueba \\(F\\) para 2 varianzas se realiza con el ejemplo de la sección 15.6 (Swan &amp; Sandilands, 1995), donde se tenían braquiópodos en dos capas (A, B) y se les midió la longitud (cm). Es factible la suposicion de igualdad de varianzas (\\(\\sigma^2_1=\\sigma^2_2\\))? Asuma \\(\\alpha = 0.05\\). Identificar la población, distribución, y la prueba apropiada: Población: variabilidad de la longitud de braquiopodos en las capas A y B Distribución: de razon de varianzas (\\(F\\)) Prueba: \\(F\\) para 2 muestras (varianzas), se quiere hacer inferencia sobre la variabilidad, y se quieren comparar las varianzas de 2 muestras Establecer las hipótesis nula y alterna: \\(H_0: \\sigma^2_1 = \\sigma^2_2, \\ \\frac{\\sigma^2_1}{\\sigma^2_2} = 1 \\to\\) La variabilidad la longitud de braquiopodos de la capa A es igual a la variabilidad la longitud de braquiopodos de la capa B \\(H_1: \\sigma^2_1 \\neq \\sigma^2_2, \\ \\frac{\\sigma^2_1}{\\sigma^2_2} \\neq 1 \\to\\) La variabilidad la longitud de braquiopodos de la capa A es diferente a la variabilidad la longitud de braquiopodos de la capa B Determinar parámetros de la distribución a comparar (\\(H_0\\)): \\(v_1 = n_1-1 = 8-1 = 7\\) \\(v_2 = n_2-1 = 10-1 = 9\\) Determinar valores críticos \\(\\alpha = 0.05\\) \\(F_{\\alpha/2,v_1,v_2} = F_{0.05/2,8,10} = 3.85\\) \\(F_{1-\\alpha/2,v_1,v_2} = F_{1-0.05/2,8,10} = 0.23\\) Calcular el estadístico de prueba \\(\\frac{\\sigma^2_1}{\\sigma^2_2} = \\frac{0.042}{0.029} = 1.45\\) \\(0.345 &lt; \\frac{\\sigma^2_1}{\\sigma^2_2} &lt; 6.985 \\to 95\\% \\ IC \\ [0.345,6.985]\\) Tomar una decisión El estadístico de prueba cae entre los valores críticos El valor-p es mayor a \\(\\alpha = 0.05\\), \\(p = 0.599\\) El valor hipotético del parámetro (1) cae dentro del intervalo de confianza, \\(IC \\ [0.345, 6.985]\\) Decisión: No se rechaza \\(H_0\\) En R la función var.test para realizar esta prueba, donde se ocupan brindar los vectores de datos, la razon de varianzas (ratio), y el nivel de confianza. a = 0.05 f.res = var.test(x = A,y = B, ratio = 1, conf.level = 1-a) f.res ## ## F test to compare two variances ## ## data: A and B ## F = 1.4367, num df = 7, denom df = 9, p-value = 0.5994 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.3423094 6.9294596 ## sample estimates: ## ratio of variances ## 1.436688 Conclusión: La variabilidad de la longitud de los braquiopodos en las capas A y B no varia significativamente, \\(\\text{razon de varianzas} = 1.44\\), 95% IC \\([0.34\\), \\(6.93]\\), \\(F(7, 9) = 1.44\\), \\(p = .599\\). Referencias "],
["estadística-no-paramétrica.html", "Capítulo 16 Estadística No Paramétrica 16.1 Introducción 16.2 Pruebas no-paramétricas 16.3 Tamaño del efecto 16.4 Bondad de ajuste 16.5 Homogeneidad 16.6 Independencia / asociación", " Capítulo 16 Estadística No Paramétrica 16.1 Introducción 16.2 Pruebas no-paramétricas Prueba-\\(\\chi^2\\) de bondad de ajuste (16.4) \\(H_0:\\) Datos siguen la distribución/proporción esperada Prueba-\\(\\chi^2\\) de homogeneidad (16.5) \\(H_0:\\) Distribución homogenea entre las categorías de variables cualitativas Prueba-\\(\\chi^2\\) de independencia/asociación (16.6) \\(H_0:\\) No hay relación entre variables cualitativas o éstas son independientes entre si 16.3 Tamaño del efecto Tabla 16.1: Tamaños de efecto estandarizados para pruebas no-paramétricas Prueba Tamaño de efecto Correlación \\[\\begin{equation} r_{s} = \\frac{d_s}{\\sqrt{d_s^2 + \\frac{(n_1 + n_2)^2}{n_1 n_2}}} \\tag{15.12} \\end{equation}\\] \\(\\chi^2\\) \\[\\begin{equation} \\phi = \\sqrt{\\frac{\\chi^2}{N}} \\tag{16.1} \\end{equation}\\] \\(\\chi^2\\) \\[\\begin{equation} V = \\sqrt{\\frac{\\chi^2}{N \\cdot v_{min}}} \\tag{16.2} \\end{equation}\\] Notas: \\(N\\) = total de observaciones \\(r_{s}\\) = coeficiente de correlación de Spearman \\(v_{min}\\) = grados de libertad mínimo de las filas o columnas de la tabla de contingencia El \\(\\phi\\) y Cramer \\(V\\) comprenden el rango de 0 a 1 y se pueden interpretar similar a \\(r\\), donde valores cercanos a 0 indican poca o nula asociación y valores cerca de 1 indican alta asociación (Fritz et al., 2012; Tomczak &amp; Tomczak, 2014). 16.4 Bondad de ajuste 16.5 Homogeneidad 16.6 Independencia / asociación Referencias "],
["estadística-direccional.html", "Capítulo 17 Estadística Direccional", " Capítulo 17 Estadística Direccional "],
["secuencias-de-datos.html", "Capítulo 18 Secuencias de Datos", " Capítulo 18 Secuencias de Datos "],
["geoestadística.html", "Capítulo 19 Geoestadística", " Capítulo 19 Geoestadística "],
["referencias.html", "Referencias", " Referencias "]
]
